import type { Question } from '../types';

export const neuralNetworksQuestions: Question[] = [
  {
    id: 'ai-neural-networks-001',
    block: 'ai',
    topic: 'neural-networks',
    topicLabel: 'Нейронные сети',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что делает функция активации в нейронной сети?',
    options: [
      'Нормализует входные данные перед подачей в сеть',
      'Вносит нелинейность, позволяя сети моделировать сложные зависимости',
      'Определяет скорость обучения модели',
      'Инициализирует начальные веса нейронов',
    ],
    correctIndex: 1,
    explanation:
      'Функция активации вносит нелинейность в вычисления нейронной сети. Без неё сеть любой глубины была бы эквивалентна одному линейному преобразованию (композиция линейных функций — линейная функция). Популярные функции: ReLU (max(0, x)) — стандарт для скрытых слоёв, Sigmoid (1/(1+e^-x)) — для вероятностей, Softmax — для мультиклассовой классификации, GELU — используется в трансформерах.',
  },
  {
    id: 'ai-neural-networks-002',
    block: 'ai',
    topic: 'neural-networks',
    topicLabel: 'Нейронные сети',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое backpropagation (обратное распространение ошибки)?',
    options: [
      'Метод инициализации весов нейронной сети',
      'Алгоритм вычисления градиентов функции потерь по весам сети через цепное правило дифференцирования',
      'Способ передачи данных от входа к выходу сети',
      'Метод обрезки (pruning) неактивных нейронов',
    ],
    correctIndex: 1,
    explanation:
      'Backpropagation — алгоритм эффективного вычисления градиентов функции потерь по всем параметрам (весам) сети. Использует цепное правило (chain rule): градиенты вычисляются от выходного слоя к входному, переиспользуя промежуточные результаты. Вычисленные градиенты затем используются оптимизатором (SGD, Adam) для обновления весов. Forward pass → вычисление loss → backward pass (градиенты) → обновление весов.',
  },
  {
    id: 'ai-neural-networks-003',
    block: 'ai',
    topic: 'neural-networks',
    topicLabel: 'Нейронные сети',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Для какого типа данных изначально были разработаны свёрточные нейронные сети (CNN)?',
    options: [
      'Табличные данные',
      'Изображения',
      'Текстовые последовательности',
      'Графовые данные',
    ],
    correctIndex: 1,
    explanation:
      'CNN (Convolutional Neural Networks) разработаны для обработки данных с сеточной структурой, прежде всего изображений. Ключевые идеи: свёрточные фильтры (kernels) для извлечения локальных признаков, разделение весов (weight sharing) — один фильтр применяется ко всему изображению, pooling для уменьшения размерности. CNN также применяются для аудио (1D-свёртки) и видео (3D-свёртки). Архитектуры-вехи: LeNet, AlexNet, VGG, ResNet.',
  },
  {
    id: 'ai-neural-networks-004',
    block: 'ai',
    topic: 'neural-networks',
    topicLabel: 'Нейронные сети',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какую проблему решают остаточные связи (residual connections) в архитектуре ResNet?',
    options: [
      'Ускоряют инференс за счёт пропуска слоёв',
      'Решают проблему затухания/взрыва градиентов при обучении глубоких сетей',
      'Уменьшают количество параметров модели',
      'Позволяют обучать сеть без функций активации',
    ],
    correctIndex: 1,
    explanation:
      'Residual connections (skip connections) добавляют вход блока к его выходу: y = F(x) + x. Это решает проблему деградации: при увеличении глубины обычных сетей качество сначала растёт, потом падает из-за затухания градиентов. Skip connections позволяют градиентам «течь» напрямую через сеть, обходя слои. Сеть учится моделировать остаток (residual) F(x) = H(x) - x, что проще чем прямое отображение H(x). ResNet (2015) позволил обучать сети со 100+ слоями. Идея residual connections используется повсеместно, включая трансформеры.',
  },
  {
    id: 'ai-neural-networks-005',
    block: 'ai',
    topic: 'neural-networks',
    topicLabel: 'Нейронные сети',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что делает Dropout во время обучения нейронной сети?',
    options: [
      'Удаляет слои с наименьшим вкладом в результат',
      'Случайно «выключает» нейроны с заданной вероятностью на каждом шаге обучения, предотвращая переобучение',
      'Уменьшает learning rate при достижении плато',
      'Автоматически останавливает обучение при росте ошибки на валидации',
    ],
    correctIndex: 1,
    explanation:
      'Dropout (Srivastava, 2014) — техника регуляризации: на каждом шаге обучения каждый нейрон «выключается» с вероятностью p (обычно 0.1–0.5). Это заставляет сеть не полагаться на конкретные нейроны и создаёт неявный ансамбль подсетей. При инференсе dropout отключается, а выходы масштабируются (или используется inverted dropout при обучении). Интуиция: как если бы каждый раз обучалась случайная подсеть, а при инференсе использовалось усреднение.',
  },
  {
    id: 'ai-neural-networks-006',
    block: 'ai',
    topic: 'neural-networks',
    topicLabel: 'Нейронные сети',
    difficulty: 'middle',
    type: 'open',
    question: 'Сравните RNN, LSTM и GRU. Какие проблемы решает каждая архитектура и почему трансформеры их в значительной степени заменили?',
    sampleAnswer:
      'RNN (Recurrent Neural Network) обрабатывает последовательности, передавая скрытое состояние между шагами. Проблема: затухание/взрыв градиентов при длинных последовательностях — сеть «забывает» ранние токены. LSTM (Long Short-Term Memory) решает это через механизм вентилей (gates): forget gate, input gate, output gate и ячейку памяти (cell state), позволяющую информации «течь» через длинные последовательности. GRU (Gated Recurrent Unit) — упрощённая версия LSTM с двумя вентилями (reset, update), меньше параметров, сравнимое качество. Трансформеры заменили их потому что: (1) attention позволяет напрямую обращаться к любому токену без пошаговой передачи, (2) параллелизация — RNN/LSTM обрабатывают токены последовательно, трансформеры — параллельно, (3) лучше масштабируются с объёмом данных и вычислений.',
    explanation:
      'Эволюция: RNN → LSTM/GRU → Transformer. Каждый шаг решал ключевую проблему предшественника. RNN не справлялись с длинными зависимостями, LSTM/GRU частично решили это, но оставались последовательными. Трансформеры (2017) решили обе проблемы через self-attention. Однако RNN возвращаются в новых формах (Mamba, RWKV) для эффективного инференса.',
  },
  {
    id: 'ai-neural-networks-007',
    block: 'ai',
    topic: 'neural-networks',
    topicLabel: 'Нейронные сети',
    difficulty: 'senior',
    type: 'open',
    question: 'Объясните проблему vanishing/exploding gradients. Какие архитектурные решения и техники инициализации были разработаны для её решения?',
    sampleAnswer:
      'При backpropagation через глубокую сеть градиенты перемножаются на каждом слое. Если множители < 1 — градиенты затухают (vanishing), если > 1 — взрываются (exploding). Затухание: нижние слои не обучаются. Взрыв: NaN в весах, нестабильное обучение. Решения: (1) Архитектурные: residual connections (ResNet), highway networks, gating mechanisms (LSTM). (2) Инициализация: Xavier/Glorot (для sigmoid/tanh), He/Kaiming (для ReLU) — подбирают дисперсию начальных весов так, чтобы сигнал не затухал и не взрывался. (3) Нормализация: Batch Normalization, Layer Normalization — стабилизируют распределение активаций между слоями. (4) Функции активации: ReLU и варианты (LeakyReLU, ELU) вместо sigmoid, у которого gradient saturates. (5) Gradient clipping — обрезка градиентов по норме для предотвращения взрыва.',
    explanation:
      'Vanishing/exploding gradients — одна из причин, почему глубокое обучение стало практичным только после 2010-х. Комбинация ReLU + He initialization + BatchNorm + residual connections позволила обучать сети с сотнями слоёв. В трансформерах LayerNorm + residual connections — стандартный рецепт стабильности.',
  },
  {
    id: 'ai-neural-networks-008',
    block: 'ai',
    topic: 'neural-networks',
    topicLabel: 'Нейронные сети',
    difficulty: 'senior',
    type: 'open',
    question: 'Сравните оптимизаторы SGD, Adam и AdamW. Когда стоит выбирать каждый из них?',
    sampleAnswer:
      'SGD (Stochastic Gradient Descent): обновляет веса пропорционально градиенту с фиксированным learning rate. С momentum — добавляет «инерцию», ускоряя сходимость. Плюсы: простой, часто лучше обобщает. Минусы: чувствителен к learning rate, медленнее сходится. Adam (Adaptive Moment Estimation): адаптивный learning rate для каждого параметра, использует экспоненциально скользящие средние градиента (1-й момент) и квадрата градиента (2-й момент). Плюсы: быстрая сходимость, менее чувствителен к начальному LR. Минусы: может хуже обобщать чем SGD. AdamW: исправляет проблему Adam с weight decay — разделяет L2-регуляризацию и адаптивные learning rates (decoupled weight decay). Стандарт для обучения трансформеров. Рекомендации: SGD + momentum — для CNN и задач с хорошо изученными гиперпараметрами. AdamW — для трансформеров и LLM. Adam — для быстрого прототипирования.',
    explanation:
      'Выбор оптимизатора влияет на скорость сходимости и качество финальной модели. AdamW стал де-факто стандартом для обучения трансформеров после статьи "Decoupled Weight Decay Regularization" (Loshchilov & Hutter, 2019). Для дообучения LLM также используются 8-bit Adam, Adafactor (экономия памяти), LAMB/LARS (для больших batch size).',
  },
  {
    id: 'ai-neural-networks-009',
    block: 'ai',
    topic: 'neural-networks',
    topicLabel: 'Нейронные сети',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое Batch Normalization и почему Layer Normalization предпочтительнее для трансформеров?',
    options: [
      'BatchNorm нормализует по мини-батчу, LayerNorm — по признакам одного примера; LayerNorm не зависит от размера батча и стабильнее при переменной длине последовательностей',
      'BatchNorm работает только при обучении, а LayerNorm — только при инференсе',
      'LayerNorm быстрее вычисляется за счёт меньшего количества параметров',
      'BatchNorm и LayerNorm идентичны, различие только в реализации',
    ],
    correctIndex: 0,
    explanation:
      'BatchNorm: нормализует активации по мини-батчу (по оси batch), вычисляя среднее и дисперсию по всем примерам в батче для каждого признака. Проблемы: зависит от размера батча, при инференсе использует running statistics, нестабилен при переменной длине последовательностей. LayerNorm: нормализует по всем признакам одного примера, не зависит от других примеров в батче. Идеален для трансформеров: работает одинаково при обучении и инференсе, стабилен при любом batch size и длине последовательности.',
  },
];
