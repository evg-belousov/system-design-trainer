import type { Question } from '../types';

export const scalabilityQuestions: Question[] = [
  {
    id: 'sd-scalability-001',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое вертикальное масштабирование (Scale Up)?',
    options: [
      'Добавление новых серверов в кластер для распределения нагрузки',
      'Увеличение ресурсов (CPU, RAM, диск) существующего сервера',
      'Разделение базы данных на несколько шардов',
      'Использование CDN для раздачи статического контента',
    ],
    correctIndex: 1,
    explanation:
      'Вертикальное масштабирование (Scale Up) -- это увеличение мощности одного сервера: добавление процессорных ядер, оперативной памяти, более быстрых дисков. Это самый простой способ масштабирования, но он имеет физический предел -- нельзя бесконечно наращивать ресурсы одной машины. Кроме того, вертикальное масштабирование создаёт единую точку отказа (Single Point of Failure).',
  },
  {
    id: 'sd-scalability-002',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Чем горизонтальное масштабирование отличается от вертикального?',
    options: [
      'Горизонтальное масштабирование дешевле вертикального во всех случаях',
      'Горизонтальное масштабирование предполагает добавление новых серверов, а вертикальное -- увеличение ресурсов существующего',
      'Вертикальное масштабирование требует балансировщика нагрузки, а горизонтальное -- нет',
      'Горизонтальное масштабирование применяется только для баз данных',
    ],
    correctIndex: 1,
    explanation:
      'Горизонтальное масштабирование (Scale Out) -- это добавление новых серверов (нод) в систему для распределения нагрузки. В отличие от вертикального масштабирования, оно теоретически не имеет верхнего предела и обеспечивает отказоустойчивость. Однако горизонтальное масштабирование требует более сложной архитектуры: балансировки нагрузки, синхронизации данных, управления состоянием сессий и т.д.',
  },
  {
    id: 'sd-scalability-003',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'open',
    question: 'Объясните, что такое шардирование (sharding) базы данных и какие стратегии выбора ключа шардирования существуют. Какие проблемы может вызвать неправильный выбор ключа?',
    sampleAnswer:
      'Шардирование -- это разделение данных одной базы на несколько независимых частей (шардов), каждая из которых хранится на отдельном сервере. Основные стратегии: 1) На основе диапазона (range-based) -- данные делятся по диапазонам ключей (например, пользователи A-M на шарде 1, N-Z на шарде 2). Просто реализуется, но может приводить к неравномерному распределению. 2) На основе хэша (hash-based) -- хэш-функция от ключа определяет шард. Обеспечивает равномерное распределение, но усложняет range-запросы. 3) На основе директории (directory-based) -- отдельный сервис хранит маппинг ключей на шарды. Гибкий подход, но директория становится узким местом. Неправильный выбор ключа может привести к «горячим шардам» (hotspots), когда один шард получает непропорционально большую нагрузку, сводя на нет преимущества шардирования.',
    explanation:
      'Шардирование -- ключевая техника горизонтального масштабирования баз данных. Правильный выбор ключа шардирования критически важен: он должен обеспечивать равномерное распределение данных и запросов, минимизировать cross-shard запросы и учитывать паттерны доступа к данным. Например, шардирование по user_id хорошо работает для данных, привязанных к пользователю, но плохо для глобальных аналитических запросов.',
  },
  {
    id: 'sd-scalability-004',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'open',
    question: 'Вы проектируете систему, которая должна обрабатывать 1 миллион запросов в секунду. Опишите стратегию масштабирования на уровне приложения, данных и инфраструктуры. Какие компромиссы вам придётся принять?',
    sampleAnswer:
      'Для обработки 1M RPS потребуется комплексный подход: 1) Уровень приложения: stateless-сервисы за L7 балансировщиком, автоскейлинг на основе метрик (CPU, latency), асинхронная обработка через очереди для не-критичных операций. 2) Уровень данных: агрессивное кэширование (Redis/Memcached) для горячих данных, шардирование БД, read-реплики для чтения, CQRS для разделения чтения и записи, денормализация данных для уменьшения JOIN-ов. 3) Инфраструктура: мульти-регионное развёртывание, CDN для статики, DNS-балансировка между регионами, circuit breakers для защиты от каскадных отказов. Компромиссы: eventual consistency вместо strong consistency, увеличение операционной сложности, стоимость инфраструктуры, сложность отладки распределённой системы, необходимость idempotent-операций.',
    explanation:
      'Масштабирование до 1M RPS -- это задача, требующая системного подхода. Нельзя просто добавить серверов: нужно устранить все узкие места на каждом уровне архитектуры. Ключевые принципы: кэшировать как можно больше, делать обработку асинхронной где возможно, избегать shared state, проектировать для отказов (design for failure). На практике большинство систем такого масштаба используют eventual consistency и разделение на горячий/холодный пути обработки данных.',
  },
  {
    id: 'sd-scalability-005',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что означает закон Амдала в контексте масштабируемости системы?',
    options: [
      'Производительность системы линейно растёт с добавлением серверов',
      'Ускорение системы ограничено долей последовательной (не параллелизуемой) части обработки',
      'Удвоение ресурсов всегда приводит к удвоению производительности',
      'Стоимость масштабирования растёт логарифмически',
    ],
    correctIndex: 1,
    explanation:
      'Закон Амдала утверждает, что максимальное ускорение системы при распараллеливании ограничено долей последовательного кода. Если 10% обработки запроса нельзя распараллелить, то максимальное ускорение не превысит 10x, сколько бы серверов вы ни добавили. Это фундаментальный принцип, который объясняет, почему при масштабировании важно выявлять и оптимизировать узкие места (bottlenecks), а не просто добавлять ресурсы.',
  },
  {
    id: 'sd-scalability-006',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое stateless-сервис и почему он упрощает горизонтальное масштабирование?',
    options: [
      'Сервис без базы данных — все данные хранятся в памяти',
      'Сервис, не хранящий состояние сессии между запросами, что позволяет направить любой запрос на любой экземпляр',
      'Сервис, работающий только с GET-запросами',
      'Сервис, который не использует внешние зависимости',
    ],
    correctIndex: 1,
    explanation:
      'Stateless-сервис не хранит состояние клиентской сессии между запросами. Каждый запрос содержит всю необходимую информацию для обработки (например, через JWT-токен). Это упрощает горизонтальное масштабирование, потому что любой экземпляр сервиса может обработать любой запрос — не нужно привязывать клиента к конкретному серверу (sticky sessions). Состояние при необходимости выносится в внешние хранилища (Redis, база данных).',
  },
  {
    id: 'sd-scalability-007',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какой из следующих компонентов чаще всего становится узким местом (bottleneck) при масштабировании веб-приложения?',
    options: [
      'DNS-сервер',
      'CDN для статических файлов',
      'Реляционная база данных',
      'Балансировщик нагрузки',
    ],
    correctIndex: 2,
    explanation:
      'Реляционная база данных чаще всего становится узким местом при масштабировании, потому что: 1) горизонтальное масштабирование БД значительно сложнее, чем масштабирование stateless-сервисов; 2) ACID-транзакции и блокировки ограничивают параллелизм; 3) JOIN-операции и сложные запросы потребляют много ресурсов; 4) данные должны быть консистентными, что ограничивает возможности распределения. Поэтому большинство стратегий масштабирования начинается с кэширования, read-реплик и денормализации данных.',
  },
  {
    id: 'sd-scalability-008',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое auto-scaling в контексте облачной инфраструктуры?',
    options: [
      'Автоматическое обновление программного обеспечения на серверах',
      'Автоматическое увеличение или уменьшение количества экземпляров сервиса в зависимости от текущей нагрузки',
      'Автоматическое резервное копирование данных',
      'Автоматическая балансировка данных между шардами',
    ],
    correctIndex: 1,
    explanation:
      'Auto-scaling — это механизм облачных платформ (AWS Auto Scaling, Kubernetes HPA), который автоматически изменяет количество экземпляров сервиса на основе метрик: загрузки CPU, количества запросов, длины очереди и т.д. При росте нагрузки добавляются новые экземпляры (scale out), при снижении — лишние удаляются (scale in). Это позволяет оптимизировать стоимость инфраструктуры и обеспечить адекватную производительность.',
  },
  {
    id: 'sd-scalability-009',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какая метрика НЕ является типичным триггером для auto-scaling политик?',
    options: [
      'Средняя загрузка CPU выше 70%',
      'Количество запросов в секунду (RPS)',
      'Размер исходного кода приложения',
      'Средняя задержка ответа (latency) выше порогового значения',
    ],
    correctIndex: 2,
    explanation:
      'Размер исходного кода не имеет отношения к auto-scaling. Типичные метрики для триггеров auto-scaling: загрузка CPU, использование памяти, количество запросов (RPS), задержка ответов (latency), длина очереди сообщений, количество активных соединений. Важно выбирать метрику, которая коррелирует с реальной нагрузкой на сервис, и правильно настраивать пороги и cooldown-период для предотвращения «дребезга» (thrashing) — частого добавления и удаления экземпляров.',
  },
  {
    id: 'sd-scalability-010',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Чем отличается масштабируемость (scalability) от эластичности (elasticity)?',
    options: [
      'Это синонимы, разницы нет',
      'Масштабируемость — способность системы справляться с ростом нагрузки, эластичность — способность автоматически адаптировать ресурсы к текущей нагрузке в реальном времени',
      'Эластичность применяется только к базам данных',
      'Масштабируемость работает только вертикально, эластичность — горизонтально',
    ],
    correctIndex: 1,
    explanation:
      'Масштабируемость (scalability) — это способность системы обрабатывать растущую нагрузку за счёт добавления ресурсов. Эластичность (elasticity) — это способность системы автоматически выделять и освобождать ресурсы в соответствии с текущей нагрузкой. Эластичная система не только масштабируется вверх при пиках, но и сжимается при спаде, оптимизируя затраты. Например, интернет-магазин может быть масштабируемым (выдерживает Black Friday), а эластичным он станет, если автоматически добавляет серверы к распродаже и убирает после неё.',
  },
  {
    id: 'sd-scalability-011',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'open',
    question: 'Что такое capacity planning (планирование ёмкости)? Какие шаги необходимо выполнить для оценки ресурсов, необходимых системе?',
    sampleAnswer:
      'Capacity planning — это процесс определения необходимых ресурсов для обеспечения заданного уровня производительности. Основные шаги: 1) Определить ключевые метрики (RPS, пропускная способность, latency SLA). 2) Оценить текущую и прогнозируемую нагрузку (рост пользователей, сезонные пики). 3) Провести нагрузочное тестирование для определения пропускной способности одного экземпляра. 4) Рассчитать количество экземпляров: required_instances = peak_RPS / single_instance_RPS * safety_factor. 5) Учесть запас на отказоустойчивость (N+2 redundancy). 6) Спланировать ресурсы для зависимостей (БД, кэш, очереди). 7) Пересматривать план регулярно на основе реальных данных мониторинга.',
    explanation:
      'Capacity planning предотвращает ситуации, когда система не выдерживает нагрузку (under-provisioning) или когда ресурсы простаивают (over-provisioning). Обычно закладывают safety margin в 30-50% сверх расчётной нагрузки. Важно учитывать не только средние значения, но и пиковые нагрузки (P99). В облачных средах auto-scaling частично решает проблему, но capacity planning всё равно необходим для бюджетирования и определения архитектурных ограничений.',
  },
  {
    id: 'sd-scalability-012',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какое основное преимущество read-реплик базы данных для масштабирования?',
    options: [
      'Увеличение скорости записи данных',
      'Распределение нагрузки чтения между несколькими копиями БД',
      'Автоматическое шардирование данных',
      'Устранение необходимости в кэшировании',
    ],
    correctIndex: 1,
    explanation:
      'Read-реплики — это копии основной базы данных, которые асинхронно получают обновления от master-узла и обслуживают запросы на чтение. Поскольку большинство приложений имеют соотношение чтения к записи 80/20 или выше, read-реплики позволяют значительно увеличить пропускную способность системы. Однако из-за асинхронной репликации данные на репликах могут быть слегка устаревшими (replication lag), что нужно учитывать при проектировании.',
  },
  {
    id: 'sd-scalability-013',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'open',
    question: 'Опишите основные компромиссы (trade-offs) между вертикальным и горизонтальным масштабированием. В каких ситуациях вертикальное масштабирование может быть предпочтительнее?',
    sampleAnswer:
      'Вертикальное масштабирование: преимущества — простота (не нужно менять архитектуру), нет проблем с распределённой координацией, транзакции и консистентность данных работают «из коробки». Недостатки — физический предел ресурсов одной машины, единая точка отказа, стоимость растёт нелинейно (мощный сервер стоит непропорционально дороже). Горизонтальное масштабирование: преимущества — теоретически неограниченный рост, отказоустойчивость, использование commodity-серверов. Недостатки — сложность архитектуры, необходимость stateless-дизайна, проблемы с распределёнными транзакциями и консистентностью. Вертикальное масштабирование предпочтительнее: на ранних стадиях проекта (проще и быстрее), для монолитных приложений с сильными внутренними связями, для баз данных с интенсивной транзакционной нагрузкой, когда текущая машина далека от предела своих ресурсов.',
    explanation:
      'На практике большинство систем начинают с вертикального масштабирования (проще) и переходят к горизонтальному по мере роста. Многие компании успешно работают на вертикальном масштабировании дольше, чем ожидают: Stack Overflow долгое время обслуживал миллионы пользователей с двух серверов. Преждевременная оптимизация под горизонтальное масштабирование может усложнить систему без реальной необходимости.',
  },
  {
    id: 'sd-scalability-014',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое consistent hashing и какую проблему он решает при масштабировании распределённых систем?',
    options: [
      'Алгоритм шифрования данных для безопасной передачи между узлами',
      'Способ распределения данных по узлам, минимизирующий перераспределение при добавлении или удалении узлов',
      'Метод проверки целостности данных в распределённом кэше',
      'Протокол достижения консенсуса между узлами кластера',
    ],
    correctIndex: 1,
    explanation:
      'Consistent hashing решает проблему массового перераспределения данных при изменении количества узлов. В обычном хэшировании (key % N) при добавлении или удалении узла почти все ключи перемещаются. В consistent hashing узлы и ключи отображаются на кольцо (hash ring), и при изменении количества узлов перемещается только ~1/N часть ключей. Это критически важно для распределённых кэшей (Memcached), хранилищ (DynamoDB, Cassandra) и систем доставки контента. Виртуальные узлы (vnodes) обеспечивают более равномерное распределение нагрузки.',
  },
  {
    id: 'sd-scalability-015',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'open',
    question: 'Объясните концепцию backpressure в масштабируемых системах. Как она помогает предотвратить каскадные отказы?',
    sampleAnswer:
      'Backpressure — это механизм обратного давления, при котором перегруженный компонент системы сигнализирует вышестоящим компонентам о необходимости снизить скорость отправки данных. Без backpressure быстрый продюсер может перегрузить медленного потребителя, вызвав OOM, рост задержек и каскадные отказы. Способы реализации: 1) Ограничение размера очереди — при заполнении буфера продюсер блокируется или получает ошибку. 2) Rate limiting — ограничение количества запросов в единицу времени. 3) Credit-based flow control — потребитель выдаёт «кредиты» продюсеру, определяя сколько сообщений он может принять. 4) Reactive Streams — стандарт асинхронной потоковой обработки с backpressure (реализации: Project Reactor, RxJava, Akka Streams). 5) Circuit breaker — размыкание «цепи» при обнаружении перегрузки. Backpressure помогает предотвратить каскадные отказы, обеспечивая graceful degradation вместо полного падения.',
    explanation:
      'Backpressure — один из ключевых механизмов устойчивости в распределённых системах. Без него система ведёт себя как цепочка: если одно звено замедляется, запросы копятся, потребление памяти растёт, и система падает целиком. С backpressure система деградирует плавно — сбрасывает лишнюю нагрузку на входе, а не позволяет ей проникать вглубь.',
  },
  {
    id: 'sd-scalability-016',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Какой паттерн позволяет разделить модели чтения и записи данных для независимого масштабирования?',
    options: [
      'Saga',
      'CQRS (Command Query Responsibility Segregation)',
      'Circuit Breaker',
      'Service Mesh',
    ],
    correctIndex: 1,
    explanation:
      'CQRS (Command Query Responsibility Segregation) разделяет систему на две части: команды (запись/изменение данных) и запросы (чтение данных). Это позволяет независимо масштабировать чтение и запись, использовать разные модели данных и хранилища для каждой стороны. Например, запись может идти в нормализованную PostgreSQL, а чтение — из денормализованного Elasticsearch. Данные между сторонами синхронизируются через события. CQRS особенно полезен в системах с сильным дисбалансом между чтением и записью.',
  },
  {
    id: 'sd-scalability-017',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'open',
    question: 'Как масштабировать WebSocket-соединения в системе с миллионами одновременных подключений? Какие архитектурные решения необходимы?',
    sampleAnswer:
      'Масштабирование WebSocket-соединений — сложная задача, потому что каждое соединение является stateful (привязано к конкретному серверу). Архитектурные решения: 1) Sticky sessions на балансировщике — направлять клиента на тот же сервер, где установлено его соединение. 2) Pub/Sub для межсерверной коммуникации — если пользователь A подключён к серверу 1, а сообщение пришло на сервер 2, Redis Pub/Sub или Kafka доставят его на нужный сервер. 3) Connection-aware routing — хранить маппинг user → server в Redis для маршрутизации целевых сообщений. 4) Оптимизация ресурсов — использовать epoll/kqueue для обработки множества соединений одним потоком (C10K problem), тюнинг лимитов файловых дескрипторов ОС. 5) Горизонтальное разделение — шардирование пользователей по серверам (например, по chat room или geographic region). 6) Fallback на long polling — для клиентов, не поддерживающих WebSocket.',
    explanation:
      'Системы реального времени вроде Discord (миллионы одновременных пользователей) используют комбинацию этих подходов. Discord, например, разделяет пользователей по гильдиям, каждая гильдия обслуживается определённым сервером. Ключевой принцип — минимизировать cross-server коммуникацию за счёт умного шардирования.',
  },
  {
    id: 'sd-scalability-018',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое CDN и как она помогает масштабированию?',
    options: [
      'Централизованная база данных для хранения пользовательских данных',
      'Сеть распределённых серверов, доставляющих контент с ближайшего к пользователю узла',
      'Специальный протокол для шифрования трафика',
      'Инструмент мониторинга производительности серверов',
    ],
    correctIndex: 1,
    explanation:
      'CDN (Content Delivery Network) — это географически распределённая сеть серверов, которая кэширует и отдаёт контент (изображения, JS/CSS, видео) с узла, ближайшего к пользователю. CDN помогает масштабированию, снижая нагрузку на origin-серверы (основные серверы приложения) и уменьшая задержку для конечных пользователей. Популярные CDN: CloudFlare, AWS CloudFront, Akamai. Современные CDN также поддерживают edge computing — выполнение логики на периферийных узлах.',
  },
  {
    id: 'sd-scalability-019',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какой подход к масштабированию базы данных предполагает разделение данных одной таблицы по строкам между несколькими серверами?',
    options: [
      'Вертикальное партиционирование',
      'Горизонтальное партиционирование (шардирование)',
      'Репликация master-slave',
      'Денормализация',
    ],
    correctIndex: 1,
    explanation:
      'Горизонтальное партиционирование (шардирование) разделяет строки одной таблицы между несколькими серверами по ключу шардирования. Например, пользователи с ID 1-1000000 на шарде 1, 1000001-2000000 на шарде 2. Вертикальное партиционирование, напротив, разделяет таблицу по столбцам (разные группы столбцов на разных серверах). Шардирование позволяет масштабировать и чтение, и запись, но усложняет cross-shard запросы и транзакции.',
  },
  {
    id: 'sd-scalability-020',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'open',
    question: 'Объясните, как работает паттерн «Cell-Based Architecture» для масштабирования. Какие преимущества он даёт по сравнению с классическим горизонтальным масштабированием?',
    sampleAnswer:
      'Cell-Based Architecture — паттерн, при котором инфраструктура делится на изолированные ячейки (cells), каждая из которых является полностью автономной копией системы, обслуживающей подмножество пользователей. Каждая ячейка содержит свои серверы приложений, базы данных, кэши и очереди. Маршрутизатор на входе направляет пользователя в его ячейку (обычно по hash от user_id). Преимущества: 1) Изоляция отказов (blast radius) — проблема в одной ячейке не затрагивает другие. 2) Независимое масштабирование — можно добавлять новые ячейки без изменения существующих. 3) Упрощение деплоя — можно обновлять ячейки по одной (canary deployment на уровне ячеек). 4) Предсказуемая производительность — каждая ячейка обслуживает фиксированное количество пользователей. Недостатки: операционная сложность, дублирование инфраструктуры, сложность cross-cell операций. Этот паттерн используется AWS (availability zones), Slack, и другими компаниями, работающими на масштабе.',
    explanation:
      'Cell-Based Architecture — это эволюция горизонтального масштабирования для систем, где blast radius (радиус поражения при сбое) критически важен. Вместо одного большого кластера, который может упасть целиком, система разделена на независимые ячейки. Даже если одна ячейка полностью недоступна, остальные продолжают работать. AWS использует этот принцип в основе своей архитектуры — каждый регион и availability zone спроектирован как изолированная ячейка.',
  },
  {
    id: 'sd-scalability-021',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое auto-scaling group в облачных платформах?',
    options: [
      'Группа пользователей с правами на масштабирование',
      'Набор виртуальных машин, количество которых автоматически изменяется в зависимости от нагрузки',
      'Группа баз данных, синхронизирующихся автоматически',
      'Набор контейнеров в одном поде Kubernetes',
    ],
    correctIndex: 1,
    explanation:
      'Auto-scaling group (ASG) — это группа идентичных виртуальных машин (EC2 в AWS, VM в GCP/Azure), количество которых автоматически увеличивается или уменьшается на основе заданных правил и метрик. ASG обеспечивает: минимальное и максимальное количество инстансов, автоматическую замену нездоровых инстансов, распределение по зонам доступности для отказоустойчивости, интеграцию с балансировщиком нагрузки.',
  },
  {
    id: 'sd-scalability-022',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какой тип масштабирования предпочтителен для stateful-приложений?',
    options: [
      'Горизонтальное масштабирование всегда лучше',
      'Вертикальное масштабирование, так как оно не требует синхронизации состояния между узлами',
      'Диагональное масштабирование',
      'Масштабирование невозможно для stateful-приложений',
    ],
    correctIndex: 1,
    explanation:
      'Для stateful-приложений вертикальное масштабирование часто предпочтительнее, потому что при горизонтальном масштабировании необходимо решать проблему синхронизации состояния между узлами. Это может требовать sticky sessions, распределённого хранения состояния или перепроектирования приложения. Однако лучшая практика — делать приложения stateless, вынося состояние во внешние хранилища (Redis, базы данных).',
  },
  {
    id: 'sd-scalability-023',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое cooldown period в контексте auto-scaling?',
    options: [
      'Период охлаждения серверов для предотвращения перегрева',
      'Время ожидания после масштабирования, в течение которого новые действия по масштабированию блокируются',
      'Время запуска нового инстанса',
      'Период, когда серверы работают с минимальной нагрузкой',
    ],
    correctIndex: 1,
    explanation:
      'Cooldown period — это временной интервал после действия масштабирования (scale out или scale in), в течение которого auto-scaler не инициирует новые действия. Это предотвращает «дребезг» (thrashing) — ситуацию, когда система постоянно добавляет и удаляет инстансы из-за колебаний метрик. Типичные значения cooldown: 3-5 минут.',
  },
  {
    id: 'sd-scalability-024',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'junior',
    type: 'open',
    question: 'Объясните разницу между реактивным и предиктивным (proactive) auto-scaling. Когда применяется каждый подход?',
    sampleAnswer:
      'Реактивное auto-scaling срабатывает в ответ на текущие метрики: когда CPU превышает 70%, добавляются инстансы. Это простой подход, но он имеет задержку — пока новые инстансы запустятся и прогреются, система может быть перегружена. Предиктивное (proactive) auto-scaling использует машинное обучение для анализа исторических паттернов нагрузки и масштабирует заранее. Например, если каждый понедельник в 9:00 нагрузка растёт, система добавит инстансы к 8:50. AWS Predictive Scaling анализирует данные за 14 дней. Реактивное масштабирование подходит для непредсказуемой нагрузки, предиктивное — для регулярных паттернов (рабочие часы, сезонные пики).',
    explanation:
      'На практике комбинируют оба подхода: предиктивное масштабирование обеспечивает базовую ёмкость для ожидаемой нагрузки, а реактивное — справляется с неожиданными всплесками. Это обеспечивает и экономию ресурсов, и надёжность.',
  },
  {
    id: 'sd-scalability-025',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какая стратегия масштабирования базы данных увеличивает пропускную способность чтения без изменения схемы данных?',
    options: [
      'Шардирование по ключу',
      'Добавление read-реплик',
      'Вертикальное партиционирование',
      'Денормализация таблиц',
    ],
    correctIndex: 1,
    explanation:
      'Read-реплики — самый простой способ масштабировать чтение. Данные асинхронно реплицируются с master-узла на одну или несколько реплик, которые обслуживают запросы на чтение. Это не требует изменения схемы данных или приложения (только routing запросов). Ограничения: не масштабирует запись, возможен replication lag (устаревшие данные на репликах).',
  },
  {
    id: 'sd-scalability-026',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое connection pooling на уровне приложения и как он помогает масштабированию БД?',
    options: [
      'Шифрование соединений между приложением и БД',
      'Повторное использование соединений к БД вместо создания нового для каждого запроса',
      'Распределение соединений между несколькими базами данных',
      'Автоматическое переключение на резервную БД',
    ],
    correctIndex: 1,
    explanation:
      'Connection pooling — механизм, при котором приложение поддерживает пул готовых соединений к БД и переиспользует их для запросов. Создание TCP-соединения и аутентификация в БД — дорогие операции (десятки миллисекунд). Пул позволяет: снизить задержку запросов, уменьшить нагрузку на БД (меньше соединений), контролировать максимальное количество соединений. Инструменты: HikariCP, PgBouncer, ProxySQL.',
  },
  {
    id: 'sd-scalability-027',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'open',
    question: 'Как масштабировать систему, которая активно использует JOIN-операции между большими таблицами? Какие архитектурные изменения могут потребоваться?',
    sampleAnswer:
      'JOIN-операции плохо масштабируются горизонтально, потому что данные для JOIN могут находиться на разных шардах. Стратегии: 1) Денормализация — хранить связанные данные вместе, устраняя необходимость JOIN. 2) Materialized views — предвычислять и кэшировать результаты частых JOIN-ов. 3) Co-location — шардировать связанные таблицы по одному ключу, чтобы данные для JOIN были на одном шарде. 4) CQRS — разделить модели чтения и записи; read-модель денормализована для быстрых запросов без JOIN. 5) Архитектурное разделение — вынести аналитические запросы с JOIN в отдельную реплику или data warehouse (Snowflake, BigQuery). 6) Кэширование результатов — для редко меняющихся данных кэшировать результаты сложных запросов.',
    explanation:
      'Многие компании при масштабировании переходят от реляционной модели с JOIN к денормализованным структурам или документным БД. Это trade-off: усложнение записи и потенциальные аномалии обновления в обмен на масштабируемое чтение. Важно анализировать реальные паттерны запросов, а не денормализовывать преждевременно.',
  },
  {
    id: 'sd-scalability-028',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое re-sharding и почему это сложная операция?',
    options: [
      'Создание резервной копии шардированной базы данных',
      'Перераспределение данных между шардами при изменении их количества или стратегии шардирования',
      'Удаление устаревших шардов',
      'Синхронизация данных между шардами',
    ],
    correctIndex: 1,
    explanation:
      'Re-sharding — перераспределение данных при изменении количества шардов или ключа шардирования. Это сложно, потому что: 1) Нужно переместить огромные объёмы данных без остановки системы. 2) Во время миграции данные могут быть на старом и новом месте — нужна корректная маршрутизация. 3) Риск потери данных или рассогласованности. 4) Влияние на производительность во время миграции. Consistent hashing минимизирует объём перемещаемых данных. Vitess, Citus и другие решения автоматизируют re-sharding для PostgreSQL и MySQL.',
  },
  {
    id: 'sd-scalability-029',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какой подход к масштабированию WebSocket-соединений использует publish-subscribe для доставки сообщений между серверами?',
    options: [
      'Sticky sessions с балансировкой по IP',
      'Redis Pub/Sub или аналогичная message bus для межсерверной коммуникации',
      'Хранение всех соединений в базе данных',
      'Периодический polling вместо WebSocket',
    ],
    correctIndex: 1,
    explanation:
      'При горизонтальном масштабировании WebSocket-серверов пользователи подключаются к разным серверам. Если пользователь A на сервере 1 отправляет сообщение пользователю B на сервере 2, нужен механизм межсерверной доставки. Redis Pub/Sub (или Kafka, NATS) решает эту задачу: каждый сервер подписывается на канал и получает сообщения, адресованные его клиентам. Это паттерн используется в Socket.IO с адаптером Redis.',
  },
  {
    id: 'sd-scalability-030',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'open',
    question: 'Что такое edge computing и как оно помогает масштабированию глобальных приложений?',
    sampleAnswer:
      'Edge computing — это выполнение вычислений на серверах, географически близких к пользователям (edge locations), вместо центрального дата-центра. Преимущества: 1) Снижение latency — данные обрабатываются ближе к пользователю (миллисекунды вместо сотен миллисекунд). 2) Разгрузка origin-серверов — часть логики выполняется на edge. 3) Персонализация по региону — A/B-тесты, локализация. 4) Обработка данных IoT на месте — фильтрация и агрегация до отправки в облако. Примеры: Cloudflare Workers, AWS Lambda@Edge, Vercel Edge Functions. Ограничения: ограниченные ресурсы на edge, сложность синхронизации состояния, не все задачи подходят для edge.',
    explanation:
      'Edge computing — следующий шаг после CDN. Если CDN только кэширует статику, edge computing позволяет выполнять произвольную логику: авторизацию, редиректы, модификацию запросов, даже рендеринг страниц. Vercel и Cloudflare активно продвигают edge-first архитектуру для веб-приложений.',
  },
  {
    id: 'sd-scalability-031',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое target tracking scaling policy в AWS Auto Scaling?',
    options: [
      'Политика, которая отслеживает целевой сервер для репликации',
      'Политика, которая автоматически поддерживает заданное значение метрики (например, CPU 50%)',
      'Политика, которая масштабирует до заданного количества инстансов',
      'Политика, которая отслеживает затраты и оптимизирует бюджет',
    ],
    correctIndex: 1,
    explanation:
      'Target tracking scaling policy — наиболее простой и эффективный тип политики auto-scaling. Вы задаёте целевое значение метрики (например, средняя загрузка CPU 50%), и AWS автоматически добавляет или удаляет инстансы для поддержания этого значения. Преимущества: не нужно настраивать пороги scale-out и scale-in отдельно, система сама адаптируется. Поддерживаемые метрики: CPU, request count per target, custom CloudWatch metrics.',
  },
  {
    id: 'sd-scalability-032',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Какой механизм Kubernetes используется для автоматического масштабирования подов на основе метрик?',
    options: [
      'ReplicaSet',
      'Horizontal Pod Autoscaler (HPA)',
      'Deployment Controller',
      'Service Mesh',
    ],
    correctIndex: 1,
    explanation:
      'Horizontal Pod Autoscaler (HPA) — контроллер Kubernetes, который автоматически изменяет количество реплик пода на основе метрик. По умолчанию использует CPU и память, но с Metrics Server или Prometheus Adapter можно использовать кастомные метрики (RPS, queue length). HPA периодически (по умолчанию 15 сек) проверяет метрики и корректирует количество реплик. Также существует VPA (Vertical Pod Autoscaler) для вертикального масштабирования и KEDA для event-driven масштабирования.',
  },
  {
    id: 'sd-scalability-033',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'open',
    question: 'Опишите стратегию масштабирования системы для глобальной аудитории (multi-region deployment). Какие компоненты необходимо рассмотреть?',
    sampleAnswer:
      'Multi-region deployment требует комплексного подхода: 1) DNS и глобальная маршрутизация — GeoDNS (Route53, Cloudflare) направляет пользователей в ближайший регион; latency-based routing для оптимизации по задержке; failover при недоступности региона. 2) CDN — edge-кэширование статики и динамического контента во всех регионах. 3) Данные — синхронная репликация внутри региона, асинхронная между регионами (conflict resolution, eventual consistency); глобальные БД (CockroachDB, Spanner) или regional databases с CDC. 4) Приложение — stateless сервисы в каждом регионе; regional caches (Redis); асинхронная коммуникация через глобальный Kafka/EventBridge. 5) Routing logic — определение «домашнего» региона пользователя; follow-the-sun для некоторых операций. Компромиссы: стоимость (инфраструктура в каждом регионе), сложность операций, eventual consistency.',
    explanation:
      'Multi-region — это не просто копирование инфраструктуры. Нужно решить, какие данные реплицировать глобально, какие хранить регионально, как обрабатывать конфликты при записи в разные регионы. Многие компании начинают с active-passive (один регион активный, другой для DR), затем переходят к active-active.',
  },
  {
    id: 'sd-scalability-034',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое database proxy (например, ProxySQL, PgBouncer) и как он помогает масштабированию?',
    options: [
      'Инструмент для шифрования трафика к БД',
      'Промежуточный слой между приложением и БД для connection pooling, routing и балансировки',
      'Резервная копия базы данных',
      'Инструмент мониторинга запросов к БД',
    ],
    correctIndex: 1,
    explanation:
      'Database proxy — промежуточный слой, который обеспечивает: 1) Connection pooling — мультиплексирование множества клиентских соединений в меньшее количество соединений к БД. 2) Read/write splitting — автоматическая маршрутизация SELECT на реплики, INSERT/UPDATE/DELETE на master. 3) Load balancing — распределение нагрузки между репликами. 4) Query routing — маршрутизация по шардам при шардированной БД. 5) Failover — автоматическое переключение при отказе master. PgBouncer (PostgreSQL), ProxySQL (MySQL), Vitess (MySQL sharding).',
  },
  {
    id: 'sd-scalability-035',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'open',
    question: 'Как реализовать graceful degradation при масштабировании? Какие паттерны помогают системе оставаться работоспособной под экстремальной нагрузкой?',
    sampleAnswer:
      'Graceful degradation — способность системы продолжать работу с ограниченной функциональностью вместо полного отказа. Паттерны: 1) Circuit Breaker — при отказе зависимости «размыкает цепь» и возвращает fallback-ответ, предотвращая каскадный отказ. 2) Bulkhead — изоляция ресурсов (thread pools, connections) между компонентами; отказ одного не влияет на другие. 3) Rate Limiting + Load Shedding — ограничение запросов; при перегрузке отклонять низкоприоритетные запросы с 503 вместо деградации для всех. 4) Feature flags — отключение тяжёлых функций (рекомендации, персонализация) при пиках. 5) Timeout + Retry with backoff — предотвращение зависания на медленных зависимостях. 6) Queue-based load leveling — буферизация запросов в очереди вместо прямой обработки. 7) Static fallback — возврат кэшированных или статических данных при недоступности динамических.',
    explanation:
      'Netflix — пионер graceful degradation. Их Hystrix (сейчас Resilience4j) реализует circuit breaker и bulkhead. При отказе сервиса рекомендаций пользователь видит популярные фильмы вместо персонализированных — это лучше, чем ошибка. Ключевой принцип: определить критический путь (critical path) и защитить его.',
  },
  {
    id: 'sd-scalability-036',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое single point of failure (SPOF) и почему его важно устранять при масштабировании?',
    options: [
      'Единственная точка входа в систему для пользователей',
      'Компонент системы, отказ которого приводит к отказу всей системы',
      'Главный сервер, обрабатывающий все запросы',
      'База данных, хранящая критически важные данные',
    ],
    correctIndex: 1,
    explanation:
      'Single Point of Failure (SPOF) — компонент, отказ которого делает всю систему недоступной. Примеры SPOF: единственный сервер БД, один балансировщик нагрузки, единственный DNS-сервер. Устранение SPOF: репликация (master-slave, multi-master), кластеризация, географическое распределение, резервирование (N+1, N+2). При масштабировании важно не создавать новых SPOF и устранять существующие.',
  },
  {
    id: 'sd-scalability-037',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какой компонент помогает распределять статический контент по географически распределённым серверам?',
    options: [
      'Load Balancer',
      'Message Queue',
      'CDN (Content Delivery Network)',
      'API Gateway',
    ],
    correctIndex: 2,
    explanation:
      'CDN (Content Delivery Network) — сеть географически распределённых серверов, которая кэширует и отдаёт статический контент (изображения, CSS, JS, видео) с узла, ближайшего к пользователю. CDN снижает latency (контент ближе к пользователю), разгружает origin-серверы (меньше запросов к основной инфраструктуре), повышает отказоустойчивость. Популярные CDN: CloudFlare, AWS CloudFront, Akamai, Fastly.',
  },
  {
    id: 'sd-scalability-038',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое shard key и какие критерии важны при его выборе?',
    options: [
      'Ключ шифрования для защиты данных на шарде',
      'Поле, по которому данные распределяются между шардами; должен обеспечивать равномерное распределение и соответствовать паттернам запросов',
      'Уникальный идентификатор шарда в кластере',
      'Внешний ключ для связи таблиц на разных шардах',
    ],
    correctIndex: 1,
    explanation:
      'Shard key — поле (или комбинация полей), по которому данные распределяются между шардами. Критерии выбора: 1) Высокая кардинальность — много уникальных значений для равномерного распределения. 2) Равномерное распределение — избегать hotspots (например, timestamp как ключ создаст hotspot на последнем шарде). 3) Соответствие запросам — данные, часто запрашиваемые вместе, должны быть на одном шарде. 4) Неизменяемость — изменение shard key требует перемещения данных.',
  },
  {
    id: 'sd-scalability-039',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'open',
    question: 'Как масштабировать batch-обработку больших объёмов данных? Какие архитектурные паттерны применяются?',
    sampleAnswer:
      'Масштабирование batch-обработки: 1) Партиционирование данных — разбить входные данные на независимые части (по дате, региону, хэшу ID) для параллельной обработки. 2) Map-Reduce — классический паттерн: map-фаза обрабатывает данные параллельно, reduce-фаза агрегирует результаты. Реализации: Hadoop, Spark. 3) Serverless batch — AWS Batch, Google Cloud Dataflow; автоматическое масштабирование workers под объём данных. 4) Queue-based processing — данные в очереди (SQS, Kafka), множество workers обрабатывают параллельно. 5) Checkpointing — сохранение прогресса для возобновления при сбое без повторной обработки с начала. 6) Incremental processing — обрабатывать только изменения (CDC) вместо полного dataset. 7) Data locality — перемещать вычисления к данным, а не наоборот.',
    explanation:
      'Batch-обработка отличается от online-систем: latency менее критична, важнее throughput и надёжность. Современный тренд — переход от чистого batch к micro-batch (Spark Streaming) или streaming-first (Kafka Streams, Flink) с batch как частным случаем.',
  },
  {
    id: 'sd-scalability-040',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое thundering herd problem при масштабировании?',
    options: [
      'Проблема перегрева серверов при высокой нагрузке',
      'Ситуация, когда множество процессов одновременно пытаются получить ресурс, создавая пиковую нагрузку',
      'Медленное распространение изменений между серверами',
      'Неравномерное распределение данных между шардами',
    ],
    correctIndex: 1,
    explanation:
      'Thundering herd — ситуация, когда множество клиентов или процессов одновременно пытаются получить один ресурс. Примеры: истечение TTL популярного ключа в кэше (все запросы идут в БД), восстановление сервиса после сбоя (все клиенты переподключаются), разблокировка mutex (все ожидающие потоки активируются). Решения: jitter (случайная задержка), request coalescing (объединение одинаковых запросов), exponential backoff, semaphores вместо mutex.',
  },
  {
    id: 'sd-scalability-041',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'open',
    question: 'Как организовать масштабирование системы аутентификации для миллионов пользователей? Какие компоненты и паттерны необходимы?',
    sampleAnswer:
      'Масштабирование аутентификации: 1) Stateless токены (JWT) — после аутентификации выдаётся подписанный токен, который валидируется без обращения к БД. Это устраняет необходимость централизованного хранения сессий. 2) Распределённый кэш сессий — если нужны server-side sessions, использовать Redis Cluster с репликацией. 3) Read-реплики БД пользователей — аутентификация в основном читает (проверка пароля), реплики разгружают master. 4) Rate limiting — защита от brute-force; distributed rate limiter на Redis. 5) Кэширование user lookup — кэшировать данные пользователя после успешной аутентификации. 6) Асинхронные операции — логирование аудита, отправка уведомлений через очереди. 7) Региональное развёртывание — auth-сервис в каждом регионе с репликацией пользовательских данных. 8) Token refresh — короткоживущие access tokens + долгоживущие refresh tokens для уменьшения нагрузки.',
    explanation:
      'Системы аутентификации критичны: любой сбой блокирует всех пользователей. Auth0, Okta и подобные сервисы обрабатывают миллиарды аутентификаций, используя все эти паттерны. Важно также учитывать безопасность: защита от timing attacks, хранение паролей (bcrypt, Argon2), MFA.',
  },
  {
    id: 'sd-scalability-042',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Какой паттерн позволяет независимо масштабировать команды разработчиков вместе с их сервисами?',
    options: [
      'Monolithic Architecture',
      'Domain-Driven Design с Bounded Contexts и микросервисами',
      'Shared Database Pattern',
      'Distributed Monolith',
    ],
    correctIndex: 1,
    explanation:
      'Domain-Driven Design (DDD) с Bounded Contexts и микросервисами позволяет масштабировать организацию вместе с системой. Каждая команда владеет одним или несколькими bounded contexts (бизнес-доменами), реализованными как независимые сервисы. Команды могут деплоить, масштабировать и развивать свои сервисы независимо. Это применение закона Конвея: архитектура системы отражает структуру организации. Spotify Squads, Amazon Two-Pizza Teams — примеры такого подхода.',
  },
  {
    id: 'sd-scalability-043',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'open',
    question: 'Объясните концепцию data locality и как она влияет на масштабирование распределённых систем.',
    sampleAnswer:
      'Data locality — принцип, при котором вычисления выполняются там, где находятся данные, а не наоборот. Перемещение данных через сеть — дорогая операция (latency, bandwidth). Применения: 1) Hadoop/Spark — задачи распределяются на узлы, где хранятся нужные блоки данных. 2) Шардирование — запросы маршрутизируются на шард с данными пользователя. 3) Edge computing — обработка IoT-данных на месте их генерации. 4) Кэширование — хранение горячих данных ближе к месту использования. 5) Co-location — размещение связанных сервисов в одном дата-центре/регионе. Влияние на масштабирование: правильная data locality позволяет добавлять узлы без роста сетевого трафика. При плохой locality сеть становится bottleneck, ограничивая масштабирование.',
    explanation:
      'Data locality — один из фундаментальных принципов распределённых систем. Google MapReduce, Hadoop, Spark построены вокруг этой идеи. При проектировании шардирования или микросервисов важно группировать данные, которые обрабатываются вместе.',
  },
  {
    id: 'sd-scalability-044',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое load leveling и как очереди сообщений помогают в этом?',
    options: [
      'Балансировка нагрузки между серверами',
      'Выравнивание пиков нагрузки за счёт буферизации запросов в очереди',
      'Распределение данных между шардами',
      'Уменьшение размера передаваемых данных',
    ],
    correctIndex: 1,
    explanation:
      'Load leveling — сглаживание пиков нагрузки с помощью очередей сообщений. Вместо синхронной обработки запросы помещаются в очередь, и workers обрабатывают их с постоянной скоростью. При всплеске запросов очередь растёт, но серверы не перегружаются. Это позволяет: избежать отказов при пиках, использовать меньше серверов (ориентируясь на среднюю, а не пиковую нагрузку), обеспечить предсказуемую производительность.',
  },
  {
    id: 'sd-scalability-045',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какой подход к масштабированию позволяет запускать код без управления серверами?',
    options: [
      'Bare Metal Servers',
      'Virtual Machines',
      'Serverless / FaaS (Function as a Service)',
      'Container Orchestration',
    ],
    correctIndex: 2,
    explanation:
      'Serverless / FaaS (AWS Lambda, Google Cloud Functions, Azure Functions) позволяет запускать код без управления серверами. Платформа автоматически масштабирует количество экземпляров функции от нуля до тысяч в зависимости от нагрузки. Преимущества: автоматическое масштабирование, оплата за фактическое использование, нет операционных затрат на инфраструктуру. Ограничения: cold start latency, лимиты на время выполнения, stateless-модель, vendor lock-in.',
  },
  {
    id: 'sd-scalability-046',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'middle',
    type: 'open',
    question: 'Как масштабировать систему поиска для миллионов документов? Какие архитектурные компоненты необходимы?',
    sampleAnswer:
      'Масштабирование поиска: 1) Распределённый поисковый движок (Elasticsearch, Solr) — данные шардируются между узлами, каждый шард обрабатывает свою часть запроса, результаты объединяются координатором. 2) Репликация шардов — каждый шард имеет реплики для отказоустойчивости и распределения нагрузки чтения. 3) Индексация отдельно от поиска — bulk indexing в off-peak часы или асинхронно через очередь. 4) Кэширование запросов — популярные запросы кэшируются (Elasticsearch query cache, application-level cache). 5) Tiered storage — горячие данные на SSD, холодные на HDD или архивируются. 6) Query routing — направление специфичных запросов (по дате, региону) на конкретные шарды. 7) Pre-computed aggregations — материализованные представления для частых агрегаций.',
    explanation:
      'Elasticsearch используется компаниями вроде Uber, Netflix, GitHub для поиска на масштабе. Ключевой принцип — разделение indexing throughput и search latency: это разные метрики с разными требованиями. При росте часто выделяют отдельные кластеры для indexing и search.',
  },
  {
    id: 'sd-scalability-047',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое data partitioning на уровне времени (time-based partitioning) и когда оно применяется?',
    options: [
      'Разделение данных по временным зонам пользователей',
      'Разделение данных по временным периодам для оптимизации запросов к временным рядам и упрощения архивирования',
      'Синхронизация данных между серверами в разное время',
      'Кэширование данных на определённое время',
    ],
    correctIndex: 1,
    explanation:
      'Time-based partitioning — разделение данных по временным периодам (день, неделя, месяц). Каждая партиция содержит данные за свой период. Преимущества: 1) Оптимизация запросов — запросы за период обращаются только к нужным партициям. 2) Простое архивирование — старые партиции можно переместить на дешёвое хранилище или удалить. 3) Равномерный рост — каждый период примерно одинакового размера. Применяется для: логов, метрик, временных рядов (IoT), событий, транзакций. PostgreSQL table partitioning, TimescaleDB hypertables реализуют этот паттерн.',
  },
  {
    id: 'sd-scalability-048',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'open',
    question: 'Как масштабировать систему нотификаций (push notifications, email, SMS) для миллионов пользователей?',
    sampleAnswer:
      'Масштабирование нотификаций: 1) Очереди сообщений — события триггерят нотификации через Kafka/SQS; отделение генерации от отправки. 2) Приоритетные очереди — критичные нотификации (OTP, безопасность) в высокоприоритетной очереди. 3) Rate limiting — соблюдение лимитов провайдеров (APNS, FCM, SMS-gateway); очередь с контролем скорости отправки. 4) Батчинг — группировка нотификаций одному пользователю, FCM topic messaging для массовых рассылок. 5) Failover между провайдерами — несколько SMS-провайдеров, переключение при отказе. 6) Шардирование по типу канала — отдельные воркеры для push, email, SMS со своими характеристиками. 7) User preferences service — проверка настроек пользователя (отписки, расписание «не беспокоить»). 8) Delivery tracking — отслеживание статуса доставки, повторные попытки.',
    explanation:
      'Системы нотификаций — классический пример event-driven архитектуры. Discord, Slack обрабатывают миллионы нотификаций в секунду. Важно учитывать специфику каждого канала: push имеет ограничения на payload, email — на rate, SMS — на стоимость.',
  },
  {
    id: 'sd-scalability-049',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'junior',
    type: 'open',
    question: 'Что такое scaling cube и какие три измерения масштабирования он описывает?',
    sampleAnswer:
      'Scaling Cube (куб масштабирования) — модель, описывающая три ортогональных способа масштабирования приложения: 1) X-axis (горизонтальное клонирование) — запуск нескольких идентичных копий приложения за балансировщиком нагрузки. Каждая копия может обработать любой запрос. Самый простой способ масштабирования. 2) Y-axis (функциональная декомпозиция) — разделение приложения на сервисы по функциям (микросервисы). Каждый сервис отвечает за свою часть бизнес-логики и масштабируется независимо. 3) Z-axis (шардирование данных) — разделение данных между экземплярами по ключу (user_id, region). Каждый экземпляр обрабатывает только свой сегмент данных. Комбинация всех трёх измерений обеспечивает максимальную масштабируемость.',
    explanation:
      'Scaling Cube — полезная модель из книги «The Art of Scalability». На практике большинство систем используют все три измерения: X-axis для stateless-сервисов, Y-axis для разделения на микросервисы, Z-axis для шардирования баз данных.',
  },
  {
    id: 'sd-scalability-050',
    block: 'sd',
    topic: 'scalability',
    topicLabel: 'Масштабируемость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Какой паттерн обеспечивает масштабирование при обработке запросов, требующих данных из нескольких шардов?',
    options: [
      'Scatter-Gather — параллельная отправка запросов на все шарды и агрегация результатов',
      'Синхронная последовательная обработка каждого шарда',
      'Кэширование всех данных на каждом шарде',
      'Репликация данных между всеми шардами',
    ],
    correctIndex: 0,
    explanation:
      'Scatter-Gather — паттерн, при котором координатор параллельно отправляет запрос на все релевантные шарды (scatter), затем собирает и агрегирует ответы (gather). Это позволяет масштабировать cross-shard запросы: время выполнения ≈ время самого медленного шарда, а не сумма времён. Используется в Elasticsearch (query phase), MongoDB (sharded queries), и других распределённых системах. Ограничения: overhead на координацию, проблемы с глубокой пагинацией (каждый шард возвращает top N).',
  },
];
