import type { Question } from '../types';

export const capTheoremQuestions: Question[] = [
  {
    id: 'sd-cap-001',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что утверждает CAP-теорема?',
    options: [
      'Распределённая система может одновременно обеспечить высокую скорость, безопасность и масштабируемость',
      'Распределённая система может одновременно обеспечить только два из трёх свойств: консистентность, доступность и устойчивость к разделению сети',
      'Любая система должна быть спроектирована с учётом C (Caching), A (API) и P (Performance)',
      'Консистентность данных всегда важнее доступности системы',
    ],
    correctIndex: 1,
    explanation:
      'CAP-теорема (теорема Брюера) утверждает, что распределённая система не может одновременно обеспечить все три свойства: Consistency (консистентность -- все узлы видят одинаковые данные), Availability (доступность -- каждый запрос получает ответ), Partition Tolerance (устойчивость к разделению -- система работает при разрыве связи между узлами). Поскольку сетевые разделения в распределённых системах неизбежны, на практике выбор стоит между CP (консистентность + устойчивость) и AP (доступность + устойчивость).',
  },
  {
    id: 'sd-cap-002',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'open',
    question: 'Приведите примеры CP и AP систем. Объясните, почему они сделали именно такой выбор.',
    sampleAnswer:
      'CP-системы (Consistency + Partition Tolerance): 1) Apache ZooKeeper -- координационный сервис для распределённых систем. Гарантирует линейную консистентность: все клиенты видят одинаковое состояние. При сетевом разделении меньшинство узлов перестаёт обслуживать запросы (жертвует доступностью). Это критично, потому что ZooKeeper используется для выбора лидера и распределённых блокировок, где консистентность обязательна. 2) MongoDB (с настройкой majority write concern) -- обеспечивает строгую консистентность записей, подтверждённых большинством реплик. AP-системы (Availability + Partition Tolerance): 1) Cassandra -- при сетевом разделении продолжает обслуживать запросы на всех доступных узлах, даже если данные могут быть временно рассинхронизированы. Это подходит для систем, где доступность критичнее точности (лента новостей, счётчики). 2) DynamoDB -- по умолчанию обеспечивает eventual consistency с опцией strongly consistent reads. Выбор AP обусловлен требованием Amazon к 99.99% доступности.',
    explanation:
      'Важно понимать, что CAP-теорема описывает поведение системы только при наличии сетевого разделения. В нормальном режиме работы система может обеспечивать все три свойства. Также многие современные системы позволяют настраивать баланс между C и A на уровне отдельных операций (tunable consistency). Например, Cassandra позволяет задать уровень консистентности (ONE, QUORUM, ALL) для каждого запроса.',
  },
  {
    id: 'sd-cap-003',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'open',
    question: 'Какие ограничения и критика существуют у CAP-теоремы? Что такое PACELC как её расширение?',
    sampleAnswer:
      'Критика CAP-теоремы: 1) Бинарность: CAP представляет свойства как бинарные (есть/нет), но на практике существует спектр уровней консистентности (strong, eventual, causal, read-your-writes) и доступности (99.9%, 99.99%). 2) Редкость разделений: сетевые разделения случаются редко, а CAP ничего не говорит о компромиссах в нормальном режиме. 3) Не учитывает задержку (latency): даже без разделения, строгая консистентность требует координации между узлами, что увеличивает задержку. PACELC -- расширение CAP: при разделении (P) выбирай между доступностью (A) и консистентностью (C), иначе (E -- else) выбирай между задержкой (L -- Latency) и консистентностью (C). Например: Cassandra -- PA/EL (при разделении выбирает доступность, без разделения -- низкую задержку). MongoDB -- PC/EC (при разделении выбирает консистентность, без разделения тоже консистентность с более высокой задержкой). DynamoDB -- PA/EL по умолчанию, PC/EC для consistent reads.',
    explanation:
      'PACELC, предложенная Дэниелом Абади, даёт более точную модель для описания поведения распределённых систем. Она объясняет, почему Cassandra и DynamoDB (обе AP по CAP) ведут себя по-разному в нормальном режиме: Cassandra оптимизирована для низкой задержки (EL), а системы вроде Spanner оптимизированы для строгой консистентности (EC) даже без разделений, но ценой более высокой задержки.',
  },
  {
    id: 'sd-cap-004',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое eventual consistency (согласованность в конечном счёте)?',
    options: [
      'Данные всегда консистентны во всех узлах в любой момент времени',
      'При отсутствии новых обновлений все реплики со временем придут к одинаковому состоянию данных',
      'Данные никогда не становятся консистентными в распределённой системе',
      'Консистентность достигается только после перезапуска всех узлов системы',
    ],
    correctIndex: 1,
    explanation:
      'Eventual consistency -- это модель консистентности, при которой гарантируется, что если прекратить обновления данных, то через некоторое время все реплики придут к одинаковому состоянию. В промежутке между обновлением и полной синхронизацией разные узлы могут возвращать разные версии данных. Эта модель используется в AP-системах (DynamoDB, Cassandra, DNS). Она обеспечивает высокую доступность и низкую задержку, но требует от приложения умения работать с потенциально устаревшими данными.',
  },
  {
    id: 'sd-cap-005',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что означает буква "P" (Partition Tolerance) в CAP-теореме?',
    options: [
      'Система может разделять данные между таблицами (partitioning)',
      'Система продолжает работать, даже если связь между некоторыми узлами сети нарушена',
      'Система поддерживает параллельную обработку запросов',
      'Система обеспечивает защиту данных от несанкционированного доступа',
    ],
    correctIndex: 1,
    explanation:
      'Partition Tolerance (устойчивость к разделению сети) означает, что система продолжает функционировать, даже если сетевое соединение между некоторыми узлами потеряно (network partition). В распределённых системах сетевые разделения неизбежны (кабель обрезали, switch вышел из строя, дата-центр стал недоступен), поэтому P -- обязательное свойство для любой распределённой системы. Вопрос на практике стоит так: что делать при разделении -- жертвовать консистентностью (AP) или доступностью (CP)?',
  },
  {
    id: 'sd-cap-006',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какая система является примером CP-системы (Consistency + Partition Tolerance)?',
    options: [
      'Apache Cassandra',
      'DNS',
      'Apache ZooKeeper',
      'Amazon DynamoDB (в режиме по умолчанию)',
    ],
    correctIndex: 2,
    explanation:
      'Apache ZooKeeper -- классический пример CP-системы. Он используется для координации распределённых систем (выбор лидера, распределённые блокировки, хранение конфигурации), где консистентность критически важна. При сетевом разделении узлы, оказавшиеся в меньшинстве, перестают обслуживать запросы (жертвуют доступностью), чтобы гарантировать, что все клиенты видят одинаковое состояние. Cassandra и DynamoDB -- примеры AP-систем, DNS -- также AP (разные серверы могут возвращать разные записи во время обновления).',
  },
  {
    id: 'sd-cap-007',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Почему в распределённой системе невозможно отказаться от Partition Tolerance (P)?',
    options: [
      'Потому что это самое дешёвое свойство для реализации',
      'Потому что сетевые сбои в распределённых системах неизбежны и система должна уметь с ними справляться',
      'Потому что Partition Tolerance улучшает производительность',
      'Потому что без P невозможно реализовать шифрование данных',
    ],
    correctIndex: 1,
    explanation:
      'В реальном мире сетевые разделения (network partitions) неизбежны: оборудование выходит из строя, кабели повреждаются, маршрутизаторы перегружаются, дата-центры теряют связь. Система, не устойчивая к разделениям, перестаёт работать при любом сетевом сбое, что неприемлемо для распределённых систем. Поэтому на практике выбор стоит не между C, A и P, а между CP и AP: при сетевом разделении жертвовать консистентностью (продолжать обслуживать запросы с потенциально устаревшими данными) или доступностью (отказывать в обслуживании до восстановления связи).',
  },
  {
    id: 'sd-cap-008',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое кворумное чтение и запись (quorum reads/writes) в распределённых системах?',
    options: [
      'Чтение и запись данных только лидер-узлом',
      'Механизм, при котором операция считается успешной, когда подтверждена большинством узлов (W + R > N)',
      'Параллельное чтение данных со всех узлов одновременно',
      'Запись данных с шифрованием на каждом узле',
    ],
    correctIndex: 1,
    explanation:
      'Кворумный подход (quorum) определяет, сколько узлов должны подтвердить операцию. При N узлах, W (write quorum) -- минимум подтверждений для записи, R (read quorum) -- минимум ответов для чтения. Если W + R > N, то чтение гарантированно пересечётся хотя бы с одним узлом, содержащим последнюю запись, обеспечивая строгую консистентность. Примеры: N=3, W=2, R=2 (strong consistency), N=3, W=1, R=1 (eventual consistency, максимальная доступность). Cassandra позволяет настраивать W и R для каждого запроса: QUORUM, ONE, ALL.',
  },
  {
    id: 'sd-cap-009',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'open',
    question: 'Что такое линеаризуемость (linearizability) и чем она отличается от других моделей консистентности?',
    sampleAnswer:
      'Линеаризуемость (linearizability) -- самая строгая модель консистентности. Она гарантирует, что все операции выглядят так, как будто выполняются мгновенно в какой-то момент между началом и завершением операции, и все клиенты видят одинаковый порядок операций. Это создаёт иллюзию единственной копии данных. Сравнение с другими моделями: 1) Sequential consistency: все клиенты видят одинаковый порядок операций, но он может отличаться от реального временного порядка. 2) Causal consistency: гарантирует порядок только для причинно связанных операций (если A произошло до B и B зависит от A, все видят A до B). 3) Eventual consistency: гарантирует только конечную сходимость, без гарантий порядка. Линеаризуемость стоит дорого: требует координации между узлами (обычно через консенсус-протоколы типа Raft/Paxos), что увеличивает задержку. Её обеспечивают: ZooKeeper, etcd, Google Spanner (через TrueTime).',
    explanation:
      'Линеаризуемость необходима для: выбора лидера (только один узел должен стать лидером), распределённых блокировок, уникальных ограничений (два пользователя не должны занять один username). Для большинства приложений (лента новостей, каталог товаров) достаточно более слабых моделей консистентности, что позволяет улучшить производительность и доступность.',
  },
  {
    id: 'sd-cap-010',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое модель консистентности "Read Your Writes"?',
    options: [
      'Модель, гарантирующая, что все пользователи видят все записи в реальном времени',
      'Модель, гарантирующая, что пользователь после записи данных всегда видит свою запись при последующем чтении',
      'Модель, при которой чтение всегда быстрее записи',
      'Модель, при которой данные записываются только при подтверждении всеми узлами',
    ],
    correctIndex: 1,
    explanation:
      'Read Your Writes (RYW) -- модель консистентности, при которой клиент после выполнения записи гарантированно видит результат этой записи при последующих чтениях. Без RYW пользователь может обновить профиль, перезагрузить страницу и увидеть старые данные (потому что чтение попало на реплику, ещё не получившую обновление). Реализация: 1) Sticky sessions -- привязка клиента к одному узлу. 2) Чтение с мастера после записи (в течение короткого окна). 3) Включение версии/timestamp последней записи клиента в запрос чтения. RYW -- компромисс между strong consistency и eventual consistency, часто достаточный для хорошего UX.',
  },
  {
    id: 'sd-cap-011',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'open',
    question: 'Как работают векторные часы (vector clocks) и зачем они нужны в распределённых системах?',
    sampleAnswer:
      'Векторные часы -- механизм определения причинно-следственного порядка событий в распределённых системах. Каждый узел поддерживает вектор из N счётчиков (по одному на каждый узел системы). Правила: 1) При локальном событии узел увеличивает свой счётчик на 1. 2) При отправке сообщения узел увеличивает свой счётчик и прикладывает текущий вектор к сообщению. 3) При получении сообщения узел обновляет каждый элемент своего вектора как max(свой, полученный) и увеличивает свой счётчик. Сравнение векторов: если все элементы вектора A <= соответствующих элементов B и хотя бы один строго меньше, то A произошло до B (A -> B). Если ни один вектор не доминирует -- события конкурентны (нет причинной связи). Применение в Dynamo-style системах: при конфликте (конкурентные записи) система сохраняет обе версии и предлагает клиенту разрешить конфликт (как делает Amazon DynamoDB) или использует автоматическое разрешение (Last Writer Wins по timestamp).',
    explanation:
      'Векторные часы решают проблему определения порядка событий, которую обычные физические часы не могут решить из-за невозможности точной синхронизации часов между узлами (проблема, описанная Лесли Лэмпортом). На практике полноценные vector clocks используются редко из-за их размера (растут с числом узлов). Альтернативы: Lamport timestamps (проще, но меньше информации), Dotted Version Vectors (оптимизация для хранилищ типа Riak).',
  },
  {
    id: 'sd-cap-012',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое алгоритм Raft и какую задачу он решает?',
    options: [
      'Алгоритм балансировки нагрузки между серверами',
      'Алгоритм консенсуса, позволяющий группе узлов договориться об одном и том же значении, даже если часть узлов отказала',
      'Алгоритм шифрования данных в распределённых системах',
      'Алгоритм сжатия данных для снижения сетевого трафика',
    ],
    correctIndex: 1,
    explanation:
      'Raft -- алгоритм распределённого консенсуса, разработанный как более понятная альтернатива Paxos. Он позволяет группе узлов (кластеру) достичь согласия о последовательности операций, даже если часть узлов отказала (до (N-1)/2 для N узлов). Raft разделяет задачу на три подзадачи: 1) Выбор лидера (Leader Election): один узел избирается лидером, все записи идут через него. 2) Репликация лога (Log Replication): лидер реплицирует операции на follower-узлы. 3) Безопасность (Safety): гарантирует, что все узлы применяют одни и те же операции в одном порядке. Raft используется в etcd (хранилище Kubernetes), CockroachDB, Consul, TiKV.',
  },
  {
    id: 'sd-cap-013',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'open',
    question: 'Что такое проблема split-brain в распределённых системах и как её решать?',
    sampleAnswer:
      'Split-brain (расщепление мозга) -- ситуация, при которой сетевое разделение делит кластер на две (или более) части, и каждая часть считает себя единственной рабочей и продолжает принимать записи. Это приводит к конфликтам данных: две части кластера независимо модифицируют одни и те же данные, и при восстановлении связи данные рассинхронизированы. Решения: 1) Кворум (Quorum): только часть с большинством узлов (> N/2) может продолжать работу. Меньшинство переходит в read-only или перестаёт обслуживать запросы. 2) Fencing tokens: лидер получает монотонно возрастающий токен. Хранилище отклоняет запросы со старым токеном, предотвращая «zombie leader» проблему. 3) STONITH (Shoot The Other Node In The Head): принудительное выключение узлов меньшинства через IPMI/BMC. 4) Witness/Arbiter node: дополнительный лёгкий узел, который не хранит данные, но участвует в голосовании, обеспечивая нечётное число голосов. 5) Автоматическое разрешение конфликтов: CRDTs (Conflict-free Replicated Data Types) -- структуры данных, математически гарантирующие сходимость без координации.',
    explanation:
      'Split-brain -- одна из самых опасных ситуаций в распределённых системах, особенно для CP-систем (баз данных, координационных сервисов). Некорректная обработка split-brain может привести к потере или повреждению данных. Kubernetes использует etcd с Raft-консенсусом, который требует кворума и автоматически предотвращает split-brain.',
  },
  {
    id: 'sd-cap-014',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'open',
    question: 'Что такое PACELC-теорема и чем она дополняет CAP?',
    sampleAnswer:
      'PACELC -- расширение CAP-теоремы, предложенное Дэниелом Абади. Формулировка: при наличии сетевого разделения (P) выбирай между доступностью (A) и консистентностью (C); иначе (E -- Else), в нормальном режиме, выбирай между задержкой (L -- Latency) и консистентностью (C). CAP ничего не говорит о поведении системы без разделений, а PACELC заполняет этот пробел. Классификация систем: PA/EL -- при разделении выбирает доступность, без разделения -- низкую задержку. Примеры: Cassandra, DynamoDB (по умолчанию). PC/EC -- при разделении выбирает консистентность, без разделения тоже консистентность. Примеры: MongoDB (с majority), Google Spanner. PA/EC -- при разделении выбирает доступность, но без разделения гарантирует консистентность. Пример: некоторые конфигурации MongoDB. PC/EL -- при разделении выбирает консистентность, но без разделения оптимизирует задержку. Пример: PNUTS (Yahoo).',
    explanation:
      'PACELC объясняет, почему системы, классифицируемые одинаково по CAP, ведут себя по-разному. Например, Cassandra и VoltDB обе устойчивы к разделениям, но Cassandra оптимизирована для задержки (EL), а VoltDB -- для консистентности (EC). PACELC помогает архитекторам более точно выбирать базу данных, учитывая поведение системы не только в аварийных, но и в нормальных условиях.',
  },
  {
    id: 'sd-cap-015',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'open',
    question: 'Что означают свойства C (Consistency) и A (Availability) в CAP-теореме? Приведите простые примеры.',
    sampleAnswer:
      'Consistency (консистентность) означает, что все узлы системы возвращают одинаковые данные в один и тот же момент времени. Если один узел записал значение "X=5", то любой узел, получивший запрос на чтение X, должен вернуть 5 (или более новое значение). Пример: банковский счёт -- после списания средств любой запрос баланса должен показать актуальную сумму, иначе возможно повторное списание. Availability (доступность) означает, что каждый запрос к работающему узлу системы получает ответ (не обязательно с самыми свежими данными). Система не отказывает в обслуживании, даже если часть узлов недоступна. Пример: DNS -- даже если корневой сервер обновил запись, кэширующие серверы продолжают отвечать старыми данными вместо отказа. Социальная сеть -- лучше показать ленту с небольшой задержкой в обновлениях, чем не показать ничего.',
    explanation:
      'На практике выбор между C и A зависит от бизнес-требований. Для финансовых систем (банки, платежи) обычно выбирают CP, для социальных сетей и контентных платформ -- AP. Важно понимать, что CAP описывает поведение только при сетевом разделении; в нормальном режиме система может обеспечивать и C, и A.',
  },
  {
    id: 'sd-cap-016',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое Causal Consistency (причинная консистентность)?',
    options: [
      'Модель, при которой все данные всегда консистентны на всех узлах',
      'Модель, при которой причинно связанные операции видны всем узлам в правильном порядке, а конкурентные операции могут быть видны в разном порядке',
      'Модель, при которой данные становятся консистентными только после перезагрузки системы',
      'Модель, при которой консистентность достигается за счёт блокировки всех узлов',
    ],
    correctIndex: 1,
    explanation:
      'Causal Consistency -- модель консистентности, которая гарантирует, что причинно связанные операции видны всем узлам в одном и том же порядке. Если операция B зависит от результата операции A (например, ответ на комментарий), то все узлы увидят A до B. Но если операции независимы (два независимых комментария), разные узлы могут видеть их в разном порядке. Causal Consistency сильнее eventual consistency, но слабее linearizability. Она обеспечивает хороший баланс между консистентностью и производительностью. Реализуется через: vector clocks, logical timestamps, dependency tracking. Пример: MongoDB с версии 3.6 поддерживает causal consistency sessions.',
  },
  {
    id: 'sd-cap-017',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Как Google Spanner обеспечивает глобальную строгую консистентность при географическом распределении?',
    options: [
      'Использует классический Two-Phase Commit без оптимизаций',
      'Использует TrueTime API с атомными и GPS-часами для назначения глобально упорядоченных timestamp, обеспечивая внешнюю консистентность',
      'Отказывается от консистентности в пользу доступности',
      'Хранит все данные в одном дата-центре',
    ],
    correctIndex: 1,
    explanation:
      'Google Spanner -- уникальная распределённая база данных, обеспечивающая глобальную строгую консистентность (external consistency -- ещё сильнее linearizability). Ключевая инновация -- TrueTime API: специальные серверы с атомными часами и GPS-приёмниками предоставляют глобальное время с известной погрешностью (обычно < 7мс). Spanner назначает транзакциям timestamp из TrueTime и ждёт, пока погрешность не пройдёт (commit wait), гарантируя, что порядок timestamp соответствует реальному порядку событий. Это позволяет обеспечить строгую консистентность без дорогостоящего консенсуса для чтений (read-only транзакции не требуют координации). Цена: повышенная задержка записи из-за commit wait.',
  },
  {
    id: 'sd-cap-018',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'open',
    question: 'Что такое anti-entropy в распределённых системах и какие механизмы используются для синхронизации реплик?',
    sampleAnswer:
      'Anti-entropy -- механизмы поддержания согласованности данных между репликами в распределённых системах, работающие в фоновом режиме. Основные подходы: 1) Read Repair: при чтении данных с нескольких реплик (кворумное чтение), если обнаружено расхождение, актуальная версия записывается на устаревшие реплики. Плюс: не требует отдельного процесса. Минус: работает только для читаемых данных. 2) Anti-Entropy Protocol (Merkle Trees): периодический процесс, сравнивающий данные между репликами с помощью деревьев Меркла. Узлы обмениваются хэшами поддеревьев, что позволяет быстро найти расхождения без сравнения всех данных. Используется в Cassandra, DynamoDB. 3) Hinted Handoff: если целевой узел недоступен при записи, другой узел сохраняет запись во временном хранилище (hint) и доставляет её, когда целевой узел восстановится. 4) Gossip Protocol: узлы периодически обмениваются информацией с случайно выбранными соседями, распространяя обновления по всему кластеру (эпидемический алгоритм).',
    explanation:
      'Anti-entropy критически важна для AP-систем, где данные могут рассинхронизироваться при сетевых разделениях или отказах узлов. Cassandra использует все четыре механизма: gossip для обмена метаданными кластера, hinted handoff для доставки пропущенных записей, read repair при чтениях, и Merkle tree anti-entropy (nodetool repair) для полной синхронизации.',
  },
  {
    id: 'sd-cap-019',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какая из следующих систем является примером AP-системы (Availability + Partition Tolerance)?',
    options: [
      'PostgreSQL с синхронной репликацией',
      'Apache ZooKeeper',
      'Apache Cassandra',
      'etcd',
    ],
    correctIndex: 2,
    explanation:
      'Apache Cassandra -- классический пример AP-системы. При сетевом разделении она продолжает обслуживать запросы на всех доступных узлах, обеспечивая доступность ценой потенциальной рассинхронизации данных (eventual consistency). Cassandra использует tunable consistency: для каждого запроса можно настроить уровень консистентности (ONE, QUORUM, ALL), что позволяет балансировать между доступностью и консистентностью. ZooKeeper и etcd -- CP-системы (используют консенсус, жертвуют доступностью). PostgreSQL с синхронной репликацией -- CP-система.',
  },
  {
    id: 'sd-cap-020',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое CRDTs (Conflict-free Replicated Data Types) и как они обеспечивают консистентность без координации?',
    options: [
      'Специальные типы баз данных с встроенной репликацией',
      'Структуры данных, математически гарантирующие сходимость реплик при конкурентных обновлениях без необходимости координации между узлами',
      'Протокол передачи данных для минимизации конфликтов в сети',
      'Метод шифрования данных для безопасной репликации',
    ],
    correctIndex: 1,
    explanation:
      'CRDTs (Conflict-free Replicated Data Types) -- специальные структуры данных, разработанные так, что конкурентные обновления на разных репликах всегда можно объединить (merge) без конфликтов. Математически гарантируется eventual consistency без координации между узлами. Типы: G-Counter (только возрастающий счётчик), PN-Counter (счётчик с инкрементом и декрементом), G-Set (множество с добавлением), OR-Set (множество с добавлением и удалением), LWW-Register (регистр с Last Writer Wins). Применение: совместное редактирование (Google Docs подобные), корзина покупок, онлайн-счётчики, чаты. Используются в Riak, Redis (CRDT-типы), Automerge, Yjs.',
  },
];
