import type { Question } from '../types';

export const capTheoremQuestions: Question[] = [
  {
    id: 'sd-cap-001',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что утверждает CAP-теорема?',
    options: [
      'Распределённая система может одновременно обеспечить высокую скорость, безопасность и масштабируемость',
      'Распределённая система может одновременно обеспечить только два из трёх свойств: консистентность, доступность и устойчивость к разделению сети',
      'Любая система должна быть спроектирована с учётом C (Caching), A (API) и P (Performance)',
      'Консистентность данных всегда важнее доступности системы',
    ],
    correctIndex: 1,
    explanation:
      'CAP-теорема (теорема Брюера) утверждает, что распределённая система не может одновременно обеспечить все три свойства: Consistency (консистентность -- все узлы видят одинаковые данные), Availability (доступность -- каждый запрос получает ответ), Partition Tolerance (устойчивость к разделению -- система работает при разрыве связи между узлами). Поскольку сетевые разделения в распределённых системах неизбежны, на практике выбор стоит между CP (консистентность + устойчивость) и AP (доступность + устойчивость).',
  },
  {
    id: 'sd-cap-002',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'open',
    question: 'Приведите примеры CP и AP систем. Объясните, почему они сделали именно такой выбор.',
    sampleAnswer:
      'CP-системы (Consistency + Partition Tolerance): 1) Apache ZooKeeper -- координационный сервис для распределённых систем. Гарантирует линейную консистентность: все клиенты видят одинаковое состояние. При сетевом разделении меньшинство узлов перестаёт обслуживать запросы (жертвует доступностью). Это критично, потому что ZooKeeper используется для выбора лидера и распределённых блокировок, где консистентность обязательна. 2) MongoDB (с настройкой majority write concern) -- обеспечивает строгую консистентность записей, подтверждённых большинством реплик. AP-системы (Availability + Partition Tolerance): 1) Cassandra -- при сетевом разделении продолжает обслуживать запросы на всех доступных узлах, даже если данные могут быть временно рассинхронизированы. Это подходит для систем, где доступность критичнее точности (лента новостей, счётчики). 2) DynamoDB -- по умолчанию обеспечивает eventual consistency с опцией strongly consistent reads. Выбор AP обусловлен требованием Amazon к 99.99% доступности.',
    explanation:
      'Важно понимать, что CAP-теорема описывает поведение системы только при наличии сетевого разделения. В нормальном режиме работы система может обеспечивать все три свойства. Также многие современные системы позволяют настраивать баланс между C и A на уровне отдельных операций (tunable consistency). Например, Cassandra позволяет задать уровень консистентности (ONE, QUORUM, ALL) для каждого запроса.',
  },
  {
    id: 'sd-cap-003',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'open',
    question: 'Какие ограничения и критика существуют у CAP-теоремы? Что такое PACELC как её расширение?',
    sampleAnswer:
      'Критика CAP-теоремы: 1) Бинарность: CAP представляет свойства как бинарные (есть/нет), но на практике существует спектр уровней консистентности (strong, eventual, causal, read-your-writes) и доступности (99.9%, 99.99%). 2) Редкость разделений: сетевые разделения случаются редко, а CAP ничего не говорит о компромиссах в нормальном режиме. 3) Не учитывает задержку (latency): даже без разделения, строгая консистентность требует координации между узлами, что увеличивает задержку. PACELC -- расширение CAP: при разделении (P) выбирай между доступностью (A) и консистентностью (C), иначе (E -- else) выбирай между задержкой (L -- Latency) и консистентностью (C). Например: Cassandra -- PA/EL (при разделении выбирает доступность, без разделения -- низкую задержку). MongoDB -- PC/EC (при разделении выбирает консистентность, без разделения тоже консистентность с более высокой задержкой). DynamoDB -- PA/EL по умолчанию, PC/EC для consistent reads.',
    explanation:
      'PACELC, предложенная Дэниелом Абади, даёт более точную модель для описания поведения распределённых систем. Она объясняет, почему Cassandra и DynamoDB (обе AP по CAP) ведут себя по-разному в нормальном режиме: Cassandra оптимизирована для низкой задержки (EL), а системы вроде Spanner оптимизированы для строгой консистентности (EC) даже без разделений, но ценой более высокой задержки.',
  },
  {
    id: 'sd-cap-004',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое eventual consistency (согласованность в конечном счёте)?',
    options: [
      'Данные всегда консистентны во всех узлах в любой момент времени',
      'При отсутствии новых обновлений все реплики со временем придут к одинаковому состоянию данных',
      'Данные никогда не становятся консистентными в распределённой системе',
      'Консистентность достигается только после перезапуска всех узлов системы',
    ],
    correctIndex: 1,
    explanation:
      'Eventual consistency -- это модель консистентности, при которой гарантируется, что если прекратить обновления данных, то через некоторое время все реплики придут к одинаковому состоянию. В промежутке между обновлением и полной синхронизацией разные узлы могут возвращать разные версии данных. Эта модель используется в AP-системах (DynamoDB, Cassandra, DNS). Она обеспечивает высокую доступность и низкую задержку, но требует от приложения умения работать с потенциально устаревшими данными.',
  },
  {
    id: 'sd-cap-005',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что означает буква "P" (Partition Tolerance) в CAP-теореме?',
    options: [
      'Система может разделять данные между таблицами (partitioning)',
      'Система продолжает работать, даже если связь между некоторыми узлами сети нарушена',
      'Система поддерживает параллельную обработку запросов',
      'Система обеспечивает защиту данных от несанкционированного доступа',
    ],
    correctIndex: 1,
    explanation:
      'Partition Tolerance (устойчивость к разделению сети) означает, что система продолжает функционировать, даже если сетевое соединение между некоторыми узлами потеряно (network partition). В распределённых системах сетевые разделения неизбежны (кабель обрезали, switch вышел из строя, дата-центр стал недоступен), поэтому P -- обязательное свойство для любой распределённой системы. Вопрос на практике стоит так: что делать при разделении -- жертвовать консистентностью (AP) или доступностью (CP)?',
  },
  {
    id: 'sd-cap-006',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какая система является примером CP-системы (Consistency + Partition Tolerance)?',
    options: [
      'Apache Cassandra',
      'DNS',
      'Apache ZooKeeper',
      'Amazon DynamoDB (в режиме по умолчанию)',
    ],
    correctIndex: 2,
    explanation:
      'Apache ZooKeeper -- классический пример CP-системы. Он используется для координации распределённых систем (выбор лидера, распределённые блокировки, хранение конфигурации), где консистентность критически важна. При сетевом разделении узлы, оказавшиеся в меньшинстве, перестают обслуживать запросы (жертвуют доступностью), чтобы гарантировать, что все клиенты видят одинаковое состояние. Cassandra и DynamoDB -- примеры AP-систем, DNS -- также AP (разные серверы могут возвращать разные записи во время обновления).',
  },
  {
    id: 'sd-cap-007',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Почему в распределённой системе невозможно отказаться от Partition Tolerance (P)?',
    options: [
      'Потому что это самое дешёвое свойство для реализации',
      'Потому что сетевые сбои в распределённых системах неизбежны и система должна уметь с ними справляться',
      'Потому что Partition Tolerance улучшает производительность',
      'Потому что без P невозможно реализовать шифрование данных',
    ],
    correctIndex: 1,
    explanation:
      'В реальном мире сетевые разделения (network partitions) неизбежны: оборудование выходит из строя, кабели повреждаются, маршрутизаторы перегружаются, дата-центры теряют связь. Система, не устойчивая к разделениям, перестаёт работать при любом сетевом сбое, что неприемлемо для распределённых систем. Поэтому на практике выбор стоит не между C, A и P, а между CP и AP: при сетевом разделении жертвовать консистентностью (продолжать обслуживать запросы с потенциально устаревшими данными) или доступностью (отказывать в обслуживании до восстановления связи).',
  },
  {
    id: 'sd-cap-008',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое кворумное чтение и запись (quorum reads/writes) в распределённых системах?',
    options: [
      'Чтение и запись данных только лидер-узлом',
      'Механизм, при котором операция считается успешной, когда подтверждена большинством узлов (W + R > N)',
      'Параллельное чтение данных со всех узлов одновременно',
      'Запись данных с шифрованием на каждом узле',
    ],
    correctIndex: 1,
    explanation:
      'Кворумный подход (quorum) определяет, сколько узлов должны подтвердить операцию. При N узлах, W (write quorum) -- минимум подтверждений для записи, R (read quorum) -- минимум ответов для чтения. Если W + R > N, то чтение гарантированно пересечётся хотя бы с одним узлом, содержащим последнюю запись, обеспечивая строгую консистентность. Примеры: N=3, W=2, R=2 (strong consistency), N=3, W=1, R=1 (eventual consistency, максимальная доступность). Cassandra позволяет настраивать W и R для каждого запроса: QUORUM, ONE, ALL.',
  },
  {
    id: 'sd-cap-009',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'open',
    question: 'Что такое линеаризуемость (linearizability) и чем она отличается от других моделей консистентности?',
    sampleAnswer:
      'Линеаризуемость (linearizability) -- самая строгая модель консистентности. Она гарантирует, что все операции выглядят так, как будто выполняются мгновенно в какой-то момент между началом и завершением операции, и все клиенты видят одинаковый порядок операций. Это создаёт иллюзию единственной копии данных. Сравнение с другими моделями: 1) Sequential consistency: все клиенты видят одинаковый порядок операций, но он может отличаться от реального временного порядка. 2) Causal consistency: гарантирует порядок только для причинно связанных операций (если A произошло до B и B зависит от A, все видят A до B). 3) Eventual consistency: гарантирует только конечную сходимость, без гарантий порядка. Линеаризуемость стоит дорого: требует координации между узлами (обычно через консенсус-протоколы типа Raft/Paxos), что увеличивает задержку. Её обеспечивают: ZooKeeper, etcd, Google Spanner (через TrueTime).',
    explanation:
      'Линеаризуемость необходима для: выбора лидера (только один узел должен стать лидером), распределённых блокировок, уникальных ограничений (два пользователя не должны занять один username). Для большинства приложений (лента новостей, каталог товаров) достаточно более слабых моделей консистентности, что позволяет улучшить производительность и доступность.',
  },
  {
    id: 'sd-cap-010',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое модель консистентности "Read Your Writes"?',
    options: [
      'Модель, гарантирующая, что все пользователи видят все записи в реальном времени',
      'Модель, гарантирующая, что пользователь после записи данных всегда видит свою запись при последующем чтении',
      'Модель, при которой чтение всегда быстрее записи',
      'Модель, при которой данные записываются только при подтверждении всеми узлами',
    ],
    correctIndex: 1,
    explanation:
      'Read Your Writes (RYW) -- модель консистентности, при которой клиент после выполнения записи гарантированно видит результат этой записи при последующих чтениях. Без RYW пользователь может обновить профиль, перезагрузить страницу и увидеть старые данные (потому что чтение попало на реплику, ещё не получившую обновление). Реализация: 1) Sticky sessions -- привязка клиента к одному узлу. 2) Чтение с мастера после записи (в течение короткого окна). 3) Включение версии/timestamp последней записи клиента в запрос чтения. RYW -- компромисс между strong consistency и eventual consistency, часто достаточный для хорошего UX.',
  },
  {
    id: 'sd-cap-011',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'open',
    question: 'Как работают векторные часы (vector clocks) и зачем они нужны в распределённых системах?',
    sampleAnswer:
      'Векторные часы -- механизм определения причинно-следственного порядка событий в распределённых системах. Каждый узел поддерживает вектор из N счётчиков (по одному на каждый узел системы). Правила: 1) При локальном событии узел увеличивает свой счётчик на 1. 2) При отправке сообщения узел увеличивает свой счётчик и прикладывает текущий вектор к сообщению. 3) При получении сообщения узел обновляет каждый элемент своего вектора как max(свой, полученный) и увеличивает свой счётчик. Сравнение векторов: если все элементы вектора A <= соответствующих элементов B и хотя бы один строго меньше, то A произошло до B (A -> B). Если ни один вектор не доминирует -- события конкурентны (нет причинной связи). Применение в Dynamo-style системах: при конфликте (конкурентные записи) система сохраняет обе версии и предлагает клиенту разрешить конфликт (как делает Amazon DynamoDB) или использует автоматическое разрешение (Last Writer Wins по timestamp).',
    explanation:
      'Векторные часы решают проблему определения порядка событий, которую обычные физические часы не могут решить из-за невозможности точной синхронизации часов между узлами (проблема, описанная Лесли Лэмпортом). На практике полноценные vector clocks используются редко из-за их размера (растут с числом узлов). Альтернативы: Lamport timestamps (проще, но меньше информации), Dotted Version Vectors (оптимизация для хранилищ типа Riak).',
  },
  {
    id: 'sd-cap-012',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое алгоритм Raft и какую задачу он решает?',
    options: [
      'Алгоритм балансировки нагрузки между серверами',
      'Алгоритм консенсуса, позволяющий группе узлов договориться об одном и том же значении, даже если часть узлов отказала',
      'Алгоритм шифрования данных в распределённых системах',
      'Алгоритм сжатия данных для снижения сетевого трафика',
    ],
    correctIndex: 1,
    explanation:
      'Raft -- алгоритм распределённого консенсуса, разработанный как более понятная альтернатива Paxos. Он позволяет группе узлов (кластеру) достичь согласия о последовательности операций, даже если часть узлов отказала (до (N-1)/2 для N узлов). Raft разделяет задачу на три подзадачи: 1) Выбор лидера (Leader Election): один узел избирается лидером, все записи идут через него. 2) Репликация лога (Log Replication): лидер реплицирует операции на follower-узлы. 3) Безопасность (Safety): гарантирует, что все узлы применяют одни и те же операции в одном порядке. Raft используется в etcd (хранилище Kubernetes), CockroachDB, Consul, TiKV.',
  },
  {
    id: 'sd-cap-013',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'open',
    question: 'Что такое проблема split-brain в распределённых системах и как её решать?',
    sampleAnswer:
      'Split-brain (расщепление мозга) -- ситуация, при которой сетевое разделение делит кластер на две (или более) части, и каждая часть считает себя единственной рабочей и продолжает принимать записи. Это приводит к конфликтам данных: две части кластера независимо модифицируют одни и те же данные, и при восстановлении связи данные рассинхронизированы. Решения: 1) Кворум (Quorum): только часть с большинством узлов (> N/2) может продолжать работу. Меньшинство переходит в read-only или перестаёт обслуживать запросы. 2) Fencing tokens: лидер получает монотонно возрастающий токен. Хранилище отклоняет запросы со старым токеном, предотвращая «zombie leader» проблему. 3) STONITH (Shoot The Other Node In The Head): принудительное выключение узлов меньшинства через IPMI/BMC. 4) Witness/Arbiter node: дополнительный лёгкий узел, который не хранит данные, но участвует в голосовании, обеспечивая нечётное число голосов. 5) Автоматическое разрешение конфликтов: CRDTs (Conflict-free Replicated Data Types) -- структуры данных, математически гарантирующие сходимость без координации.',
    explanation:
      'Split-brain -- одна из самых опасных ситуаций в распределённых системах, особенно для CP-систем (баз данных, координационных сервисов). Некорректная обработка split-brain может привести к потере или повреждению данных. Kubernetes использует etcd с Raft-консенсусом, который требует кворума и автоматически предотвращает split-brain.',
  },
  {
    id: 'sd-cap-014',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'open',
    question: 'Что такое PACELC-теорема и чем она дополняет CAP?',
    sampleAnswer:
      'PACELC -- расширение CAP-теоремы, предложенное Дэниелом Абади. Формулировка: при наличии сетевого разделения (P) выбирай между доступностью (A) и консистентностью (C); иначе (E -- Else), в нормальном режиме, выбирай между задержкой (L -- Latency) и консистентностью (C). CAP ничего не говорит о поведении системы без разделений, а PACELC заполняет этот пробел. Классификация систем: PA/EL -- при разделении выбирает доступность, без разделения -- низкую задержку. Примеры: Cassandra, DynamoDB (по умолчанию). PC/EC -- при разделении выбирает консистентность, без разделения тоже консистентность. Примеры: MongoDB (с majority), Google Spanner. PA/EC -- при разделении выбирает доступность, но без разделения гарантирует консистентность. Пример: некоторые конфигурации MongoDB. PC/EL -- при разделении выбирает консистентность, но без разделения оптимизирует задержку. Пример: PNUTS (Yahoo).',
    explanation:
      'PACELC объясняет, почему системы, классифицируемые одинаково по CAP, ведут себя по-разному. Например, Cassandra и VoltDB обе устойчивы к разделениям, но Cassandra оптимизирована для задержки (EL), а VoltDB -- для консистентности (EC). PACELC помогает архитекторам более точно выбирать базу данных, учитывая поведение системы не только в аварийных, но и в нормальных условиях.',
  },
  {
    id: 'sd-cap-015',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'open',
    question: 'Что означают свойства C (Consistency) и A (Availability) в CAP-теореме? Приведите простые примеры.',
    sampleAnswer:
      'Consistency (консистентность) означает, что все узлы системы возвращают одинаковые данные в один и тот же момент времени. Если один узел записал значение "X=5", то любой узел, получивший запрос на чтение X, должен вернуть 5 (или более новое значение). Пример: банковский счёт -- после списания средств любой запрос баланса должен показать актуальную сумму, иначе возможно повторное списание. Availability (доступность) означает, что каждый запрос к работающему узлу системы получает ответ (не обязательно с самыми свежими данными). Система не отказывает в обслуживании, даже если часть узлов недоступна. Пример: DNS -- даже если корневой сервер обновил запись, кэширующие серверы продолжают отвечать старыми данными вместо отказа. Социальная сеть -- лучше показать ленту с небольшой задержкой в обновлениях, чем не показать ничего.',
    explanation:
      'На практике выбор между C и A зависит от бизнес-требований. Для финансовых систем (банки, платежи) обычно выбирают CP, для социальных сетей и контентных платформ -- AP. Важно понимать, что CAP описывает поведение только при сетевом разделении; в нормальном режиме система может обеспечивать и C, и A.',
  },
  {
    id: 'sd-cap-016',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое Causal Consistency (причинная консистентность)?',
    options: [
      'Модель, при которой все данные всегда консистентны на всех узлах',
      'Модель, при которой причинно связанные операции видны всем узлам в правильном порядке, а конкурентные операции могут быть видны в разном порядке',
      'Модель, при которой данные становятся консистентными только после перезагрузки системы',
      'Модель, при которой консистентность достигается за счёт блокировки всех узлов',
    ],
    correctIndex: 1,
    explanation:
      'Causal Consistency -- модель консистентности, которая гарантирует, что причинно связанные операции видны всем узлам в одном и том же порядке. Если операция B зависит от результата операции A (например, ответ на комментарий), то все узлы увидят A до B. Но если операции независимы (два независимых комментария), разные узлы могут видеть их в разном порядке. Causal Consistency сильнее eventual consistency, но слабее linearizability. Она обеспечивает хороший баланс между консистентностью и производительностью. Реализуется через: vector clocks, logical timestamps, dependency tracking. Пример: MongoDB с версии 3.6 поддерживает causal consistency sessions.',
  },
  {
    id: 'sd-cap-017',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Как Google Spanner обеспечивает глобальную строгую консистентность при географическом распределении?',
    options: [
      'Использует классический Two-Phase Commit без оптимизаций',
      'Использует TrueTime API с атомными и GPS-часами для назначения глобально упорядоченных timestamp, обеспечивая внешнюю консистентность',
      'Отказывается от консистентности в пользу доступности',
      'Хранит все данные в одном дата-центре',
    ],
    correctIndex: 1,
    explanation:
      'Google Spanner -- уникальная распределённая база данных, обеспечивающая глобальную строгую консистентность (external consistency -- ещё сильнее linearizability). Ключевая инновация -- TrueTime API: специальные серверы с атомными часами и GPS-приёмниками предоставляют глобальное время с известной погрешностью (обычно < 7мс). Spanner назначает транзакциям timestamp из TrueTime и ждёт, пока погрешность не пройдёт (commit wait), гарантируя, что порядок timestamp соответствует реальному порядку событий. Это позволяет обеспечить строгую консистентность без дорогостоящего консенсуса для чтений (read-only транзакции не требуют координации). Цена: повышенная задержка записи из-за commit wait.',
  },
  {
    id: 'sd-cap-018',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'open',
    question: 'Что такое anti-entropy в распределённых системах и какие механизмы используются для синхронизации реплик?',
    sampleAnswer:
      'Anti-entropy -- механизмы поддержания согласованности данных между репликами в распределённых системах, работающие в фоновом режиме. Основные подходы: 1) Read Repair: при чтении данных с нескольких реплик (кворумное чтение), если обнаружено расхождение, актуальная версия записывается на устаревшие реплики. Плюс: не требует отдельного процесса. Минус: работает только для читаемых данных. 2) Anti-Entropy Protocol (Merkle Trees): периодический процесс, сравнивающий данные между репликами с помощью деревьев Меркла. Узлы обмениваются хэшами поддеревьев, что позволяет быстро найти расхождения без сравнения всех данных. Используется в Cassandra, DynamoDB. 3) Hinted Handoff: если целевой узел недоступен при записи, другой узел сохраняет запись во временном хранилище (hint) и доставляет её, когда целевой узел восстановится. 4) Gossip Protocol: узлы периодически обмениваются информацией с случайно выбранными соседями, распространяя обновления по всему кластеру (эпидемический алгоритм).',
    explanation:
      'Anti-entropy критически важна для AP-систем, где данные могут рассинхронизироваться при сетевых разделениях или отказах узлов. Cassandra использует все четыре механизма: gossip для обмена метаданными кластера, hinted handoff для доставки пропущенных записей, read repair при чтениях, и Merkle tree anti-entropy (nodetool repair) для полной синхронизации.',
  },
  {
    id: 'sd-cap-019',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какая из следующих систем является примером AP-системы (Availability + Partition Tolerance)?',
    options: [
      'PostgreSQL с синхронной репликацией',
      'Apache ZooKeeper',
      'Apache Cassandra',
      'etcd',
    ],
    correctIndex: 2,
    explanation:
      'Apache Cassandra -- классический пример AP-системы. При сетевом разделении она продолжает обслуживать запросы на всех доступных узлах, обеспечивая доступность ценой потенциальной рассинхронизации данных (eventual consistency). Cassandra использует tunable consistency: для каждого запроса можно настроить уровень консистентности (ONE, QUORUM, ALL), что позволяет балансировать между доступностью и консистентностью. ZooKeeper и etcd -- CP-системы (используют консенсус, жертвуют доступностью). PostgreSQL с синхронной репликацией -- CP-система.',
  },
  {
    id: 'sd-cap-020',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое CRDTs (Conflict-free Replicated Data Types) и как они обеспечивают консистентность без координации?',
    options: [
      'Специальные типы баз данных с встроенной репликацией',
      'Структуры данных, математически гарантирующие сходимость реплик при конкурентных обновлениях без необходимости координации между узлами',
      'Протокол передачи данных для минимизации конфликтов в сети',
      'Метод шифрования данных для безопасной репликации',
    ],
    correctIndex: 1,
    explanation:
      'CRDTs (Conflict-free Replicated Data Types) -- специальные структуры данных, разработанные так, что конкурентные обновления на разных репликах всегда можно объединить (merge) без конфликтов. Математически гарантируется eventual consistency без координации между узлами. Типы: G-Counter (только возрастающий счётчик), PN-Counter (счётчик с инкрементом и декрементом), G-Set (множество с добавлением), OR-Set (множество с добавлением и удалением), LWW-Register (регистр с Last Writer Wins). Применение: совместное редактирование (Google Docs подобные), корзина покупок, онлайн-счётчики, чаты. Используются в Riak, Redis (CRDT-типы), Automerge, Yjs.',
  },
  {
    id: 'sd-cap-021',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'open',
    question: 'Приведите пример из жизни, иллюстрирующий выбор между консистентностью и доступностью.',
    sampleAnswer:
      'Пример: банкомат (ATM). При сетевом разделении (банкомат потерял связь с центральным сервером): CP-выбор: банкомат отказывает в обслуживании ("Сервис временно недоступен"), пока связь не восстановится. Гарантия, что операции консистентны. AP-выбор: банкомат позволяет снять ограниченную сумму (например, до $200) на основе локальных данных. Доступность сохраняется, но возможен овердрафт, который будет исправлен после восстановления связи. Большинство банкоматов выбирают компромисс: для мелких операций -- AP (доступность с лимитом), для крупных -- CP (требуется онлайн-подтверждение).',
    explanation:
      'Этот пример показывает, что выбор между C и A -- это бизнес-решение, а не чисто техническое. Стоимость недоступности (потеря клиентов) vs стоимость неконсистентности (овердрафт, мошенничество) определяет выбор.',
  },
  {
    id: 'sd-cap-022',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что происходит с CA-системой (Consistency + Availability) при сетевом разделении?',
    options: [
      'Она продолжает работать нормально',
      'Она перестаёт существовать как распределённая система, так как игнорирует разделение',
      'Она автоматически переключается в режим CP',
      'Она становится быстрее',
    ],
    correctIndex: 1,
    explanation:
      'CA-система -- это система, которая обеспечивает консистентность и доступность, но не устойчива к сетевым разделениям. На практике это означает, что система работает на одном узле или в сети, где разделения невозможны (что нереалистично для распределённых систем). При возникновении разделения CA-система либо прекратит работу, либо будет вынуждена выбирать между C и A. Классические RDBMS (PostgreSQL single node) -- пример CA до добавления репликации.',
  },
  {
    id: 'sd-cap-023',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое Tunable Consistency?',
    options: [
      'Возможность изменять уровень консистентности для отдельных операций',
      'Автоматическая настройка консистентности системой',
      'Консистентность, которая улучшается со временем',
      'Консистентность только для определённых типов данных',
    ],
    correctIndex: 0,
    explanation:
      'Tunable Consistency позволяет задавать уровень консистентности для каждой операции (read/write) отдельно. В Cassandra: ONE (запись/чтение на один узел), QUORUM (большинство), ALL (все узлы). Формула: R + W > N гарантирует strong consistency (где R -- read consistency, W -- write consistency, N -- replication factor). Пример: N=3, W=2, R=2 (strong), N=3, W=1, R=1 (eventual, max availability). Это позволяет балансировать между консистентностью, доступностью и задержкой для каждого use case.',
  },
  {
    id: 'sd-cap-024',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'open',
    question: 'Объясните формулу R + W > N для достижения строгой консистентности. Приведите примеры конфигураций.',
    sampleAnswer:
      'Формула R + W > N используется в системах с кворумной репликацией. N -- количество реплик данных. W -- количество узлов, которые должны подтвердить запись (write quorum). R -- количество узлов, с которых нужно прочитать (read quorum). Если R + W > N, то множества узлов записи и чтения обязательно пересекаются, гарантируя, что читатель увидит последнюю запись. Примеры для N=3: 1) W=2, R=2 (R+W=4>3): strong consistency, tolerates 1 node failure для read и write. 2) W=3, R=1 (R+W=4>3): fast reads, slow writes, any node can serve reads. 3) W=1, R=3 (R+W=4>3): fast writes, slow reads. 4) W=1, R=1 (R+W=2<3): eventual consistency, max availability и speed.',
    explanation:
      'DynamoDB, Cassandra, Riak используют эту модель. Выбор W и R зависит от паттерна нагрузки: read-heavy (низкий R), write-heavy (низкий W), balanced (W=R=quorum). При W=ALL или R=ALL система менее отказоустойчива -- отказ одного узла блокирует операции.',
  },
  {
    id: 'sd-cap-025',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое монотонное чтение (Monotonic Reads)?',
    options: [
      'Чтение данных строго в хронологическом порядке',
      'Гарантия того, что клиент не увидит более старую версию данных после того, как увидел более новую',
      'Чтение только с одного узла',
      'Чтение данных только в возрастающем порядке ID',
    ],
    correctIndex: 1,
    explanation:
      'Monotonic Reads -- модель консистентности, гарантирующая, что если клиент прочитал версию данных X, то последующие чтения вернут X или более новую версию, никогда более старую. Без этой гарантии при чтении с разных реплик клиент может наблюдать "скачки назад во времени". Пример нарушения: пользователь видит комментарий, обновляет страницу, комментарий исчезает (попал на отстающую реплику). Реализация: sticky sessions (всегда читать с одной реплики) или передача версии последнего чтения в запросе.',
  },
  {
    id: 'sd-cap-026',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'open',
    question: 'Опишите алгоритм Paxos. Какую задачу он решает?',
    sampleAnswer:
      'Paxos -- алгоритм достижения консенсуса в распределённой системе, предложенный Лесли Лэмпортом. Позволяет группе узлов договориться об одном значении, даже если часть узлов отказала. Роли: Proposers (предлагают значения), Acceptors (голосуют), Learners (узнают результат). Фазы Basic Paxos: 1) Prepare: Proposer отправляет Prepare(n) с номером предложения n. Acceptor отвечает Promise, если n > всех ранее виденных, и сообщает ранее принятые значения. 2) Accept: Proposer, получив большинство Promise, отправляет Accept(n, v) с выбранным значением v (либо своё, либо уже принятое ранее). 3) Acceptor принимает, если не Promise-ил бо́льший номер. 4) Значение выбрано (chosen), когда большинство Acceptors его приняли. Multi-Paxos оптимизирует для последовательности значений (log). Paxos сложен для понимания и реализации, поэтому появился Raft.',
    explanation:
      'Paxos доказанно корректен, но его сложность породила множество вариаций и багов в реализациях. Лэмпорт написал статью "Paxos Made Simple", которая всё равно считается сложной. Raft был создан как понятная альтернатива. Google Chubby, Apache ZooKeeper используют варианты Paxos.',
  },
  {
    id: 'sd-cap-027',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Какая гарантия нарушается при использовании Last Writer Wins (LWW) для разрешения конфликтов?',
    options: [
      'Availability',
      'Causal consistency -- конкурентные обновления могут быть потеряны, даже если они не конфликтуют логически',
      'Partition tolerance',
      'Durability',
    ],
    correctIndex: 1,
    explanation:
      'Last Writer Wins (LWW) разрешает конфликты, выбирая значение с более поздним timestamp. Проблема: если два пользователя одновременно обновляют разные поля одного документа, одно обновление будет потеряно, хотя логически они не конфликтуют. Пример: User A устанавливает name="Alice", User B устанавливает email="bob@test.com" одновременно. При LWW один из updates пропадёт. Альтернативы: CRDTs (merge на уровне полей), application-level conflict resolution, vector clocks для обнаружения конфликтов.',
  },
  {
    id: 'sd-cap-028',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'open',
    question: 'Что такое Sloppy Quorum и Hinted Handoff? Как они влияют на доступность?',
    sampleAnswer:
      'Sloppy Quorum -- вариация кворума, при которой запись может быть подтверждена узлами, не являющимися "домашними" репликами ключа, если домашние реплики недоступны. Это повышает доступность: даже если все 3 домашние реплики offline, запись может пойти на другие узлы кластера. Hinted Handoff -- механизм, при котором узел, принявший "чужую" запись (hint), сохраняет её с пометкой о целевом узле. Когда целевой узел возвращается, hint передаётся ему (handoff). Это обеспечивает eventually consistent доставку данных на правильные реплики после восстановления. Dynamo (Amazon), Cassandra, Riak используют эти техники. Компромисс: повышенная доступность ценой временного нарушения R+W>N гарантий.',
    explanation:
      'Sloppy Quorum критикуется за то, что создаёт иллюзию консистентности: клиент получает подтверждение записи, но данные могут быть временно недоступны для чтения с "правильных" реплик. Riak W=quorum + sloppy quorum может быть менее консистентным, чем кажется.',
  },
  {
    id: 'sd-cap-029',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какой тип репликации обеспечивает RPO=0 (нулевую потерю данных)?',
    options: [
      'Асинхронная репликация',
      'Синхронная репликация',
      'Ленивая (lazy) репликация',
      'Периодическая репликация по расписанию',
    ],
    correctIndex: 1,
    explanation:
      'Синхронная репликация гарантирует, что данные записаны на реплику до подтверждения клиенту. RPO (Recovery Point Objective) = 0, так как нет окна потери данных. Недостаток: увеличенная latency записи (ждём ответа от реплики) и сниженная доступность (если реплика недоступна, запись блокируется). Асинхронная репликация имеет RPO > 0: данные могут быть потеряны между моментом записи на master и репликацией на slave.',
  },
  {
    id: 'sd-cap-030',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'open',
    question: 'Как работает алгоритм выбора лидера в Raft? Опишите процесс.',
    sampleAnswer:
      'Выбор лидера в Raft: 1) Все узлы стартуют как Followers. 2) Каждый follower имеет election timeout (случайный, 150-300ms). 3) Если follower не получает heartbeat от лидера до таймаута, он становится Candidate. 4) Candidate увеличивает свой term (эпоху), голосует за себя и отправляет RequestVote всем узлам. 5) Узел голосует за кандидата, если: а) term кандидата >= своего, б) ещё не голосовал в этом term, в) log кандидата не менее актуален. 6) Кандидат, получивший большинство голосов, становится Leader. 7) Leader отправляет heartbeats, предотвращая новые выборы. При split vote (никто не набрал большинства) -- новый раунд с увеличенным term и новыми случайными таймаутами. Safety: в каждом term максимум один лидер.',
    explanation:
      'Случайные election timeouts предотвращают постоянные split votes. Raft гарантирует, что лидер имеет все committed записи, благодаря требованию актуальности log при голосовании. etcd, CockroachDB, Consul используют Raft.',
  },
  {
    id: 'sd-cap-031',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое Byzantine Fault Tolerance (BFT)?',
    options: [
      'Устойчивость к физическому повреждению серверов',
      'Способность системы работать корректно даже при наличии узлов, ведущих себя злонамеренно или непредсказуемо',
      'Защита от DDoS-атак',
      'Шифрование данных между узлами',
    ],
    correctIndex: 1,
    explanation:
      'Byzantine Fault Tolerance (BFT) -- способность системы достигать консенсуса и работать корректно, даже если часть узлов ведёт себя злонамеренно (отправляет ложные данные, противоречивые сообщения разным узлам). Требует 3f+1 узлов для толерантности к f византийским узлам. Примеры: PBFT (Practical BFT), алгоритмы blockchain (PoW, PoS). Классические алгоритмы (Paxos, Raft) предполагают только crash-fault (узел либо работает корректно, либо молчит), не byzantine-fault.',
  },
  {
    id: 'sd-cap-032',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Какой механизм использует CockroachDB для обеспечения serializable isolation в распределённой системе?',
    options: [
      'Two-Phase Locking (2PL)',
      'Hybrid Logical Clocks (HLC) + Raft consensus для каждого range',
      'Optimistic Concurrency Control только',
      'Single-master репликация',
    ],
    correctIndex: 1,
    explanation:
      'CockroachDB использует комбинацию: Hybrid Logical Clocks (HLC) для упорядочивания событий (комбинация физических и логических часов), Raft consensus для репликации каждого range (подмножества данных), MVCC для изоляции транзакций, serializable snapshot isolation (SSI) для обнаружения write skew. Это позволяет достичь serializable isolation в geo-distributed системе без TrueTime (как у Spanner). CockroachDB -- CP-система, жертвующая availability при partition.',
  },
  {
    id: 'sd-cap-033',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'open',
    question: 'Что такое Lamport Timestamps и какую проблему они решают?',
    sampleAnswer:
      'Lamport Timestamps -- механизм логических часов, предложенный Лесли Лэмпортом для определения порядка событий в распределённой системе. Проблема: физические часы на разных узлах невозможно идеально синхронизировать, поэтому нельзя полагаться на wallclock time для упорядочивания событий. Правила: 1) Каждый процесс имеет счётчик LC, изначально 0. 2) Перед каждым событием: LC = LC + 1. 3) При отправке сообщения: включить LC в сообщение. 4) При получении сообщения: LC = max(LC, received_LC) + 1. Гарантия: если событие A произошло до B (A -> B), то LC(A) < LC(B). Ограничение: обратное не верно -- LC(A) < LC(B) не означает A -> B (события могут быть конкурентными). Для различения конкурентных событий нужны Vector Clocks.',
    explanation:
      'Lamport Timestamps достаточны для total ordering событий (присвоить уникальный порядок), но не для определения конкурентности. Amazon DynamoDB использовала вектор часов, но перешла на более простую схему. Google Spanner использует TrueTime для реальных физических меток.',
  },
  {
    id: 'sd-cap-034',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какая модель консистентности используется в DNS?',
    options: [
      'Strong consistency',
      'Linearizability',
      'Eventual consistency',
      'Sequential consistency',
    ],
    correctIndex: 2,
    explanation:
      'DNS использует eventual consistency. При обновлении DNS-записи изменение постепенно распространяется через иерархию DNS-серверов и кэши. В течение TTL (Time To Live) разные пользователи могут получать разные значения. DNS -- классический пример AP-системы: он всегда доступен (резолверы всегда отвечают), устойчив к разделениям (работает при отказе части серверов), но не гарантирует немедленную консистентность (старые записи в кэшах).',
  },
  {
    id: 'sd-cap-035',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'open',
    question: 'Опишите подход Conflict Resolution в Amazon DynamoDB. Как обрабатываются конфликтующие записи?',
    sampleAnswer:
      'DynamoDB изначально (Dynamo paper, 2007) использовал vector clocks для обнаружения конфликтов и отдавал клиенту все конфликтующие версии для разрешения (semantic reconciliation). Пример: корзина покупок -- при конфликте объединить items из обеих версий. Современный DynamoDB (AWS) упростил модель: 1) Single-item transactions по умолчанию используют Last Writer Wins на основе timestamp. 2) DynamoDB Transactions (добавлены в 2018) обеспечивают serializable ACID для операций над несколькими items. 3) Conditional writes (PutItem with ConditionExpression) позволяют реализовать optimistic locking. Выбор: LWW для простоты (если потеря конкурентных обновлений приемлема), conditional writes для critical data, transactions для ACID-требований.',
    explanation:
      'Эволюция DynamoDB показывает трейд-офф между простотой и гибкостью. Первоначальный vector clocks + client-side resolution был слишком сложен для большинства use cases. LWW проще, но может терять данные. Transactions решают проблему, но дороже.',
  },
  {
    id: 'sd-cap-036',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое Fencing Token и какую проблему он решает?',
    options: [
      'Токен для шифрования данных между узлами',
      'Монотонно возрастающий идентификатор, предотвращающий операции от устаревшего лидера (zombie leader)',
      'Токен для аутентификации клиентов',
      'Идентификатор сетевого раздела',
    ],
    correctIndex: 1,
    explanation:
      'Fencing Token -- монотонно возрастающий номер, выдаваемый при получении лока или избрании лидера. Проблема: узел получил лок, завис (GC pause, network delay), лок истёк, другой узел получил лок. Первый узел "просыпается" и пытается записать данные, не зная, что лок потерян (zombie leader). Решение: каждая операция с хранилищем включает fencing token. Хранилище отклоняет операции со старым токеном. Пример: token=33 выдан старому лидеру, token=34 -- новому. Хранилище принимает только операции с token >= 34.',
  },
  {
    id: 'sd-cap-037',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'open',
    question: 'Как Google Spanner достигает внешней консистентности (external consistency) с помощью TrueTime?',
    sampleAnswer:
      'External consistency (строже linearizability): если транзакция T1 commit-ится до начала T2, то T1 должна быть видна T2 во всех узлах. Проблема: невозможно определить "до" без синхронизированных часов. TrueTime API: TT.now() возвращает интервал [earliest, latest] с гарантией, что реальное время находится внутри. Реализация: атомные часы и GPS-приёмники в каждом дата-центре, погрешность обычно < 7ms. Commit wait: Spanner назначает транзакции commit timestamp s и ждёт, пока TT.after(s) не станет true (реальное время гарантированно прошло s). Это гарантирует, что транзакции с более ранним timestamp видимы всем. Цена: дополнительная latency записи (~7ms). Read-only транзакции не требуют координации -- выбирается safe timestamp, и данные читаются из любой реплики.',
    explanation:
      'TrueTime -- уникальная инновация Google, требующая специального оборудования. CockroachDB достигает похожих гарантий с HLC, но с бо́льшими clock skew может требовать retry. Spanner показал, что distributed serializable transactions возможны в глобальном масштабе.',
  },
  {
    id: 'sd-cap-038',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое Lease в распределённых системах?',
    options: [
      'Постоянное право владения ресурсом',
      'Временное право на эксклюзивный доступ к ресурсу с автоматическим истечением',
      'Договор между сервисами о формате данных',
      'Механизм репликации данных',
    ],
    correctIndex: 1,
    explanation:
      'Lease -- временное право на эксклюзивный доступ к ресурсу, автоматически истекающее через заданное время. Преимущество перед обычным локом: если держатель lease завис или упал, lease истечёт, и ресурс станет доступен другим. Держатель должен периодически продлевать lease. Применения: leader election (лидер держит lease на роль), distributed locks (ZooKeeper, etcd используют lease-based locks), cache invalidation. Важно: держатель должен проверять, что lease ещё действителен, перед критическими операциями (или использовать fencing tokens).',
  },
  {
    id: 'sd-cap-039',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'open',
    question: 'Приведите примеры систем с сильной консистентностью (strong consistency) и систем с eventual consistency.',
    sampleAnswer:
      'Strong consistency: 1) Google Spanner -- globally distributed database с serializable transactions. 2) etcd -- key-value store для Kubernetes, использует Raft. 3) Apache ZooKeeper -- координационный сервис с linearizable operations. 4) CockroachDB -- distributed SQL с serializable isolation. 5) PostgreSQL с синхронной репликацией. Eventual consistency: 1) Apache Cassandra -- distributed database оптимизированная для availability и write throughput. 2) Amazon DynamoDB (по умолчанию) -- managed NoSQL с eventual consistent reads. 3) DNS -- записи распространяются постепенно через кэши. 4) Amazon S3 -- объектное хранилище с eventual consistency для некоторых операций. 5) Riak -- distributed key-value store.',
    explanation:
      'Выбор модели зависит от требований приложения. Для финансовых транзакций, inventory management -- strong consistency. Для социальных лент, счётчиков просмотров, рекомендаций -- eventual consistency часто достаточна и даёт лучшую производительность.',
  },
  {
    id: 'sd-cap-040',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое Jepsen и для чего он используется?',
    options: [
      'Алгоритм консенсуса',
      'Фреймворк для тестирования распределённых систем на корректность при сетевых сбоях',
      'База данных с гарантией linearizability',
      'Протокол репликации',
    ],
    correctIndex: 1,
    explanation:
      'Jepsen -- фреймворк и серия тестов, созданных Кайлом Кингсбери (Kyle Kingsbury, aka Aphyr), для проверки гарантий консистентности распределённых систем. Jepsen вносит сетевые разделения, убивает узлы, вызывает задержки и проверяет, соблюдает ли система заявленные гарантии. Jepsen выявил баги в MongoDB, Cassandra, Elasticsearch, CockroachDB, Redis и многих других системах. Тесты публикуются на jepsen.io. Jepsen стал стандартом верификации для production-ready distributed systems.',
  },
  {
    id: 'sd-cap-041',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'open',
    question: 'Объясните разницу между strong consistency, sequential consistency и eventual consistency.',
    sampleAnswer:
      '1) Strong consistency (linearizability): все операции выглядят так, будто выполняются мгновенно в какой-то момент между началом и завершением, и все клиенты видят одинаковый порядок. Как если бы была одна копия данных. Самая строгая модель. 2) Sequential consistency: все клиенты видят одинаковый порядок операций, но этот порядок может отличаться от реального временного порядка. Операции каждого клиента сохраняют свой относительный порядок (program order). Слабее linearizability. 3) Eventual consistency: если прекратить обновления, все реплики в конечном счёте придут к одинаковому состоянию. Нет гарантий о порядке или времени конвергенции. Самая слабая модель. Между ними существуют промежуточные модели: causal consistency, read-your-writes, monotonic reads.',
    explanation:
      'На практике большинство приложений могут работать с eventual consistency, если предусмотреть UI/UX для случаев рассинхронизации (индикаторы "сохранение...", оптимистичные обновления с откатом при ошибке).',
  },
  {
    id: 'sd-cap-042',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Какой подход используется в Cassandra для достижения консистентности при чтении?',
    options: [
      'Читает только с одного мастер-узла',
      'Read Repair: при чтении с нескольких реплик обнаруживает расхождения и асинхронно исправляет устаревшие реплики',
      'Использует двухфазный коммит для каждого чтения',
      'Кэширует все данные в памяти',
    ],
    correctIndex: 1,
    explanation:
      'Read Repair в Cassandra: при чтении с consistency level > ONE (например, QUORUM), координатор запрашивает данные у нескольких реплик. Если версии расходятся, координатор: 1) Возвращает клиенту самую свежую версию (по timestamp). 2) Асинхронно отправляет эту версию устаревшим репликам для коррекции. Это "ленивый" механизм анти-энтропии, работающий только для читаемых данных. Дополнительно Cassandra использует Anti-Entropy Repair (nodetool repair) для полной синхронизации через Merkle trees.',
  },
  {
    id: 'sd-cap-043',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое Gossip Protocol и где он используется?',
    options: [
      'Протокол для шифрования сообщений',
      'Эпидемический протокол распространения информации между узлами через случайный обмен',
      'Протокол выбора лидера',
      'Протокол синхронной репликации',
    ],
    correctIndex: 1,
    explanation:
      'Gossip Protocol (epidemic protocol) -- метод распространения информации в распределённой системе. Каждый узел периодически выбирает случайного соседа и обменивается с ним информацией. Информация распространяется экспоненциально быстро (как слухи или эпидемия). Свойства: eventually consistent, scalable, fault-tolerant. Применение: membership detection (какие узлы живы), распространение метаданных кластера, failure detection. Cassandra использует gossip для обмена информацией о топологии кластера и состоянии узлов.',
  },
  {
    id: 'sd-cap-044',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'open',
    question: 'Как реализовать distributed lock с помощью Redis? Какие проблемы могут возникнуть?',
    sampleAnswer:
      'Простейшая реализация: SET key value NX PX timeout (NX -- only if not exists, PX -- expiry in ms). Освобождение: проверить value (чтобы не удалить чужой лок) и DEL. Проблемы: 1) Clock drift: если узел с локом завис дольше timeout, лок истечёт, другой получит его, первый "проснётся" и продолжит работу, считая лок своим. Решение: fencing token. 2) Single Redis: при падении Redis лок теряется. 3) Redis Sentinel failover: лок на мастере может не успеть реплицироваться на слейв до промоушена -- два клиента получат лок. Redlock: алгоритм Мартина Клеппмана для Redis cluster -- получение лока на большинстве независимых мастеров. Критика: Redlock сложен и не гарантирует корректность при определённых failure modes. Рекомендация: для критичных локов использовать ZooKeeper/etcd с Raft.',
    explanation:
      'Distributed locks -- одна из сложнейших задач. Мартин Клеппман (автор "Designing Data-Intensive Applications") и Сальваторе Санфилиппо (создатель Redis) публично дискутировали о корректности Redlock. Вывод: если потеря лока может привести к катастрофе -- используйте CP-систему (ZooKeeper, etcd), а не Redis.',
  },
  {
    id: 'sd-cap-045',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какая система является примером CP-системы, используемой для хранения конфигурации Kubernetes?',
    options: [
      'Redis',
      'MongoDB',
      'etcd',
      'Cassandra',
    ],
    correctIndex: 2,
    explanation:
      'etcd -- distributed key-value store, используемый Kubernetes для хранения всего состояния кластера (pods, services, configmaps, secrets). etcd использует Raft consensus, обеспечивая strong consistency. При сетевом разделении меньшинство узлов etcd перестаёт обслуживать запросы (CP-поведение). Это критично для Kubernetes: лучше временно не принимать новые команды, чем допустить split-brain с конфликтующими состояниями.',
  },
  {
    id: 'sd-cap-046',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'open',
    question: 'Что такое Session Guarantees и какие типы гарантий существуют?',
    sampleAnswer:
      'Session Guarantees -- набор гарантий консистентности, предоставляемых в рамках одной клиентской сессии. Типы: 1) Read Your Writes: клиент после записи видит свою запись при последующих чтениях. 2) Monotonic Reads: если клиент прочитал версию X, последующие чтения вернут X или более новую версию, никогда более старую. 3) Monotonic Writes: записи клиента применяются в порядке их отправки. 4) Writes Follow Reads: если клиент прочитал X и затем записал Y, то Y основан на версии не старше X. Реализация: sticky sessions (привязка клиента к одной реплике), передача version vector/timestamp между запросами, causal consistency protocols. Эти гарантии слабее strong consistency, но обеспечивают интуитивное поведение для пользователя.',
    explanation:
      'Session guarantees -- компромисс между eventual consistency (слишком слабо для хорошего UX) и strong consistency (слишком дорого). Многие приложения нуждаются только в read-your-writes -- пользователь должен видеть свои изменения, а изменения других могут приходить с задержкой.',
  },
  {
    id: 'sd-cap-047',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое Write-Ahead Log (WAL) и как он обеспечивает durability?',
    options: [
      'Лог для отслеживания пользовательских действий',
      'Техника, при которой изменения записываются в лог до применения к основному хранилищу, обеспечивая восстановление после сбоя',
      'Журнал ошибок системы',
      'Лог для дебаггинга',
    ],
    correctIndex: 1,
    explanation:
      'Write-Ahead Log (WAL) -- техника обеспечения durability: перед изменением данных в основном хранилище, изменение записывается в append-only лог на диске. При сбое система восстанавливается, воспроизводя незавершённые записи из WAL. Преимущества: sequential writes в WAL быстрее random writes, атомарность операций. Используется в: PostgreSQL (WAL), MySQL (redo log), Kafka (log-structured storage), LSM-trees (LevelDB, RocksDB). В распределённых системах WAL реплицируется через консенсус (Raft log = WAL).',
  },
  {
    id: 'sd-cap-048',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какой consistency level в Cassandra гарантирует чтение последней записанной версии?',
    options: [
      'ONE',
      'LOCAL_ONE',
      'QUORUM (при условии W=QUORUM для записи)',
      'ANY',
    ],
    correctIndex: 2,
    explanation:
      'В Cassandra для гарантии чтения последней записи нужно: R + W > N, где N -- replication factor. При QUORUM для чтения и записи: если N=3, QUORUM=2, то R=2, W=2, R+W=4>3 -- гарантия strong consistency. Уровень ONE обеспечивает максимальную доступность, но eventual consistency -- можно прочитать устаревшие данные. LOCAL_QUORUM обеспечивает консистентность в пределах дата-центра. ANY позволяет запись даже если все реплики недоступны (hinted handoff) -- самый слабый уровень.',
  },
  {
    id: 'sd-cap-049',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'open',
    question: 'Объясните проблему Write Skew и как serializable isolation её предотвращает.',
    sampleAnswer:
      'Write Skew -- аномалия, возникающая при snapshot isolation, когда две транзакции читают пересекающиеся данные и делают взаимоисключающие записи. Пример: два врача дежурят, правило -- минимум один должен остаться. Оба одновременно проверяют: "сколько дежурит?" -- видят 2, решают уйти, обновляют свою запись. Результат: никто не дежурит. При snapshot isolation обе транзакции видят snapshot, где дежурят 2, и не конфликтуют по записям (пишут разные строки). Serializable isolation предотвращает это: 1) Serializable Snapshot Isolation (SSI): обнаруживает опасные паттерны read-write зависимостей и абортирует одну транзакцию. 2) Actual serial execution: выполнение транзакций последовательно (Redis, VoltDB). 3) Two-Phase Locking (2PL): блокировки на чтение предотвращают конкурентное изменение.',
    explanation:
      'Write skew сложно обнаружить на уровне приложения. Если бизнес-логика имеет constraints, охватывающие несколько строк, snapshot isolation недостаточно -- нужен serializable. PostgreSQL поддерживает SSI с версии 9.1.',
  },
  {
    id: 'sd-cap-050',
    block: 'sd',
    topic: 'cap-theorem',
    topicLabel: 'CAP-теорема',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое External Consistency и чем она отличается от Linearizability?',
    options: [
      'Это одно и то же, просто разные названия',
      'External consistency гарантирует, что порядок транзакций соответствует их реальному временному порядку; linearizability определяется для отдельных операций',
      'External consistency слабее linearizability',
      'External consistency применима только к нереляционным базам',
    ],
    correctIndex: 1,
    explanation:
      'External Consistency (strict serializability) -- гарантия для транзакций: если транзакция T1 commit-ится до начала T2 в реальном времени, то T1 должна предшествовать T2 в serialization order. Linearizability определена для отдельных операций (read/write), а не транзакций. External consistency = linearizability + serializability. Google Spanner обеспечивает external consistency благодаря TrueTime. Это самая строгая модель консистентности, создающая иллюзию единственной последовательной копии данных для всех наблюдателей.',
  },
];
