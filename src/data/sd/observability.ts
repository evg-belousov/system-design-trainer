import type { Question } from '../types';

export const observabilityQuestions: Question[] = [
  {
    id: 'sd-observability-001',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какие три столпа (pillars) observability выделяют в современных распределённых системах?',
    options: [
      'CPU, Memory, Disk',
      'Logs, Metrics, Traces',
      'Alerts, Dashboards, Reports',
      'Availability, Latency, Throughput',
    ],
    correctIndex: 1,
    explanation:
      'Три столпа observability: Logs (логи) -- дискретные события с контекстом; Metrics (метрики) -- числовые измерения, агрегированные во времени; Traces (трейсы) -- путь запроса через распределённую систему. Каждый столп даёт уникальную перспективу: логи объясняют «что произошло», метрики показывают «насколько хорошо система работает», трейсы раскрывают «где именно возникла проблема» в цепочке сервисов.',
  },
  {
    id: 'sd-observability-002',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое structured logging и чем он отличается от обычного текстового логирования?',
    options: [
      'Логирование только в файлы определённого формата (.log)',
      'Запись логов в структурированном формате (JSON) с набором типизированных полей вместо произвольного текста',
      'Логирование с обязательным указанием уровня (INFO, WARN, ERROR)',
      'Хранение логов в реляционной базе данных с жёсткой схемой',
    ],
    correctIndex: 1,
    explanation:
      'Structured logging -- запись логов в формате ключ-значение (обычно JSON), где каждое поле имеет имя и тип. Пример: {"timestamp":"2024-01-15T10:30:00Z","level":"error","service":"payment","trace_id":"abc123","user_id":"u456","message":"Payment failed","error_code":"INSUFFICIENT_FUNDS"}. Это позволяет эффективно искать, фильтровать и агрегировать логи с помощью инструментов (ELK, Loki), в отличие от произвольного текста, который требует regex-парсинга.',
  },
  {
    id: 'sd-observability-003',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Для чего используется Prometheus в стеке мониторинга?',
    options: [
      'Для визуализации метрик и создания дашбордов',
      'Для сбора, хранения и запроса метрик временных рядов (time series)',
      'Для распределённого трейсинга запросов между микросервисами',
      'Для агрегации и индексирования логов',
    ],
    correctIndex: 1,
    explanation:
      'Prometheus -- open-source система мониторинга, специализирующаяся на сборе и хранении метрик в формате time series. Использует pull-модель: периодически опрашивает (scrape) /metrics эндпоинты сервисов. Имеет собственный язык запросов PromQL для анализа метрик. Prometheus хранит данные локально и поддерживает алертинг через Alertmanager. Для визуализации обычно используется Grafana как отдельный компонент.',
  },
  {
    id: 'sd-observability-004',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что означает аббревиатура SLO в контексте надёжности сервисов?',
    options: [
      'Service Level Objective -- целевой уровень качества сервиса, выраженный как процент (например, 99.9% доступности)',
      'Service Load Optimizer -- компонент для оптимизации нагрузки',
      'System Logging Output -- стандарт вывода логов',
      'Service Latency Overview -- обзор задержек сервиса',
    ],
    correctIndex: 0,
    explanation:
      'SLO (Service Level Objective) -- целевое значение для метрики качества сервиса. Например, «99.9% запросов должны обрабатываться быстрее 200мс за 30-дневное окно». SLO определяется на основе SLI (Service Level Indicator -- конкретная метрика), а SLA (Service Level Agreement) -- это юридическое соглашение с клиентом, включающее SLO и последствия его нарушения (компенсации). Иерархия: SLI → SLO → SLA.',
  },
  {
    id: 'sd-observability-005',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какой компонент стека ELK отвечает за сбор и преобразование логов перед отправкой в хранилище?',
    options: [
      'Elasticsearch',
      'Logstash',
      'Kibana',
      'Filebeat',
    ],
    correctIndex: 1,
    explanation:
      'ELK Stack: Elasticsearch (хранение и поиск), Logstash (сбор, парсинг, трансформация логов), Kibana (визуализация). Logstash принимает логи из различных источников (файлы, syslog, Kafka), трансформирует их (парсинг, обогащение, фильтрация) и отправляет в Elasticsearch. На практике часто используют Filebeat (легковесный агент) для сбора логов на хостах и передачи в Logstash или напрямую в Elasticsearch.',
  },
  {
    id: 'sd-observability-006',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое distributed tracing (распределённый трейсинг)?',
    options: [
      'Отслеживание изменений в исходном коде через систему контроля версий',
      'Отслеживание пути запроса через цепочку микросервисов с измерением времени каждого этапа',
      'Мониторинг сетевых маршрутов между серверами (traceroute)',
      'Трассировка системных вызовов процесса (strace)',
    ],
    correctIndex: 1,
    explanation:
      'Distributed tracing отслеживает прохождение запроса через все сервисы в распределённой системе. Каждый trace состоит из spans -- отрезков работы в отдельных сервисах. Span содержит: имя операции, время начала и конца, теги, ссылку на parent span. Trace ID передаётся между сервисами через заголовки (например, W3C Trace Context). Инструменты: Jaeger, Zipkin, AWS X-Ray, Datadog APM. Tracing незаменим для диагностики latency в микросервисах.',
  },
  {
    id: 'sd-observability-007',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что описывает метод RED для мониторинга микросервисов?',
    options: [
      'Resources, Errors, Duration -- метрики ресурсов хоста',
      'Rate, Errors, Duration -- ключевые метрики запросов к сервису',
      'Replicas, Events, Deployments -- метрики оркестрации',
      'Read, Execute, Delete -- метрики операций с данными',
    ],
    correctIndex: 1,
    explanation:
      'Метод RED (предложен Томом Уилки из Grafana Labs) определяет три ключевые метрики для мониторинга request-driven сервисов: Rate -- количество запросов в секунду; Errors -- количество неуспешных запросов; Duration -- распределение времени обработки запросов (latency). RED оптимален для микросервисов и API. В отличие от USE (Utilization, Saturation, Errors), который фокусируется на инфраструктурных ресурсах (CPU, память, диск, сеть), RED фокусируется на пользовательском опыте.',
  },
  {
    id: 'sd-observability-008',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какая модель сбора метрик используется в Prometheus по умолчанию?',
    options: [
      'Push-модель: сервисы отправляют метрики в Prometheus',
      'Pull-модель: Prometheus периодически опрашивает /metrics эндпоинты сервисов',
      'Event-driven: Prometheus подписывается на события через message broker',
      'Streaming: сервисы устанавливают постоянное WebSocket-соединение с Prometheus',
    ],
    correctIndex: 1,
    explanation:
      'Prometheus использует pull-модель: он сам периодически (scrape interval, обычно 15-30с) отправляет HTTP GET на /metrics эндпоинт каждого сервиса. Преимущества pull: легко определить, что сервис недоступен (failed scrape); нет необходимости в discovery на стороне сервиса; Prometheus контролирует частоту сбора. Для короткоживущих задач (batch jobs), которые могут завершиться до scrape, используется Pushgateway как промежуточное звено.',
  },
  {
    id: 'sd-observability-009',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Чем SLI отличается от SLO, и как они связаны с SLA?',
    options: [
      'SLI -- это метрика, SLO -- целевое значение этой метрики, SLA -- юридическое соглашение, включающее SLO и последствия нарушения',
      'SLI, SLO и SLA -- это синонимы, обозначающие уровень доступности сервиса',
      'SLI определяется бизнесом, SLO -- инженерами, SLA -- юристами, они не связаны между собой',
      'SLI применяется к внутренним сервисам, SLO -- к внешним, SLA -- к облачным провайдерам',
    ],
    correctIndex: 0,
    explanation:
      'SLI (Service Level Indicator) -- конкретная измеримая метрика: доля успешных запросов, задержка p99, процент доступности. SLO (Service Level Objective) -- целевое значение SLI: «p99 latency < 200ms для 99.9% запросов за 30 дней». SLA (Service Level Agreement) -- формальный договор с клиентом: «Если SLO нарушен, клиент получает 10% компенсацию». Обычно SLO устанавливают строже, чем SLA, чтобы иметь «бюджет ошибок» (error budget) до нарушения юридических обязательств.',
  },
  {
    id: 'sd-observability-010',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какой тип метрик в Prometheus представляет значение, которое может как увеличиваться, так и уменьшаться (например, количество активных соединений)?',
    options: [
      'Counter',
      'Gauge',
      'Histogram',
      'Summary',
    ],
    correctIndex: 1,
    explanation:
      'Gauge (шкала) -- тип метрики, значение которой может произвольно увеличиваться и уменьшаться. Примеры: температура CPU, количество активных запросов, размер очереди, использование памяти. Counter -- монотонно возрастающий счётчик (количество запросов, ошибок), сбрасывается только при перезапуске. Histogram -- распределение значений по корзинам (bucket), используется для latency. Summary -- аналог histogram, но рассчитывает квантили на клиенте.',
  },
  {
    id: 'sd-observability-011',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'open',
    question: 'Опишите метод USE (Utilization, Saturation, Errors) для мониторинга инфраструктуры. Приведите примеры метрик для CPU, памяти и диска.',
    sampleAnswer:
      'USE Method (предложен Бренданом Греггом) применяется к каждому ресурсу системы: Utilization (утилизация) -- доля времени, когда ресурс занят. CPU: % использования, Memory: % занятой RAM, Disk: % I/O utilization. Saturation (насыщение) -- объём работы, который ресурс не может обслужить (очередь). CPU: run queue length (load average), Memory: swap usage / OOM events, Disk: I/O wait queue depth. Errors (ошибки) -- количество ошибок ресурса. CPU: machine check exceptions, Memory: ECC errors, Disk: read/write errors, bad sectors. Метод USE отлично подходит для быстрого анализа «узких мест» инфраструктуры, а RED -- для анализа приложений. Вместе они дают полную картину.',
    explanation:
      'USE Method систематизирует анализ производительности: для каждого физического ресурса (CPU, память, диск, сеть, файловые дескрипторы) проверяются три аспекта. Это позволяет быстро находить bottleneck и исключать ресурсы, которые работают нормально. Brendan Gregg также предложил метод TSA (Thread State Analysis) для анализа производительности приложений.',
  },
  {
    id: 'sd-observability-012',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'open',
    question: 'Как работает distributed tracing? Объясните понятия trace, span и context propagation.',
    sampleAnswer:
      'Distributed tracing отслеживает путь одного запроса через все микросервисы. Trace -- полный путь запроса от начала до конца, идентифицируемый уникальным Trace ID. Span -- единица работы внутри одного сервиса (например, HTTP-обработчик, запрос к БД). Каждый span имеет: span ID, parent span ID, имя операции, timestamps (start/end), теги и логи. Context Propagation -- механизм передачи Trace ID и Span ID между сервисами через HTTP-заголовки (W3C Trace Context: traceparent, tracestate) или gRPC metadata. При получении запроса сервис извлекает контекст, создаёт дочерний span и передаёт обновлённый контекст дальше. OpenTelemetry -- современный стандарт, объединяющий OpenTracing и OpenCensus, предоставляющий SDK для автоматической инструментации.',
    explanation:
      'Без distributed tracing отладка проблем в микросервисах превращается в поиск иголки в стоге сена: нужно коррелировать логи десятков сервисов вручную. Tracing позволяет визуализировать waterfall-диаграмму запроса и мгновенно определить, какой сервис вызывает задержку. Google Dapper (2010) -- одна из первых публикаций о distributed tracing в продакшене.',
  },
  {
    id: 'sd-observability-013',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'open',
    question: 'Какие стратегии алертинга вы знаете? Как избежать «alert fatigue» (усталости от алертов)?',
    sampleAnswer:
      'Стратегии алертинга: 1) Threshold-based -- алерт при превышении порога (CPU > 90%). Простой, но подвержен false positives. 2) Rate of change -- алерт при аномальной скорости изменения (error rate удвоился за 5 минут). 3) Anomaly detection -- ML-модели определяют отклонение от нормы. 4) SLO-based alerting (рекомендуется Google SRE) -- алерт при расходовании error budget: burn rate alerting (скорость сжигания бюджета ошибок). Избежание alert fatigue: иерархия severity (critical/warning/info); только actionable алерты попадают в on-call; группировка связанных алертов; runbook для каждого алерта; регулярная ревизия алертов (удаление бесполезных); routing по компетенциям (DB-алерты -- DBA-команде). Золотое правило: каждый алерт должен требовать немедленного человеческого действия, иначе это не алерт, а уведомление.',
    explanation:
      'Google SRE Book рекомендует мультиоконный мультибёрнрейт алертинг: быстрый burn rate за короткое окно (5% бюджета за 1 час) вызывает page, медленный burn rate за длинное окно (10% за 3 дня) вызывает ticket. Это резко снижает количество ложных срабатываний при сохранении чувствительности к реальным инцидентам.',
  },
  {
    id: 'sd-observability-014',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое APM (Application Performance Monitoring) и какую информацию он предоставляет?',
    options: [
      'Мониторинг только серверного оборудования (CPU, память, диск)',
      'Комплексный мониторинг производительности приложения: транзакции, latency, error rates, зависимости, профилирование кода',
      'Мониторинг исключительно бизнес-метрик (конверсии, выручка)',
      'Средство для автоматического исправления ошибок в коде',
    ],
    correctIndex: 1,
    explanation:
      'APM (Application Performance Monitoring) предоставляет глубокий insight в работу приложения: автоматическое обнаружение и трейсинг транзакций, карта зависимостей между сервисами, профилирование кода (какие методы занимают больше времени), мониторинг баз данных (медленные запросы), Real User Monitoring (RUM) для фронтенда. Примеры: Datadog APM, New Relic, Dynatrace, Elastic APM. APM объединяет метрики, трейсы и логи в единый контекст.',
  },
  {
    id: 'sd-observability-015',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Какой подход к сэмплированию (sampling) в distributed tracing позволяет принять решение о записи трейса после его завершения?',
    options: [
      'Head-based sampling -- решение принимается в начале трейса',
      'Tail-based sampling -- решение принимается после сбора всех spans трейса',
      'Probabilistic sampling -- фиксированный процент трейсов записывается',
      'Rate limiting sampling -- ограничение количества трейсов в секунду',
    ],
    correctIndex: 1,
    explanation:
      'Tail-based sampling собирает все spans трейса в буфер (обычно в collector) и принимает решение о сохранении после завершения. Это позволяет всегда сохранять трейсы с ошибками, аномально высоким latency или специфическими атрибутами, даже если они редки. Head-based sampling (решение в начале) проще и дешевле, но может пропустить интересные трейсы. Tail-based sampling требует больше памяти и вычислительных ресурсов в collector, но обеспечивает значительно более качественные данные для отладки.',
  },
  {
    id: 'sd-observability-016',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое error budget и как он связан с SLO?',
    options: [
      'Финансовый бюджет, выделенный на исправление ошибок в коде',
      'Допустимое количество ошибок (100% минус SLO), которое можно потратить на развёртывание новых функций и эксперименты',
      'Максимальное количество инцидентов в квартал, после которого команда увольняется',
      'Запас мощности серверов для обработки ошибочных запросов',
    ],
    correctIndex: 1,
    explanation:
      'Error budget = 100% - SLO. Если SLO = 99.9%, error budget = 0.1% (около 43 минут простоя в месяц). Пока error budget не исчерпан, команда может выпускать фичи, проводить эксперименты, миграции. Когда error budget исчерпан, приоритет переключается на надёжность: замораживание релизов, исправление технического долга. Это формализует баланс между скоростью разработки и надёжностью, убирая субъективные оценки из дискуссий между product и engineering.',
  },
  {
    id: 'sd-observability-017',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'open',
    question: 'Спроектируйте систему observability для платформы из 50+ микросервисов. Какие компоненты выберете и как обеспечите корреляцию между логами, метриками и трейсами?',
    sampleAnswer:
      'Архитектура: 1) Метрики: Prometheus + Thanos (или Mimir) для long-term storage и multi-cluster; Grafana для визуализации. Каждый сервис экспортирует RED-метрики через /metrics. 2) Логи: структурированные JSON-логи → Filebeat/FluentBit → Kafka (буфер) → Elasticsearch (или Loki для экономии). Kibana/Grafana для поиска. 3) Трейсы: OpenTelemetry SDK во всех сервисах → OTel Collector → Jaeger/Tempo для хранения. Tail-based sampling в collector. Корреляция: единый trace_id пробрасывается через все сервисы (W3C Trace Context). Логи содержат поля trace_id и span_id. Метрики снабжаются exemplars -- ссылками на конкретные trace_id. В Grafana: из метрики → клик на exemplar → переход к трейсу → из span → переход к логам с тем же trace_id. OpenTelemetry выступает единой точкой инструментации: один SDK генерирует метрики, логи и трейсы с общим контекстом. Alerting: Alertmanager + PagerDuty для critical, Slack для warning.',
    explanation:
      'Ключ к эффективной observability -- корреляция между тремя столпами. Без неё приходится вручную сопоставлять данные из разных систем. OpenTelemetry стал де-факто стандартом, позволяя избежать vendor lock-in и обеспечить единый контекст. Стоимость observability для 50+ сервисов может быть значительной (Elasticsearch требует много ресурсов), поэтому важно планировать retention policies и sampling.',
  },
  {
    id: 'sd-observability-018',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'open',
    question: 'Объясните концепцию SLO-based alerting с burn rate. Как это работает и почему это лучше порогового алертинга?',
    sampleAnswer:
      'SLO-based alerting с burn rate отслеживает скорость расходования error budget. Burn rate = (фактическая частота ошибок) / (допустимая частота ошибок по SLO). Если SLO = 99.9% (допустимо 0.1% ошибок), а текущая частота ошибок 1%, burn rate = 1%/0.1% = 10x. Мультиоконный подход: быстрое окно (5 мин) с высоким порогом burn rate (14x) ловит острые инциденты; медленное окно (6 часов) с низким порогом (6x) ловит медленную деградацию. Преимущества: алерт напрямую связан с пользовательским опытом; автоматически адаптируется к масштабу системы; меньше false positives (краткий всплеск ошибок при большом запасе бюджета не вызывает алерт); сигнализирует о реальной угрозе нарушения SLO, а не о произвольном пороге. Реализация: в Prometheus через recording rules, вычисляющие error ratio за разные окна.',
    explanation:
      'Google SRE Book и Sloth (open-source генератор SLO для Prometheus) детально описывают этот подход. Пороговый алертинг (CPU > 90%) не учитывает контекст: 90% CPU может быть нормой для compute-intensive сервиса. SLO-based алертинг переключает фокус с «ресурс перегружен» на «пользователи страдают», что значительно повышает signal-to-noise ratio.',
  },
  {
    id: 'sd-observability-019',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'open',
    question: 'Как обеспечить observability для асинхронных систем, использующих message queues (Kafka, RabbitMQ)? Какие дополнительные метрики и подходы необходимы?',
    sampleAnswer:
      'Observability асинхронных систем имеет свои особенности: 1) Context Propagation через сообщения: trace_id и span_id сериализуются в заголовки сообщений (Kafka headers, AMQP properties). Consumer создаёт дочерний span, связывая обработку с исходным запросом. 2) Ключевые метрики очередей: consumer lag (разница между offset producer и consumer -- критичнее throughput), partition assignment, rebalance events, consumer group health, dead letter queue size, message age (время от публикации до обработки). 3) End-to-end latency: измерение полного пути сообщения (produce → broker → consume → process). 4) Idempotency tracking: мониторинг дубликатов сообщений. 5) Backpressure visibility: мониторинг скорости заполнения очередей vs скорости обработки. 6) Alerting: consumer lag растёт > N минут, DLQ size > 0, consumer не подключён. Инструменты: Kafka Exporter для Prometheus, Burrow для lag monitoring.',
    explanation:
      'В асинхронных системах проблемы проявляются с задержкой: ошибка обработки может обнаружиться спустя часы после отправки сообщения. Consumer lag -- «канарейка в угольной шахте»: если lag растёт, это ранний сигнал о проблемах с производительностью consumer или увеличении нагрузки. Dead letter queue с нулевым размером -- тоже важная метрика для алертинга.',
  },
  {
    id: 'sd-observability-020',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое OpenTelemetry и какую проблему он решает?',
    options: [
      'Проприетарный формат логов от Google для Cloud Platform',
      'Открытый стандарт и набор SDK/API для генерации, сбора и экспорта телеметрии (метрик, логов, трейсов), обеспечивающий vendor-нейтральную инструментацию',
      'Графический интерфейс для визуализации метрик, альтернатива Grafana',
      'Инструмент для нагрузочного тестирования микросервисов',
    ],
    correctIndex: 1,
    explanation:
      'OpenTelemetry (OTel) -- проект CNCF, объединивший OpenTracing и OpenCensus. Он предоставляет: единый API и SDK для инструментации (метрики, логи, трейсы); OTel Collector -- агент для сбора, обработки и экспорта телеметрии в любой backend (Jaeger, Prometheus, Datadog и др.); автоматическую инструментацию для популярных фреймворков. Главное преимущество -- vendor-нейтральность: инструментируете код один раз, меняете backend без изменения кода. OTel стал де-факто стандартом индустрии.',
  },
  {
    id: 'sd-observability-021',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое Grafana и какую роль она играет в стеке мониторинга?',
    options: [
      'Система хранения метрик временных рядов',
      'Платформа визуализации данных, позволяющая создавать дашборды для метрик, логов и трейсов из различных источников',
      'Агент сбора метрик с хостов и контейнеров',
      'База данных для хранения логов',
    ],
    correctIndex: 1,
    explanation:
      'Grafana -- open-source платформа для визуализации и аналитики. Она не хранит данные, а подключается к различным источникам: Prometheus, Elasticsearch, Loki, InfluxDB, MySQL и многим другим. Основные возможности: создание интерактивных дашбордов, настройка алертов, корреляция данных из разных источников на одном графике, шаблонизация (variables), аннотации для отметки событий.',
  },
  {
    id: 'sd-observability-022',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какой уровень логирования следует использовать для информации, полезной при отладке, но не нужной в продакшене?',
    options: [
      'ERROR',
      'INFO',
      'DEBUG',
      'WARN',
    ],
    correctIndex: 2,
    explanation:
      'DEBUG -- уровень логирования для детальной информации, полезной при отладке: входные параметры функций, промежуточные состояния, SQL-запросы. В продакшене DEBUG обычно отключён (слишком много данных). Иерархия уровней (от менее к более критичным): TRACE → DEBUG → INFO → WARN → ERROR → FATAL. INFO -- значимые бизнес-события, WARN -- потенциальные проблемы, ERROR -- ошибки, требующие внимания.',
  },
  {
    id: 'sd-observability-023',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое cardinality в контексте метрик и почему высокая cardinality проблематична?',
    options: [
      'Количество дашбордов в Grafana',
      'Количество уникальных комбинаций меток (labels) метрики, высокая cardinality создаёт нагрузку на хранилище и запросы',
      'Частота сбора метрик (scrape interval)',
      'Количество алертов, срабатывающих одновременно',
    ],
    correctIndex: 1,
    explanation:
      'Cardinality (мощность) -- количество уникальных временных рядов для метрики, определяемое комбинациями меток. Пример: http_requests{method, endpoint, status_code, user_id}. Если user_id -- метка, cardinality = количество пользователей × методы × endpoints × статусы = миллионы. Высокая cardinality перегружает Prometheus (память, CPU), замедляет запросы. Правило: избегать меток с неограниченным числом значений (user_id, request_id, IP).',
  },
  {
    id: 'sd-observability-024',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'open',
    question: 'Что такое health check endpoint и какую информацию он должен предоставлять?',
    sampleAnswer:
      'Health check endpoint (обычно /health или /healthz) -- специальный API-эндпоинт, позволяющий внешним системам (load balancer, Kubernetes, мониторинг) проверить работоспособность сервиса. Типы проверок: 1) Liveness -- сервис работает и не завис (процесс жив). 2) Readiness -- сервис готов принимать трафик (все зависимости доступны). Рекомендуемая информация: статус (UP/DOWN), проверки зависимостей (БД, кэш, downstream сервисы), время работы (uptime), версия приложения. Ответ: HTTP 200 = здоров, HTTP 503 = нездоров. Kubernetes использует liveness probe для перезапуска зависших pod-ов и readiness probe для исключения из балансировки.',
    explanation:
      'Health checks -- критически важная часть observability. Без них load balancer может направлять трафик на нездоровые инстансы. Kubernetes livenessProbe и readinessProbe -- стандартный паттерн. Health checks должны быть быстрыми (< 1 сек) и не влиять на производительность.',
  },
  {
    id: 'sd-observability-025',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое exemplar в контексте метрик Prometheus?',
    options: [
      'Пример конфигурации Prometheus',
      'Ссылка на конкретный trace_id в метрике, позволяющая перейти от агрегированных данных к детальному трейсу',
      'Шаблон для создания дашбордов',
      'Тип визуализации в Grafana',
    ],
    correctIndex: 1,
    explanation:
      'Exemplar -- механизм связывания метрик с трейсами. Prometheus 2.25+ поддерживает хранение exemplars: к точке данных метрики (например, histogram bucket) прикрепляется trace_id конкретного запроса, попавшего в этот bucket. В Grafana: клик на точку графика → переход к соответствующему трейсу в Jaeger/Tempo. Это ключевой механизм корреляции между метриками и трейсами, позволяющий ответить на вопрос «почему latency выросла?» переходом к конкретным медленным запросам.',
  },
  {
    id: 'sd-observability-026',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Чем Loki отличается от Elasticsearch для хранения логов?',
    options: [
      'Loki быстрее индексирует все поля логов',
      'Loki индексирует только метаданные (labels), а не содержимое логов, что делает его более экономичным в хранении и проще в эксплуатации',
      'Loki поддерживает только JSON-формат логов',
      'Elasticsearch не может работать с Grafana',
    ],
    correctIndex: 1,
    explanation:
      'Grafana Loki -- система агрегации логов, вдохновлённая Prometheus. Ключевое отличие от ELK: Loki НЕ индексирует содержимое логов, только метки (labels) как {app="api", env="prod"}. Текст логов хранится в сжатом виде и сканируется при запросе. Преимущества: в 10-100x дешевле по хранению, проще операционно (нет сложной кластеризации ES), единый язык запросов LogQL (похож на PromQL). Недостаток: полнотекстовый поиск медленнее, чем в ES. Идеально для cloud-native стека с Prometheus и Grafana.',
  },
  {
    id: 'sd-observability-027',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое span и какие атрибуты он содержит в distributed tracing?',
    options: [
      'Временной интервал между двумя метриками',
      'Единица работы в трейсе: имя операции, время начала/конца, parent span ID, теги, логи и статус',
      'Сетевой пакет между микросервисами',
      'Запись в логе с timestamp',
    ],
    correctIndex: 1,
    explanation:
      'Span -- базовый строительный блок трейса, представляющий единицу работы. Атрибуты: 1) Operation name -- имя операции (HTTP GET /api/users). 2) Start/end timestamps -- время начала и конца. 3) Span ID -- уникальный идентификатор. 4) Parent span ID -- ссылка на родительский span (образует дерево). 5) Tags/attributes -- ключ-значение (http.status_code=200, db.type=postgresql). 6) Events/logs -- временные события внутри span. 7) Status -- успех/ошибка. Трейс состоит из корневого span и дочерних spans, образующих DAG (directed acyclic graph).',
  },
  {
    id: 'sd-observability-028',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'open',
    question: 'Как организовать log aggregation в Kubernetes-кластере? Опишите популярные подходы.',
    sampleAnswer:
      'Подходы к сбору логов в Kubernetes: 1) DaemonSet с log collector (Fluentd/Fluent Bit/Filebeat): pod на каждом node собирает логи из /var/log/containers/*, парсит, обогащает Kubernetes-метаданными (pod name, namespace, labels) и отправляет в хранилище. 2) Sidecar container: контейнер-sidecar в каждом pod читает логи приложения и отправляет. Гибче, но больше ресурсов. 3) Direct logging: приложение отправляет логи напрямую (через SDK). Проще, но теряются логи при crash. Стек: Fluent Bit (легковесный collector) → Kafka (буфер) → Elasticsearch или Loki → Kibana/Grafana. Promtail -- сборщик логов для Loki, специально для Kubernetes. Важно: structured logging (JSON) упрощает парсинг.',
    explanation:
      'DaemonSet с Fluent Bit -- наиболее распространённый паттерн: один pod на node, минимальный overhead, автоматический сбор всех контейнерных логов. Kubernetes добавляет метаданные автоматически. Для high-throughput систем важен буферный слой (Kafka) для защиты от пиков и потерь.',
  },
  {
    id: 'sd-observability-029',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'open',
    question: 'Объясните Golden Signals от Google SRE. Какие четыре метрики они включают?',
    sampleAnswer:
      'Golden Signals -- четыре ключевые метрики для мониторинга user-facing систем по методологии Google SRE: 1) Latency -- время обработки запросов. Важно разделять успешные и ошибочные (ошибка может быть быстрой). Метрики: p50, p95, p99. 2) Traffic -- объём нагрузки на систему: RPS, requests per second. 3) Errors -- частота ошибок: HTTP 5xx, exceptions, failed requests. Процент или абсолютное значение. 4) Saturation -- насыщение ресурсов: CPU, память, очереди, connections. Показывает, насколько система близка к пределу. Golden Signals -- расширение RED (Rate, Errors, Duration) добавлением Saturation. Фокус на пользовательском опыте: эти метрики напрямую влияют на SLO.',
    explanation:
      'Golden Signals -- стандарт индустрии для мониторинга сервисов. Они покрывают все аспекты здоровья сервиса с точки зрения пользователя. Комбинация с USE (для инфраструктуры) даёт полную картину. Datadog, New Relic и другие APM-системы строятся вокруг этих метрик.',
  },
  {
    id: 'sd-observability-030',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое recording rules в Prometheus?',
    options: [
      'Правила для записи логов',
      'Предварительно вычисленные запросы, сохраняемые как новые метрики для оптимизации производительности частых или тяжёлых запросов',
      'Правила маршрутизации алертов',
      'Конфигурация retention для метрик',
    ],
    correctIndex: 1,
    explanation:
      'Recording rules позволяют предварительно вычислять и сохранять результаты PromQL-запросов как новые метрики. Пример: вместо вычисления rate(http_requests_total[5m]) при каждом запросе к дашборду, recording rule вычисляет это значение периодически и сохраняет как http_requests:rate5m. Преимущества: ускорение дашбордов и алертов, снижение нагрузки на Prometheus, упрощение сложных запросов. Рекомендуется для: часто используемых запросов, SLO-метрик, сложных агрегаций.',
  },
  {
    id: 'sd-observability-031',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое Continuous Profiling и какую ценность он добавляет к observability?',
    options: [
      'Непрерывное тестирование производительности в CI/CD',
      'Постоянный сбор профилей CPU, памяти, аллокаций в продакшене с минимальным overhead, позволяющий находить bottleneck-и без воспроизведения проблемы',
      'Автоматическое создание профилей пользователей',
      'Мониторинг профиля нагрузки на сервис',
    ],
    correctIndex: 1,
    explanation:
      'Continuous Profiling -- четвёртый столп observability (после метрик, логов, трейсов). Инструменты (Parca, Pyroscope, Datadog Profiler) постоянно собирают CPU/memory профили с low overhead (1-5%). Это позволяет: находить функции, потребляющие больше всего CPU; выявлять memory leak-и; анализировать проблемы post-mortem без воспроизведения; сравнивать профили между версиями (regression detection). Интеграция с трейсами: профиль можно привязать к конкретному span.',
  },
  {
    id: 'sd-observability-032',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Как работает W3C Trace Context для propagation контекста между сервисами?',
    options: [
      'Trace ID передаётся через cookie',
      'Стандартизированные HTTP-заголовки traceparent и tracestate передают trace ID, span ID и флаги между сервисами',
      'Контекст хранится в централизованном хранилище',
      'Каждый сервис генерирует свой trace ID',
    ],
    correctIndex: 1,
    explanation:
      'W3C Trace Context -- стандарт (Recommendation) для propagation трейсинг-контекста. Заголовок traceparent: version-trace_id-parent_span_id-flags (например: 00-0af7651916cd43dd8448eb211c80319c-b7ad6b7169203331-01). Trace ID -- 32 hex символа (128 бит), parent span ID -- 16 hex (64 бит), flags -- 01 = sampled. Заголовок tracestate: vendor-specific данные (key=value пары). OpenTelemetry, Jaeger, Zipkin поддерживают W3C Trace Context. Это обеспечивает interoperability между разными системами трейсинга.',
  },
  {
    id: 'sd-observability-033',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'open',
    question: 'Опишите архитектуру OpenTelemetry Collector. Какие компоненты он включает и как их конфигурировать?',
    sampleAnswer:
      'OTel Collector -- универсальный агент для приёма, обработки и экспорта телеметрии. Архитектура pipeline: Receivers → Processors → Exporters. Receivers: принимают данные (OTLP, Jaeger, Zipkin, Prometheus, Kafka). Processors: обрабатывают данные -- batch (группировка для эффективности), memory_limiter (защита от OOM), attributes (добавление/удаление атрибутов), filter (фильтрация), tail_sampling (sampling после сбора всех spans). Exporters: отправляют данные (OTLP, Jaeger, Prometheus, Loki, различные облачные сервисы). Pipelines конфигурируются отдельно для traces, metrics, logs. Deployment: sidecar (per-pod) или DaemonSet (per-node) или standalone (centralized). Connector позволяет соединять pipelines (например, генерировать метрики из трейсов).',
    explanation:
      'OTel Collector -- центральный компонент современного observability стека. Он абстрагирует приложения от конкретных backend-ов: приложение отправляет данные в OTLP формате в Collector, Collector маршрутизирует в нужные системы. Tail-based sampling в Collector -- ключевая capability для экономии при сохранении важных трейсов.',
  },
  {
    id: 'sd-observability-034',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'open',
    question: 'Как реализовать SLO-based мониторинг и alerting? Опишите практический подход с использованием Prometheus.',
    sampleAnswer:
      'SLO-based мониторинг: 1) Определение SLI: выбор метрики (например, процент успешных запросов < 500ms). В Prometheus: histogram http_request_duration_seconds с bucket-ами. 2) SLI query: sum(rate(http_request_duration_seconds_bucket{le="0.5"}[5m])) / sum(rate(http_request_duration_seconds_count[5m])). 3) SLO target: 99.9% за 30-дневное окно. Error budget = 0.1%. 4) Recording rules: предварительно вычисляем SLI за разные окна (5m, 1h, 1d, 30d). 5) Burn rate alerting: alert при burn rate > 14x за 1h (быстрый инцидент) или > 6x за 6h (медленная деградация). 6) Error budget dashboard: визуализация оставшегося бюджета. Инструменты: Sloth (генератор SLO для Prometheus), Google SLO Generator, Nobl9. Интеграция: Alertmanager → PagerDuty для critical, Slack для warning.',
    explanation:
      'SLO-based подход фокусирует внимание на пользовательском опыте. Вместо алертов на произвольные пороги (CPU > 80%), алерты срабатывают когда пользователи реально страдают. Google SRE Book детально описывает multi-window multi-burn-rate alerting как наиболее эффективный подход.',
  },
  {
    id: 'sd-observability-035',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'open',
    question: 'Как обеспечить долгосрочное хранение метрик Prometheus? Сравните подходы Thanos и Cortex/Mimir.',
    sampleAnswer:
      'Prometheus локально хранит данные с ограниченным retention (обычно 15 дней). Для долгосрочного хранения: Thanos: компоненты интегрируются с Prometheus. Sidecar читает данные из Prometheus и загружает в object storage (S3/GCS). Querier объединяет данные из Prometheus и storage. Compactor оптимизирует хранение. Store Gateway обслуживает запросы к историческим данным. Подход: build поверх существующего Prometheus. Cortex/Mimir: полностью распределённая система. Prometheus remote-write отправляет данные в Cortex. Multi-tenant by design. Более сложная архитектура, но лучше масштабируется. Mimir -- форк Cortex от Grafana Labs, активно развивается. Выбор: Thanos проще для старта (меньше компонентов), Mimir лучше для multi-tenant и massive scale. Оба поддерживают PromQL, интегрируются с Grafana.',
    explanation:
      'Long-term storage -- ключевая capability для SLO-мониторинга (нужны данные за 30+ дней) и capacity planning. Object storage (S3) значительно дешевле, чем SSD для Prometheus. Grafana Labs активно развивает LGTM stack: Loki (logs), Grafana, Tempo (traces), Mimir (metrics).',
  },
  {
    id: 'sd-observability-036',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое Alertmanager и какую роль он играет в стеке Prometheus?',
    options: [
      'Система хранения алертов в базе данных',
      'Компонент, отвечающий за дедупликацию, группировку, маршрутизацию и отправку уведомлений (alerts) в различные каналы',
      'Инструмент для создания правил алертинга',
      'Агент мониторинга на хостах',
    ],
    correctIndex: 1,
    explanation:
      'Alertmanager -- компонент Prometheus-стека для обработки алертов. Prometheus оценивает alerting rules и отправляет срабатывания в Alertmanager. Alertmanager выполняет: grouping (объединение связанных алертов), inhibition (подавление алертов при наличии другого), silencing (временное отключение), routing (маршрутизация по командам/severity), отправку в receivers (email, Slack, PagerDuty, webhook). Без Alertmanager каждый алерт отправлялся бы отдельно, создавая шторм уведомлений.',
  },
  {
    id: 'sd-observability-037',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'open',
    question: 'Что такое percentile (перцентиль) в контексте мониторинга latency? Почему p99 важнее среднего значения?',
    sampleAnswer:
      'Percentile (перцентиль) -- значение, ниже которого находится определённый процент наблюдений. P99 (99-й перцентиль) -- значение latency, быстрее которого обработаны 99% запросов. P50 (медиана) -- 50% запросов. Почему p99 важнее среднего: среднее скрывает выбросы. Пример: 99 запросов по 10ms + 1 запрос по 1000ms = среднее 19.9ms, но p99 = 1000ms. Для пользователя 1% медленных запросов -- это тысячи недовольных пользователей в масштабе. Google и Amazon показали: 100ms дополнительной latency = -1% продаж. SLO обычно определяется как p99 < X ms, а не среднее.',
    explanation:
      'Percentiles -- стандарт измерения latency. Histogram в Prometheus позволяет вычислять перцентили: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])). Важно правильно выбрать bucket-ы histogram для точности вычисления нужных перцентилей.',
  },
  {
    id: 'sd-observability-038',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое service dependency map и как её построить?',
    options: [
      'Карта физического расположения серверов в дата-центре',
      'Визуализация связей между микросервисами, построенная на основе данных distributed tracing или service mesh',
      'Список NPM-зависимостей сервиса',
      'Диаграмма deployment pipeline',
    ],
    correctIndex: 1,
    explanation:
      'Service dependency map (service graph) показывает, какие сервисы вызывают какие, с метриками (RPS, latency, error rate) на рёбрах. Источники данных: distributed tracing (Jaeger, Zipkin, Tempo строят карту из spans), service mesh (Istio/Linkerd знают все вызовы через sidecar), APM (Datadog, New Relic). Карта помогает: понять архитектуру, найти критичные зависимости, визуализировать распространение инцидента, планировать изменения. Kiali -- UI для Istio service mesh с визуализацией графа.',
  },
  {
    id: 'sd-observability-039',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'open',
    question: 'Как настроить эффективный on-call процесс? Какие практики помогают избежать выгорания?',
    sampleAnswer:
      'Эффективный on-call: 1) Ротация: регулярная смена дежурных (обычно неделя), справедливое распределение, компенсация (отгулы, доплата). 2) Runbooks: документированные процедуры для каждого алерта -- что проверить, как исправить, когда эскалировать. 3) Escalation policy: если дежурный не ответил за N минут → эскалация на backup → на менеджера. 4) Post-incident review (blameless postmortem): анализ инцидентов без обвинений, выявление системных проблем. 5) Error budget: если бюджет исчерпан -- замораживание фич, фокус на надёжности. 6) Alert hygiene: регулярная ревизия алертов, удаление шумных/бесполезных. 7) Тестирование алертов: game days, chaos engineering. Инструменты: PagerDuty, Opsgenie, incident.io для управления инцидентами.',
    explanation:
      'On-call -- критическая часть SRE-практик. Google рекомендует: не более 25% времени на toil (рутинную работу), остальное на improvement. Если on-call слишком напряжённый -- это сигнал о проблемах с надёжностью системы, а не о необходимости увеличения команды.',
  },
  {
    id: 'sd-observability-040',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое log rotation и почему он важен?',
    options: [
      'Ротация дежурных инженеров для анализа логов',
      'Автоматическое архивирование и удаление старых лог-файлов для предотвращения заполнения диска',
      'Перемешивание записей в логах для безопасности',
      'Изменение формата логов между версиями приложения',
    ],
    correctIndex: 1,
    explanation:
      'Log rotation -- процесс управления лог-файлами: архивирование текущего лога, создание нового, удаление старых по политике retention. Без rotation логи заполняют диск → сервис падает. Инструменты: logrotate (Linux), встроенные механизмы в logging библиотеках (logback, log4j). Политики: по размеру (rotate при 100MB), по времени (ежедневно), по количеству (хранить 7 файлов). Сжатие (.gz) экономит место. В Kubernetes логи контейнеров автоматически ротируются kubelet-ом.',
  },
  {
    id: 'sd-observability-041',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое eBPF и как он используется для observability?',
    options: [
      'Формат сжатия бинарных логов',
      'Технология ядра Linux, позволяющая безопасно выполнять код в kernel space для наблюдения за системными событиями без модификации приложений',
      'Протокол передачи метрик между сервисами',
      'База данных для хранения трейсов',
    ],
    correctIndex: 1,
    explanation:
      'eBPF (extended Berkeley Packet Filter) -- технология Linux kernel, позволяющая запускать изолированный код в ядре. Для observability: перехват системных вызовов, сетевых пакетов, событий планировщика без изменения приложений и с минимальным overhead. Инструменты: Cilium (networking + observability), Pixie (auto-instrumentation), bpftrace (ad-hoc profiling), Falco (security monitoring). eBPF позволяет получить observability там, где невозможна инструментация кода: legacy приложения, third-party софт, kernel-level метрики.',
  },
  {
    id: 'sd-observability-042',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'open',
    question: 'Как организовать chaos engineering для проверки observability? Какие эксперименты проводить?',
    sampleAnswer:
      'Chaos engineering тестирует не только устойчивость системы, но и качество observability. Эксперименты: 1) Network latency injection: добавить 500ms latency между сервисами → проверить, что latency dashboard показывает деградацию, алерты срабатывают. 2) Service failure: убить инстанс сервиса → проверить, что error rate отображается, dependency map показывает проблему, трейсы содержат ошибки. 3) Resource exhaustion: создать memory pressure → проверить алерты на saturation метрики. 4) Cascading failure: отключить downstream → проверить видимость каскадного эффекта. 5) DNS failure: заблокировать DNS → проверить, что причина быстро идентифицируется. Инструменты: Chaos Monkey (Netflix), Gremlin, Litmus Chaos, Chaos Mesh (Kubernetes). Процесс: гипотеза → эксперимент в staging → анализ → эксперимент в production (с ограниченным blast radius).',
    explanation:
      'Game days -- регулярные учения по реагированию на инциденты с использованием chaos engineering. Цель -- убедиться, что observability позволяет быстро диагностировать проблемы, и команда умеет использовать инструменты. Netflix проводит Chaos Monkey в production постоянно.',
  },
  {
    id: 'sd-observability-043',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'open',
    question: 'Опишите стратегию мониторинга для multi-tenant SaaS-платформы. Какие специфические требования?',
    sampleAnswer:
      'Multi-tenant специфика: 1) Per-tenant метрики: cardinality challenge -- tenant_id как label создаёт высокую cardinality. Решение: отдельные recording rules для top-N tenants, агрегация для остальных; или tenant-specific Prometheus instances. 2) Tenant isolation в дашбордах: RBAC -- tenant видит только свои данные. Grafana Teams/Folders с row-level security. 3) Noisy neighbor detection: мониторинг resource consumption по tenants, алерты на аномалии (один tenant потребляет 50% ресурсов). 4) Per-tenant SLO: разные SLO для разных тарифов (enterprise vs free), отдельный error budget tracking. 5) Billing metrics: точный учёт потребления для биллинга (API calls, storage, compute). 6) Tenant health dashboard: overview всех tenants для support команды. Архитектура: метрики с tenant_id label → recording rules для агрегаций → Grafana с variables для фильтрации.',
    explanation:
      'Multi-tenant observability -- сложная задача из-за cardinality и isolation требований. Крупные SaaS (Datadog, New Relic сами являются observability платформами) используют кастомные решения. Для большинства компаний комбинация Prometheus + recording rules + Grafana RBAC достаточна.',
  },
  {
    id: 'sd-observability-044',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое Jaeger и для чего он используется?',
    options: [
      'Система хранения метрик',
      'Open-source платформа для distributed tracing, позволяющая отслеживать путь запросов через микросервисы',
      'Log aggregation система',
      'Инструмент для load testing',
    ],
    correctIndex: 1,
    explanation:
      'Jaeger -- open-source distributed tracing система, разработанная Uber и переданная в CNCF. Компоненты: Agent (sidecar для приёма spans), Collector (обработка и хранение), Query (UI и API), хранилище (Cassandra, Elasticsearch, Kafka). Возможности: визуализация трейсов (waterfall diagram), поиск по тегам и времени, сравнение трейсов, service dependency graph. Альтернативы: Zipkin (Twitter), Tempo (Grafana Labs, без индексации -- дешевле), коммерческие APM.',
  },
  {
    id: 'sd-observability-045',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое push vs pull модель сбора метрик? Приведите примеры систем.',
    options: [
      'Push и pull -- способы визуализации метрик',
      'Push: сервис отправляет метрики в collector; Pull: collector запрашивает метрики у сервиса. Prometheus -- pull, StatsD/InfluxDB -- push',
      'Push для логов, pull для трейсов',
      'Push для реального времени, pull для исторических данных',
    ],
    correctIndex: 1,
    explanation:
      'Pull модель (Prometheus): collector периодически запрашивает метрики с /metrics endpoint сервисов. Преимущества: легко определить, что сервис недоступен, централизованный контроль scrape interval. Push модель (StatsD, Graphite, InfluxDB, Datadog Agent): сервис сам отправляет метрики. Преимущества: работает за NAT/firewall, подходит для short-lived jobs (batch). Prometheus для short-lived jobs использует Pushgateway как промежуточное звено. OpenTelemetry поддерживает оба режима через OTLP.',
  },
  {
    id: 'sd-observability-046',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'open',
    question: 'Как мониторить базы данных (PostgreSQL, MySQL)? Какие ключевые метрики отслеживать?',
    sampleAnswer:
      'Ключевые метрики БД: 1) Connections: active, idle, waiting, max_connections utilization. Алерт: > 80% connections. 2) Query performance: queries per second, average query time, slow queries (> threshold). pg_stat_statements для PostgreSQL. 3) Replication: lag в секундах/байтах, состояние replica. 4) Locks: waiting locks, deadlocks. 5) Cache: buffer cache hit ratio (должен быть > 99%), index hit ratio. 6) Storage: database size, table bloat, disk I/O. 7) Transactions: commits/rollbacks per second, transaction duration. Инструменты: postgres_exporter / mysqld_exporter для Prometheus, pgBadger для анализа логов, pg_stat_statements для query analysis. Dashboards: Percona PMM предоставляет готовые dashboards. Алерты: high replication lag, connection exhaustion, high lock wait time, disk space < 20%.',
    explanation:
      'Database monitoring -- критическая часть observability. База данных часто является bottleneck-ом. Slow query log + pg_stat_statements позволяют находить проблемные запросы. Connection pooling (PgBouncer) отдельно мониторится: pool utilization, wait time.',
  },
  {
    id: 'sd-observability-047',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое AIOps и как машинное обучение применяется в observability?',
    options: [
      'Использование AI для написания кода мониторинга',
      'Применение ML для автоматического обнаружения аномалий, корреляции событий, прогнозирования инцидентов и автоматизации remediation',
      'Чат-бот для ответов на вопросы о метриках',
      'Автоматическая генерация дашбордов',
    ],
    correctIndex: 1,
    explanation:
      'AIOps (AI for IT Operations) применяет ML к observability данным: 1) Anomaly detection: автоматическое обнаружение отклонений от baseline без ручных thresholds (сезонность, тренды). 2) Event correlation: группировка связанных алертов, определение root cause. 3) Predictive alerting: предсказание проблем до их возникновения (disk full through 3 days). 4) Noise reduction: фильтрация ложных срабатываний. 5) Auto-remediation: автоматическое выполнение runbook при известных проблемах. Инструменты: Datadog Watchdog, New Relic AI, Dynatrace Davis, Moogsoft. Ограничения: требует качественных данных, может давать false positives, не заменяет понимание системы инженерами.',
  },
  {
    id: 'sd-observability-048',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'open',
    question: 'Как построить observability для serverless архитектуры (AWS Lambda, Azure Functions)? Какие специфические challenges?',
    sampleAnswer:
      'Serverless challenges: 1) Cold starts: мониторинг init duration отдельно от execution duration. 2) Short-lived: функции живут миллисекунды, традиционный scraping не работает. 3) Отсутствие агентов: нельзя установить sidecar. 4) Distributed context: propagation через event sources (API Gateway, SQS, SNS). Решения: 1) Structured logging в stdout → CloudWatch Logs → Loki/ES. Включать trace_id, request_id. 2) AWS X-Ray / OpenTelemetry Lambda layer для tracing. X-Ray SDK автоматически инструментирует AWS SDK calls. 3) CloudWatch Metrics + custom metrics через EMF (Embedded Metric Format). 4) Powertools (AWS Lambda Powertools) -- библиотека для structured logging, tracing, metrics. Метрики: invocations, errors, duration (p50/p95/p99), concurrent executions, throttles, cold start ratio. Dashboards: per-function и cross-function для обнаружения downstream dependencies.',
    explanation:
      'Serverless observability сложнее традиционной из-за эфемерности и отсутствия инфраструктуры под контролем. AWS X-Ray + CloudWatch -- baseline, OpenTelemetry Lambda layer -- для vendor-neutrality. Lumigo, Epsagon (Cisco), Thundra -- специализированные serverless observability платформы.',
  },
  {
    id: 'sd-observability-049',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое synthetic monitoring (синтетический мониторинг)?',
    options: [
      'Мониторинг искусственного интеллекта',
      'Автоматизированные проверки работоспособности сервиса из внешних точек: ping, HTTP checks, browser transactions, имитирующие действия пользователя',
      'Генерация синтетических данных для тестирования',
      'Мониторинг виртуальных машин',
    ],
    correctIndex: 1,
    explanation:
      'Synthetic monitoring -- активные проверки из внешней перспективы (как видит пользователь). Типы: 1) Ping/TCP checks -- базовая доступность. 2) HTTP checks -- статус код, время ответа, содержимое. 3) Multi-step API tests -- сценарии из нескольких запросов. 4) Browser tests -- headless browser выполняет user flow (login, checkout). 5) Global checks -- проверки из разных географических точек. Инструменты: Datadog Synthetics, New Relic Synthetics, Pingdom, Checkly. Преимущества: обнаружение проблем до жалоб пользователей, мониторинг third-party зависимостей, baseline для SLO.',
  },
  {
    id: 'sd-observability-050',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'open',
    question: 'Как организовать incident management процесс? Опишите lifecycle инцидента и роли участников.',
    sampleAnswer:
      'Incident lifecycle: 1) Detection: алерт срабатывает или пользователь сообщает о проблеме. 2) Triage: on-call определяет severity (SEV1-критический, SEV2-major, SEV3-minor). 3) Response: формирование incident team, создание war room (Slack channel, Zoom). 4) Mitigation: быстрое восстановление сервиса (rollback, restart, failover). 5) Resolution: полное устранение проблемы. 6) Post-incident review: blameless postmortem через 24-48 часов. Роли: Incident Commander (IC) -- координация, принятие решений. Communications Lead -- обновление статус-страницы, stakeholders. Technical Lead -- техническая диагностика и fix. Scribe -- документирование timeline. Инструменты: PagerDuty/Opsgenie (alerting, on-call), Slack (communication), StatusPage (external communication), Jira/Linear (tracking), Rootly/incident.io (incident management platform). Артефакты: timeline, root cause analysis, action items, metrics (MTTD, MTTR).',
    explanation:
      'Структурированный incident management критически важен для минимизации impact. Google, Facebook, Netflix публикуют свои incident management practices. MTTR (Mean Time To Recovery) -- ключевая метрика. Blameless culture -- необходимое условие для честного анализа и improvement.',
  },
];
