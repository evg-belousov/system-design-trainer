import type { Question } from '../types';

export const observabilityQuestions: Question[] = [
  {
    id: 'sd-observability-001',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какие три столпа (pillars) observability выделяют в современных распределённых системах?',
    options: [
      'CPU, Memory, Disk',
      'Logs, Metrics, Traces',
      'Alerts, Dashboards, Reports',
      'Availability, Latency, Throughput',
    ],
    correctIndex: 1,
    explanation:
      'Три столпа observability: Logs (логи) -- дискретные события с контекстом; Metrics (метрики) -- числовые измерения, агрегированные во времени; Traces (трейсы) -- путь запроса через распределённую систему. Каждый столп даёт уникальную перспективу: логи объясняют «что произошло», метрики показывают «насколько хорошо система работает», трейсы раскрывают «где именно возникла проблема» в цепочке сервисов.',
  },
  {
    id: 'sd-observability-002',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое structured logging и чем он отличается от обычного текстового логирования?',
    options: [
      'Логирование только в файлы определённого формата (.log)',
      'Запись логов в структурированном формате (JSON) с набором типизированных полей вместо произвольного текста',
      'Логирование с обязательным указанием уровня (INFO, WARN, ERROR)',
      'Хранение логов в реляционной базе данных с жёсткой схемой',
    ],
    correctIndex: 1,
    explanation:
      'Structured logging -- запись логов в формате ключ-значение (обычно JSON), где каждое поле имеет имя и тип. Пример: {"timestamp":"2024-01-15T10:30:00Z","level":"error","service":"payment","trace_id":"abc123","user_id":"u456","message":"Payment failed","error_code":"INSUFFICIENT_FUNDS"}. Это позволяет эффективно искать, фильтровать и агрегировать логи с помощью инструментов (ELK, Loki), в отличие от произвольного текста, который требует regex-парсинга.',
  },
  {
    id: 'sd-observability-003',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Для чего используется Prometheus в стеке мониторинга?',
    options: [
      'Для визуализации метрик и создания дашбордов',
      'Для сбора, хранения и запроса метрик временных рядов (time series)',
      'Для распределённого трейсинга запросов между микросервисами',
      'Для агрегации и индексирования логов',
    ],
    correctIndex: 1,
    explanation:
      'Prometheus -- open-source система мониторинга, специализирующаяся на сборе и хранении метрик в формате time series. Использует pull-модель: периодически опрашивает (scrape) /metrics эндпоинты сервисов. Имеет собственный язык запросов PromQL для анализа метрик. Prometheus хранит данные локально и поддерживает алертинг через Alertmanager. Для визуализации обычно используется Grafana как отдельный компонент.',
  },
  {
    id: 'sd-observability-004',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что означает аббревиатура SLO в контексте надёжности сервисов?',
    options: [
      'Service Level Objective -- целевой уровень качества сервиса, выраженный как процент (например, 99.9% доступности)',
      'Service Load Optimizer -- компонент для оптимизации нагрузки',
      'System Logging Output -- стандарт вывода логов',
      'Service Latency Overview -- обзор задержек сервиса',
    ],
    correctIndex: 0,
    explanation:
      'SLO (Service Level Objective) -- целевое значение для метрики качества сервиса. Например, «99.9% запросов должны обрабатываться быстрее 200мс за 30-дневное окно». SLO определяется на основе SLI (Service Level Indicator -- конкретная метрика), а SLA (Service Level Agreement) -- это юридическое соглашение с клиентом, включающее SLO и последствия его нарушения (компенсации). Иерархия: SLI → SLO → SLA.',
  },
  {
    id: 'sd-observability-005',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какой компонент стека ELK отвечает за сбор и преобразование логов перед отправкой в хранилище?',
    options: [
      'Elasticsearch',
      'Logstash',
      'Kibana',
      'Filebeat',
    ],
    correctIndex: 1,
    explanation:
      'ELK Stack: Elasticsearch (хранение и поиск), Logstash (сбор, парсинг, трансформация логов), Kibana (визуализация). Logstash принимает логи из различных источников (файлы, syslog, Kafka), трансформирует их (парсинг, обогащение, фильтрация) и отправляет в Elasticsearch. На практике часто используют Filebeat (легковесный агент) для сбора логов на хостах и передачи в Logstash или напрямую в Elasticsearch.',
  },
  {
    id: 'sd-observability-006',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое distributed tracing (распределённый трейсинг)?',
    options: [
      'Отслеживание изменений в исходном коде через систему контроля версий',
      'Отслеживание пути запроса через цепочку микросервисов с измерением времени каждого этапа',
      'Мониторинг сетевых маршрутов между серверами (traceroute)',
      'Трассировка системных вызовов процесса (strace)',
    ],
    correctIndex: 1,
    explanation:
      'Distributed tracing отслеживает прохождение запроса через все сервисы в распределённой системе. Каждый trace состоит из spans -- отрезков работы в отдельных сервисах. Span содержит: имя операции, время начала и конца, теги, ссылку на parent span. Trace ID передаётся между сервисами через заголовки (например, W3C Trace Context). Инструменты: Jaeger, Zipkin, AWS X-Ray, Datadog APM. Tracing незаменим для диагностики latency в микросервисах.',
  },
  {
    id: 'sd-observability-007',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что описывает метод RED для мониторинга микросервисов?',
    options: [
      'Resources, Errors, Duration -- метрики ресурсов хоста',
      'Rate, Errors, Duration -- ключевые метрики запросов к сервису',
      'Replicas, Events, Deployments -- метрики оркестрации',
      'Read, Execute, Delete -- метрики операций с данными',
    ],
    correctIndex: 1,
    explanation:
      'Метод RED (предложен Томом Уилки из Grafana Labs) определяет три ключевые метрики для мониторинга request-driven сервисов: Rate -- количество запросов в секунду; Errors -- количество неуспешных запросов; Duration -- распределение времени обработки запросов (latency). RED оптимален для микросервисов и API. В отличие от USE (Utilization, Saturation, Errors), который фокусируется на инфраструктурных ресурсах (CPU, память, диск, сеть), RED фокусируется на пользовательском опыте.',
  },
  {
    id: 'sd-observability-008',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какая модель сбора метрик используется в Prometheus по умолчанию?',
    options: [
      'Push-модель: сервисы отправляют метрики в Prometheus',
      'Pull-модель: Prometheus периодически опрашивает /metrics эндпоинты сервисов',
      'Event-driven: Prometheus подписывается на события через message broker',
      'Streaming: сервисы устанавливают постоянное WebSocket-соединение с Prometheus',
    ],
    correctIndex: 1,
    explanation:
      'Prometheus использует pull-модель: он сам периодически (scrape interval, обычно 15-30с) отправляет HTTP GET на /metrics эндпоинт каждого сервиса. Преимущества pull: легко определить, что сервис недоступен (failed scrape); нет необходимости в discovery на стороне сервиса; Prometheus контролирует частоту сбора. Для короткоживущих задач (batch jobs), которые могут завершиться до scrape, используется Pushgateway как промежуточное звено.',
  },
  {
    id: 'sd-observability-009',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Чем SLI отличается от SLO, и как они связаны с SLA?',
    options: [
      'SLI -- это метрика, SLO -- целевое значение этой метрики, SLA -- юридическое соглашение, включающее SLO и последствия нарушения',
      'SLI, SLO и SLA -- это синонимы, обозначающие уровень доступности сервиса',
      'SLI определяется бизнесом, SLO -- инженерами, SLA -- юристами, они не связаны между собой',
      'SLI применяется к внутренним сервисам, SLO -- к внешним, SLA -- к облачным провайдерам',
    ],
    correctIndex: 0,
    explanation:
      'SLI (Service Level Indicator) -- конкретная измеримая метрика: доля успешных запросов, задержка p99, процент доступности. SLO (Service Level Objective) -- целевое значение SLI: «p99 latency < 200ms для 99.9% запросов за 30 дней». SLA (Service Level Agreement) -- формальный договор с клиентом: «Если SLO нарушен, клиент получает 10% компенсацию». Обычно SLO устанавливают строже, чем SLA, чтобы иметь «бюджет ошибок» (error budget) до нарушения юридических обязательств.',
  },
  {
    id: 'sd-observability-010',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какой тип метрик в Prometheus представляет значение, которое может как увеличиваться, так и уменьшаться (например, количество активных соединений)?',
    options: [
      'Counter',
      'Gauge',
      'Histogram',
      'Summary',
    ],
    correctIndex: 1,
    explanation:
      'Gauge (шкала) -- тип метрики, значение которой может произвольно увеличиваться и уменьшаться. Примеры: температура CPU, количество активных запросов, размер очереди, использование памяти. Counter -- монотонно возрастающий счётчик (количество запросов, ошибок), сбрасывается только при перезапуске. Histogram -- распределение значений по корзинам (bucket), используется для latency. Summary -- аналог histogram, но рассчитывает квантили на клиенте.',
  },
  {
    id: 'sd-observability-011',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'open',
    question: 'Опишите метод USE (Utilization, Saturation, Errors) для мониторинга инфраструктуры. Приведите примеры метрик для CPU, памяти и диска.',
    sampleAnswer:
      'USE Method (предложен Бренданом Греггом) применяется к каждому ресурсу системы: Utilization (утилизация) -- доля времени, когда ресурс занят. CPU: % использования, Memory: % занятой RAM, Disk: % I/O utilization. Saturation (насыщение) -- объём работы, который ресурс не может обслужить (очередь). CPU: run queue length (load average), Memory: swap usage / OOM events, Disk: I/O wait queue depth. Errors (ошибки) -- количество ошибок ресурса. CPU: machine check exceptions, Memory: ECC errors, Disk: read/write errors, bad sectors. Метод USE отлично подходит для быстрого анализа «узких мест» инфраструктуры, а RED -- для анализа приложений. Вместе они дают полную картину.',
    explanation:
      'USE Method систематизирует анализ производительности: для каждого физического ресурса (CPU, память, диск, сеть, файловые дескрипторы) проверяются три аспекта. Это позволяет быстро находить bottleneck и исключать ресурсы, которые работают нормально. Brendan Gregg также предложил метод TSA (Thread State Analysis) для анализа производительности приложений.',
  },
  {
    id: 'sd-observability-012',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'open',
    question: 'Как работает distributed tracing? Объясните понятия trace, span и context propagation.',
    sampleAnswer:
      'Distributed tracing отслеживает путь одного запроса через все микросервисы. Trace -- полный путь запроса от начала до конца, идентифицируемый уникальным Trace ID. Span -- единица работы внутри одного сервиса (например, HTTP-обработчик, запрос к БД). Каждый span имеет: span ID, parent span ID, имя операции, timestamps (start/end), теги и логи. Context Propagation -- механизм передачи Trace ID и Span ID между сервисами через HTTP-заголовки (W3C Trace Context: traceparent, tracestate) или gRPC metadata. При получении запроса сервис извлекает контекст, создаёт дочерний span и передаёт обновлённый контекст дальше. OpenTelemetry -- современный стандарт, объединяющий OpenTracing и OpenCensus, предоставляющий SDK для автоматической инструментации.',
    explanation:
      'Без distributed tracing отладка проблем в микросервисах превращается в поиск иголки в стоге сена: нужно коррелировать логи десятков сервисов вручную. Tracing позволяет визуализировать waterfall-диаграмму запроса и мгновенно определить, какой сервис вызывает задержку. Google Dapper (2010) -- одна из первых публикаций о distributed tracing в продакшене.',
  },
  {
    id: 'sd-observability-013',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'open',
    question: 'Какие стратегии алертинга вы знаете? Как избежать «alert fatigue» (усталости от алертов)?',
    sampleAnswer:
      'Стратегии алертинга: 1) Threshold-based -- алерт при превышении порога (CPU > 90%). Простой, но подвержен false positives. 2) Rate of change -- алерт при аномальной скорости изменения (error rate удвоился за 5 минут). 3) Anomaly detection -- ML-модели определяют отклонение от нормы. 4) SLO-based alerting (рекомендуется Google SRE) -- алерт при расходовании error budget: burn rate alerting (скорость сжигания бюджета ошибок). Избежание alert fatigue: иерархия severity (critical/warning/info); только actionable алерты попадают в on-call; группировка связанных алертов; runbook для каждого алерта; регулярная ревизия алертов (удаление бесполезных); routing по компетенциям (DB-алерты -- DBA-команде). Золотое правило: каждый алерт должен требовать немедленного человеческого действия, иначе это не алерт, а уведомление.',
    explanation:
      'Google SRE Book рекомендует мультиоконный мультибёрнрейт алертинг: быстрый burn rate за короткое окно (5% бюджета за 1 час) вызывает page, медленный burn rate за длинное окно (10% за 3 дня) вызывает ticket. Это резко снижает количество ложных срабатываний при сохранении чувствительности к реальным инцидентам.',
  },
  {
    id: 'sd-observability-014',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое APM (Application Performance Monitoring) и какую информацию он предоставляет?',
    options: [
      'Мониторинг только серверного оборудования (CPU, память, диск)',
      'Комплексный мониторинг производительности приложения: транзакции, latency, error rates, зависимости, профилирование кода',
      'Мониторинг исключительно бизнес-метрик (конверсии, выручка)',
      'Средство для автоматического исправления ошибок в коде',
    ],
    correctIndex: 1,
    explanation:
      'APM (Application Performance Monitoring) предоставляет глубокий insight в работу приложения: автоматическое обнаружение и трейсинг транзакций, карта зависимостей между сервисами, профилирование кода (какие методы занимают больше времени), мониторинг баз данных (медленные запросы), Real User Monitoring (RUM) для фронтенда. Примеры: Datadog APM, New Relic, Dynatrace, Elastic APM. APM объединяет метрики, трейсы и логи в единый контекст.',
  },
  {
    id: 'sd-observability-015',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Какой подход к сэмплированию (sampling) в distributed tracing позволяет принять решение о записи трейса после его завершения?',
    options: [
      'Head-based sampling -- решение принимается в начале трейса',
      'Tail-based sampling -- решение принимается после сбора всех spans трейса',
      'Probabilistic sampling -- фиксированный процент трейсов записывается',
      'Rate limiting sampling -- ограничение количества трейсов в секунду',
    ],
    correctIndex: 1,
    explanation:
      'Tail-based sampling собирает все spans трейса в буфер (обычно в collector) и принимает решение о сохранении после завершения. Это позволяет всегда сохранять трейсы с ошибками, аномально высоким latency или специфическими атрибутами, даже если они редки. Head-based sampling (решение в начале) проще и дешевле, но может пропустить интересные трейсы. Tail-based sampling требует больше памяти и вычислительных ресурсов в collector, но обеспечивает значительно более качественные данные для отладки.',
  },
  {
    id: 'sd-observability-016',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое error budget и как он связан с SLO?',
    options: [
      'Финансовый бюджет, выделенный на исправление ошибок в коде',
      'Допустимое количество ошибок (100% минус SLO), которое можно потратить на развёртывание новых функций и эксперименты',
      'Максимальное количество инцидентов в квартал, после которого команда увольняется',
      'Запас мощности серверов для обработки ошибочных запросов',
    ],
    correctIndex: 1,
    explanation:
      'Error budget = 100% - SLO. Если SLO = 99.9%, error budget = 0.1% (около 43 минут простоя в месяц). Пока error budget не исчерпан, команда может выпускать фичи, проводить эксперименты, миграции. Когда error budget исчерпан, приоритет переключается на надёжность: замораживание релизов, исправление технического долга. Это формализует баланс между скоростью разработки и надёжностью, убирая субъективные оценки из дискуссий между product и engineering.',
  },
  {
    id: 'sd-observability-017',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'open',
    question: 'Спроектируйте систему observability для платформы из 50+ микросервисов. Какие компоненты выберете и как обеспечите корреляцию между логами, метриками и трейсами?',
    sampleAnswer:
      'Архитектура: 1) Метрики: Prometheus + Thanos (или Mimir) для long-term storage и multi-cluster; Grafana для визуализации. Каждый сервис экспортирует RED-метрики через /metrics. 2) Логи: структурированные JSON-логи → Filebeat/FluentBit → Kafka (буфер) → Elasticsearch (или Loki для экономии). Kibana/Grafana для поиска. 3) Трейсы: OpenTelemetry SDK во всех сервисах → OTel Collector → Jaeger/Tempo для хранения. Tail-based sampling в collector. Корреляция: единый trace_id пробрасывается через все сервисы (W3C Trace Context). Логи содержат поля trace_id и span_id. Метрики снабжаются exemplars -- ссылками на конкретные trace_id. В Grafana: из метрики → клик на exemplar → переход к трейсу → из span → переход к логам с тем же trace_id. OpenTelemetry выступает единой точкой инструментации: один SDK генерирует метрики, логи и трейсы с общим контекстом. Alerting: Alertmanager + PagerDuty для critical, Slack для warning.',
    explanation:
      'Ключ к эффективной observability -- корреляция между тремя столпами. Без неё приходится вручную сопоставлять данные из разных систем. OpenTelemetry стал де-факто стандартом, позволяя избежать vendor lock-in и обеспечить единый контекст. Стоимость observability для 50+ сервисов может быть значительной (Elasticsearch требует много ресурсов), поэтому важно планировать retention policies и sampling.',
  },
  {
    id: 'sd-observability-018',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'open',
    question: 'Объясните концепцию SLO-based alerting с burn rate. Как это работает и почему это лучше порогового алертинга?',
    sampleAnswer:
      'SLO-based alerting с burn rate отслеживает скорость расходования error budget. Burn rate = (фактическая частота ошибок) / (допустимая частота ошибок по SLO). Если SLO = 99.9% (допустимо 0.1% ошибок), а текущая частота ошибок 1%, burn rate = 1%/0.1% = 10x. Мультиоконный подход: быстрое окно (5 мин) с высоким порогом burn rate (14x) ловит острые инциденты; медленное окно (6 часов) с низким порогом (6x) ловит медленную деградацию. Преимущества: алерт напрямую связан с пользовательским опытом; автоматически адаптируется к масштабу системы; меньше false positives (краткий всплеск ошибок при большом запасе бюджета не вызывает алерт); сигнализирует о реальной угрозе нарушения SLO, а не о произвольном пороге. Реализация: в Prometheus через recording rules, вычисляющие error ratio за разные окна.',
    explanation:
      'Google SRE Book и Sloth (open-source генератор SLO для Prometheus) детально описывают этот подход. Пороговый алертинг (CPU > 90%) не учитывает контекст: 90% CPU может быть нормой для compute-intensive сервиса. SLO-based алертинг переключает фокус с «ресурс перегружен» на «пользователи страдают», что значительно повышает signal-to-noise ratio.',
  },
  {
    id: 'sd-observability-019',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'open',
    question: 'Как обеспечить observability для асинхронных систем, использующих message queues (Kafka, RabbitMQ)? Какие дополнительные метрики и подходы необходимы?',
    sampleAnswer:
      'Observability асинхронных систем имеет свои особенности: 1) Context Propagation через сообщения: trace_id и span_id сериализуются в заголовки сообщений (Kafka headers, AMQP properties). Consumer создаёт дочерний span, связывая обработку с исходным запросом. 2) Ключевые метрики очередей: consumer lag (разница между offset producer и consumer -- критичнее throughput), partition assignment, rebalance events, consumer group health, dead letter queue size, message age (время от публикации до обработки). 3) End-to-end latency: измерение полного пути сообщения (produce → broker → consume → process). 4) Idempotency tracking: мониторинг дубликатов сообщений. 5) Backpressure visibility: мониторинг скорости заполнения очередей vs скорости обработки. 6) Alerting: consumer lag растёт > N минут, DLQ size > 0, consumer не подключён. Инструменты: Kafka Exporter для Prometheus, Burrow для lag monitoring.',
    explanation:
      'В асинхронных системах проблемы проявляются с задержкой: ошибка обработки может обнаружиться спустя часы после отправки сообщения. Consumer lag -- «канарейка в угольной шахте»: если lag растёт, это ранний сигнал о проблемах с производительностью consumer или увеличении нагрузки. Dead letter queue с нулевым размером -- тоже важная метрика для алертинга.',
  },
  {
    id: 'sd-observability-020',
    block: 'sd',
    topic: 'observability',
    topicLabel: 'Мониторинг и Observability',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое OpenTelemetry и какую проблему он решает?',
    options: [
      'Проприетарный формат логов от Google для Cloud Platform',
      'Открытый стандарт и набор SDK/API для генерации, сбора и экспорта телеметрии (метрик, логов, трейсов), обеспечивающий vendor-нейтральную инструментацию',
      'Графический интерфейс для визуализации метрик, альтернатива Grafana',
      'Инструмент для нагрузочного тестирования микросервисов',
    ],
    correctIndex: 1,
    explanation:
      'OpenTelemetry (OTel) -- проект CNCF, объединивший OpenTracing и OpenCensus. Он предоставляет: единый API и SDK для инструментации (метрики, логи, трейсы); OTel Collector -- агент для сбора, обработки и экспорта телеметрии в любой backend (Jaeger, Prometheus, Datadog и др.); автоматическую инструментацию для популярных фреймворков. Главное преимущество -- vendor-нейтральность: инструментируете код один раз, меняете backend без изменения кода. OTel стал де-факто стандартом индустрии.',
  },
];
