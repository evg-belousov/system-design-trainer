import type { Question } from '../types';

export const replicationQuestions: Question[] = [
  {
    id: 'sd-replication-001',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое репликация базы данных?',
    options: [
      'Разделение данных на несколько серверов по определённому ключу',
      'Создание и поддержание копий данных на нескольких серверах для повышения доступности и отказоустойчивости',
      'Процесс миграции данных из одной СУБД в другую',
      'Автоматическое резервное копирование базы данных по расписанию',
    ],
    correctIndex: 1,
    explanation:
      'Репликация (replication) — это процесс копирования и синхронизации данных между несколькими серверами баз данных (репликами). Цели: повышение доступности (при падении одного сервера другие продолжают работу), отказоустойчивость, масштабирование чтения (распределение read-запросов между репликами), снижение задержки (реплика ближе к пользователю).',
  },
  {
    id: 'sd-replication-002',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'junior',
    type: 'quiz',
    question:
      'Как работает схема репликации master-slave (primary-replica)?',
    options: [
      'Все узлы равноправны и принимают как записи, так и чтения',
      'Один узел (master) принимает все записи и реплицирует изменения на slave-узлы, которые обслуживают чтения',
      'Данные разделяются поровну между master и slave, каждый хранит свою половину',
      'Slave-узел автоматически заменяет master при любом изменении конфигурации',
    ],
    correctIndex: 1,
    explanation:
      'В схеме master-slave (primary-replica) один узел (master/primary) принимает все операции записи. Изменения затем реплицируются на один или несколько slave-узлов (replica). Slave-узлы обслуживают операции чтения, разгружая master. При сбое master может быть выполнен failover — один из slave-ов промоутится до master. Эта схема хорошо подходит для read-heavy нагрузок.',
  },
  {
    id: 'sd-replication-003',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое шардирование (sharding) базы данных?',
    options: [
      'Создание нескольких копий одних и тех же данных на разных серверах',
      'Разделение данных на части (шарды), каждая из которых хранится на отдельном сервере',
      'Сжатие данных для экономии дискового пространства',
      'Вертикальное масштабирование базы данных путём добавления ресурсов на один сервер',
    ],
    correctIndex: 1,
    explanation:
      'Шардирование (sharding) — это горизонтальное разделение данных между несколькими серверами (шардами). Каждый шард содержит подмножество данных. Например, пользователи с ID 1–1000000 на шарде 1, 1000001–2000000 на шарде 2. Шардирование позволяет масштабировать как запись, так и чтение, распределяя нагрузку между серверами. Ключевой вызов — выбор ключа шардирования (shard key), который обеспечит равномерное распределение.',
  },
  {
    id: 'sd-replication-004',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'junior',
    type: 'quiz',
    question:
      'Чем синхронная репликация отличается от асинхронной?',
    options: [
      'Синхронная репликация быстрее асинхронной',
      'При синхронной репликации master ждёт подтверждения от реплик перед подтверждением записи клиенту, при асинхронной — не ждёт',
      'Синхронная репликация работает только в пределах одного дата-центра',
      'Асинхронная репликация гарантирует отсутствие потери данных при сбое master',
    ],
    correctIndex: 1,
    explanation:
      'Синхронная репликация: master отправляет изменения на реплики и ждёт подтверждения от одной или нескольких реплик перед тем, как подтвердить запись клиенту. Гарантирует, что данные не потеряются при сбое master, но увеличивает задержку записи. Асинхронная репликация: master подтверждает запись клиенту сразу после локальной записи, реплики обновляются позже. Ниже задержка записи, но при сбое master возможна потеря последних изменений (replication lag).',
  },
  {
    id: 'sd-replication-005',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'junior',
    type: 'quiz',
    question:
      'Что такое read replica и для чего она используется?',
    options: [
      'Резервная копия базы данных, которая активируется только при сбое',
      'Копия базы данных, предназначенная для обслуживания запросов на чтение, разгружающая master от read-нагрузки',
      'Специальная таблица в БД для кэширования результатов частых запросов',
      'Отдельная база данных, хранящая только индексы для быстрого поиска',
    ],
    correctIndex: 1,
    explanation:
      'Read replica — это реплика базы данных, оптимизированная для обслуживания запросов на чтение. Все записи идут на master, а read replica получает изменения через репликацию. Это позволяет: масштабировать чтение горизонтально (добавляя больше реплик), разгрузить master для операций записи, обеспечить географическую локальность (реплика ближе к пользователям). AWS RDS, Google Cloud SQL, Azure SQL — все поддерживают read replicas.',
  },
  {
    id: 'sd-replication-006',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'junior',
    type: 'open',
    question:
      'Объясните разницу между вертикальным и горизонтальным масштабированием баз данных. Когда следует применять шардирование?',
    sampleAnswer:
      'Вертикальное масштабирование (scale up) — увеличение мощности одного сервера: больше CPU, RAM, SSD. Просто, не требует изменений в архитектуре, но имеет потолок (физические ограничения) и высокую стоимость. Горизонтальное масштабирование (scale out) — добавление новых серверов. Шардирование — основной метод горизонтального масштабирования БД. Шардирование следует применять, когда: 1) Объём данных превышает ёмкость одного сервера. 2) Нагрузка на запись слишком высока для одного master. 3) Требования к задержке не позволяют использовать один сервер для всех пользователей. Перед шардированием стоит рассмотреть оптимизацию запросов, кэширование, read replicas.',
    explanation:
      'Шардирование — это крайняя мера масштабирования, которая вносит значительную сложность: cross-shard запросы, распределённые транзакции, ребалансировка данных. Рекомендуется исчерпать возможности вертикального масштабирования, кэширования и read replica перед переходом к шардированию.',
  },
  {
    id: 'sd-replication-007',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'middle',
    type: 'quiz',
    question:
      'Что такое consistent hashing и какую проблему шардирования он решает?',
    options: [
      'Метод шифрования данных при передаче между шардами',
      'Алгоритм распределения данных по шардам, минимизирующий перемещение данных при добавлении или удалении узлов',
      'Способ обеспечения консистентности данных между master и slave',
      'Алгоритм обнаружения конфликтов при multi-master репликации',
    ],
    correctIndex: 1,
    explanation:
      'Consistent hashing — алгоритм, при котором данные и узлы отображаются на кольцо (hash ring). Каждый ключ направляется к ближайшему узлу по кольцу. При добавлении/удалении узла перемещается только часть данных (примерно 1/N, где N — число узлов), а не все данные, как при обычном hash % N. Это решает проблему массовой перебалансировки. Виртуальные узлы (vnodes) обеспечивают равномерное распределение. Используется в DynamoDB, Cassandra, Riak, кэшах (Memcached).',
  },
  {
    id: 'sd-replication-008',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'middle',
    type: 'quiz',
    question:
      'Чем range-based шардирование отличается от hash-based шардирования?',
    options: [
      'Range-based распределяет данные по диапазону ключей, hash-based — по хешу ключа; range поддерживает range-запросы, hash — более равномерное распределение',
      'Range-based работает только с числовыми ключами, hash-based — только со строковыми',
      'Range-based медленнее при записи, hash-based — при чтении',
      'Они идентичны по производительности, различаются только синтаксисом',
    ],
    correctIndex: 0,
    explanation:
      'Range-based шардирование: данные делятся по диапазонам ключа (A-F на шард 1, G-L на шард 2, и т.д.). Преимущество: поддержка range-запросов (все заказы за последний месяц). Недостаток: неравномерное распределение (hotspots) — если данные концентрируются в одном диапазоне. Hash-based шардирование: ключ хешируется, шард определяется по хешу. Преимущество: равномерное распределение. Недостаток: range-запросы требуют обращения ко всем шардам (scatter-gather). MongoDB и PostgreSQL поддерживают оба типа.',
  },
  {
    id: 'sd-replication-009',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'middle',
    type: 'quiz',
    question:
      'Что означает кворумная формула W + R > N в распределённых системах?',
    options: [
      'Минимальное количество узлов для работы системы',
      'Формула, гарантирующая, что при чтении (R узлов) и записи (W узлов) из N реплик хотя бы один узел содержит актуальные данные',
      'Количество реплик, необходимое для синхронной репликации',
      'Максимальное число одновременных подключений к базе данных',
    ],
    correctIndex: 1,
    explanation:
      'В системах с кворумом (quorum) данные записываются на W узлов и читаются с R узлов из N реплик. Если W + R > N, то множества узлов записи и чтения обязательно пересекаются — хотя бы один узел содержит самую свежую версию данных. Примеры: N=3, W=2, R=2 — строгая консистентность. N=3, W=1, R=1 — высокая доступность, но возможно чтение устаревших данных. N=3, W=3, R=1 — быстрое чтение, медленная запись. Этот подход используется в Cassandra, DynamoDB, Riak.',
  },
  {
    id: 'sd-replication-010',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'middle',
    type: 'quiz',
    question:
      'Какая главная проблема multi-master (active-active) репликации?',
    options: [
      'Невозможность масштабирования чтения',
      'Конфликты записи: два мастера могут одновременно изменить одни и те же данные, требуя механизмов разрешения конфликтов',
      'Каждый master должен хранить полную копию всех данных',
      'Multi-master репликация не поддерживает транзакции',
    ],
    correctIndex: 1,
    explanation:
      'При multi-master репликации несколько узлов принимают запись. Если два мастера одновременно изменяют одну и ту же запись, возникает конфликт записи (write conflict). Стратегии разрешения: Last Writer Wins (LWW) — побеждает последняя запись по timestamp (просто, но возможна потеря данных), merge на уровне приложения, CRDT (Conflict-free Replicated Data Types) — структуры данных, автоматически разрешающие конфликты, custom conflict resolution. Multi-master используется для geo-distributed систем (пользователь пишет в ближайший master).',
  },
  {
    id: 'sd-replication-011',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'middle',
    type: 'quiz',
    question:
      'Что такое rebalancing и когда оно необходимо при шардировании?',
    options: [
      'Процесс удаления устаревших данных из шардов для экономии места',
      'Перераспределение данных между шардами при добавлении/удалении узлов или возникновении неравномерной нагрузки (hotspot)',
      'Синхронизация индексов между шардами',
      'Переключение с hash-based на range-based шардирование',
    ],
    correctIndex: 1,
    explanation:
      'Rebalancing — процесс перемещения данных между шардами для восстановления равномерного распределения. Необходимо при: добавлении нового шарда (scale out), удалении шарда, возникновении hotspot (один шард перегружен). Подходы: hash mod N — меняет размещение почти всех ключей (плохо), consistent hashing — перемещает минимум данных, fixed partitions — предварительно нарезанные партиции переназначаются между узлами (Cassandra, Elasticsearch). Rebalancing должен быть online — система продолжает работу во время перебалансировки.',
  },
  {
    id: 'sd-replication-012',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'middle',
    type: 'open',
    question:
      'Что такое replication lag? Какие проблемы он вызывает и как их минимизировать?',
    sampleAnswer:
      'Replication lag — задержка между записью на master и появлением данных на replica при асинхронной репликации. Проблемы: 1) Read-after-write inconsistency: пользователь создаёт запись, читает с реплики и не видит своих данных. 2) Monotonic reads violation: последовательные чтения возвращают данные из разных моментов времени. 3) Phantom reads: данные то появляются, то исчезают. Решения: 1) Read-your-writes: после записи читать с master или ждать синхронизации. 2) Monotonic reads: привязать пользователя к одной реплике (sticky sessions). 3) Semi-synchronous replication: хотя бы одна реплика подтверждает запись синхронно. 4) Мониторинг lag и алертинг при превышении порога.',
    explanation:
      'Replication lag — фундаментальная проблема асинхронной репликации. Типичный lag: миллисекунды–секунды, но при пиковой нагрузке может достигать минут. В PostgreSQL lag мониторится через pg_stat_replication, в MySQL — через SHOW SLAVE STATUS (Seconds_Behind_Master). Проектирование приложений должно учитывать возможность lag.',
  },
  {
    id: 'sd-replication-013',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'middle',
    type: 'open',
    question:
      'Как выбрать ключ шардирования (shard key)? Перечислите основные критерии и приведите примеры хорошего и плохого выбора.',
    sampleAnswer:
      'Критерии: 1) Высокая кардинальность — множество уникальных значений для равномерного распределения. 2) Равномерное распределение — данные распределяются поровну между шардами, без hotspots. 3) Учёт паттернов запросов — ключ должен позволять обращаться к одному шарду (а не scatter-gather по всем). 4) Стабильность — ключ не должен часто меняться. Хорошие примеры: user_id (равномерное распределение, запросы обычно по конкретному пользователю), order_id (высокая кардинальность). Плохие примеры: дата создания (все новые записи на одном шарде — hotspot), страна (неравномерное распределение — в одних странах миллионы пользователей, в других — тысячи), булево поле (2 значения = максимум 2 шарда).',
    explanation:
      'Выбор shard key — одно из самых критичных и сложно обратимых решений в архитектуре. Ошибка приводит к hotspots, cross-shard запросам и необходимости полной перебалансировки. Instagram выбрал user_id для шардирования, но пришлось решать проблему с timeline (который агрегирует данные нескольких пользователей).',
  },
  {
    id: 'sd-replication-014',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'middle',
    type: 'open',
    question:
      'Сравните стратегии разрешения конфликтов при multi-master репликации: Last Writer Wins (LWW), merge-функции и CRDT. Когда какую использовать?',
    sampleAnswer:
      'Last Writer Wins (LWW): побеждает запись с более поздним timestamp. Просто, но возможна потеря данных. Зависит от синхронизации часов (clock skew). Подходит для некритичных данных (last seen, метрики). Merge-функции: приложение реализует логику слияния конфликтующих версий. Гибко, но сложно в реализации. Подходит для сложных бизнес-объектов, где нужна ручная или domain-specific логика. CRDT (Conflict-free Replicated Data Types): математически гарантированное автоматическое разрешение конфликтов. Примеры: G-Counter (счётчик), OR-Set (множество), LWW-Register. Ограничение: поддерживают только определённый набор операций. Подходят для счётчиков, корзин покупок, совместного редактирования. Riak использует CRDT, DynamoDB — LWW, CouchDB — хранит все конфликтующие версии для разрешения на уровне приложения.',
    explanation:
      'Разрешение конфликтов — одна из сложнейших задач в распределённых системах. Универсального решения нет. На практике системы часто комбинируют подходы: CRDT для структур данных, LWW для метаданных, application-level merge для бизнес-логики.',
  },
  {
    id: 'sd-replication-015',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'senior',
    type: 'quiz',
    question:
      'Какой механизм в PostgreSQL используется для физической репликации?',
    options: [
      'Trigger-based replication — триггеры на каждую таблицу',
      'Streaming replication на основе WAL (Write-Ahead Log) — потоковая передача журнала предзаписи',
      'Statement-based replication — повторное выполнение SQL-запросов',
      'Snapshot replication — периодическая полная копия базы',
    ],
    correctIndex: 1,
    explanation:
      'PostgreSQL использует WAL-based streaming replication для физической репликации. WAL (Write-Ahead Log) — журнал, куда записываются все изменения данных перед их применением. Master потоково передаёт WAL-записи на реплики, которые применяют их к своей копии данных. Преимущества: низкая задержка, гарантия консистентности, воспроизведение всех изменений (включая DDL). PostgreSQL также поддерживает логическую репликацию (logical replication) для выборочной репликации отдельных таблиц.',
  },
  {
    id: 'sd-replication-016',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'senior',
    type: 'quiz',
    question:
      'Какая стратегия cross-datacenter репликации обеспечивает наилучший баланс между задержкой записи и защитой от потери данных?',
    options: [
      'Полностью синхронная репликация между дата-центрами',
      'Semi-synchronous: синхронная репликация внутри дата-центра + асинхронная между дата-центрами с подтверждением хотя бы одной удалённой реплики',
      'Полностью асинхронная репликация с RPO = 0',
      'Ежечасная передача полного backup между дата-центрами',
    ],
    correctIndex: 1,
    explanation:
      'Cross-datacenter репликация — критический компонент disaster recovery. Полностью синхронная репликация даёт RPO=0, но задержка между дата-центрами (десятки мс) делает каждую запись медленной. Semi-synchronous подход: запись подтверждается локально синхронно и асинхронно реплицируется в удалённый DC с подтверждением (ack). Это даёт RPO ~ секунды при приемлемой задержке. MySQL Semi-Sync Replication, PostgreSQL synchronous_standby_names с FIRST 1 — примеры реализации. Для geo-active системы часто используют multi-master с eventual consistency.',
  },
  {
    id: 'sd-replication-017',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'senior',
    type: 'open',
    question:
      'Опишите, как работает автоматический failover в master-slave репликации. Какие проблемы могут возникнуть и как их решить?',
    sampleAnswer:
      'Автоматический failover: 1) Мониторинг — sentinel/orchestrator отслеживает heartbeat master. 2) Обнаружение сбоя — если heartbeat не получен в течение timeout, master считается недоступным. 3) Выборы — определяется лучший кандидат из slave (наименьший replication lag, приоритет). 4) Промоушен — выбранный slave становится новым master, остальные slave переключаются на него. 5) Обновление конфигурации — приложения узнают о новом master (через DNS, service discovery, proxy). Проблемы: 1) Split brain — старый master восстанавливается и принимает записи параллельно с новым → конфликты. Решение: fencing (STONITH — Shoot The Other Node In The Head), проверка lease. 2) Потеря данных — slave мог не получить последние транзакции master (replication lag). Решение: semi-synchronous replication. 3) Ложные срабатывания — сетевой сбой вместо падения master. Решение: consensus-based detection (несколько sentinel-ов голосуют).',
    explanation:
      'Failover — критически важный процесс для обеспечения высокой доступности. Redis Sentinel, MySQL Orchestrator, PostgreSQL Patroni — популярные решения. Правильная реализация failover требует тщательного тестирования (Chaos Engineering) и мониторинга.',
  },
  {
    id: 'sd-replication-018',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'senior',
    type: 'open',
    question:
      'Как спроектировать глобально распределённую систему с multi-region репликацией? Опишите архитектуру, компромиссы и стратегию обработки данных.',
    sampleAnswer:
      'Архитектура: каждый регион имеет полный стек (application servers, database). Данные классифицируются: 1) Региональные (user profile) — master в «домашнем» регионе пользователя, read replicas в других. 2) Глобальные (каталог продуктов) — один глобальный master, read replicas повсюду. 3) Региональные изолированные (GDPR данные) — только в одном регионе. Стратегия записи: пользователь пишет в «домашний» регион, запись реплицируется асинхронно. Cross-region запись через redirect или relay. Компромиссы: latency vs consistency (CAP), сложность операций, стоимость cross-region трафика. Подходы: CockroachDB/Spanner — distributed SQL с global consistency (Spanner TrueTime). DynamoDB Global Tables — multi-master, LWW. Vitess/Citus — шардированный PostgreSQL/MySQL. DNS-based routing направляет пользователей в ближайший регион.',
    explanation:
      'Multi-region — одна из сложнейших задач в распределённых системах. Google Spanner решает её через TrueTime (атомные часы + GPS), обеспечивая строгую консистентность. Для большинства компаний приемлемым решением является eventual consistency с read-your-writes гарантией в «домашнем» регионе.',
  },
  {
    id: 'sd-replication-019',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'senior',
    type: 'open',
    question:
      'Объясните проблему cross-shard запросов и распределённых транзакций при шардировании. Какие подходы к их решению существуют?',
    sampleAnswer:
      'Проблема: при шардировании данные одного запроса могут находиться на разных шардах. Например, JOIN таблиц orders и products, если они шардированы по-разному. Cross-shard запросы: scatter-gather — запрос отправляется на все шарды, результаты агрегируются. Проблемы: высокая задержка, нагрузка на сеть, сложность ORDER BY/LIMIT. Распределённые транзакции: 2PC (Two-Phase Commit) — координатор + участники, гарантирует ACID, но медленный и блокирующий. 3PC — улучшенный вариант, но более сложный. Saga — eventual consistency через компенсации. Подходы к решению: 1) Денормализация — хранить нужные данные вместе на одном шарде. 2) Co-location — связанные данные на одном шарде (tenant-based sharding). 3) Application-level join — агрегация на уровне приложения. 4) Специализированные решения: Vitess (для MySQL) автоматически маршрутизирует cross-shard запросы, CockroachDB — distributed SQL с поддержкой транзакций.',
    explanation:
      'Cross-shard операции — главная «цена» шардирования. Правильный выбор shard key и co-location данных может минимизировать необходимость в cross-shard запросах. На практике 2PC используется редко из-за его блокирующей природы; Saga и eventual consistency — более масштабируемые альтернативы.',
  },
  {
    id: 'sd-replication-020',
    block: 'sd',
    topic: 'replication',
    topicLabel: 'Репликация и шардирование',
    difficulty: 'senior',
    type: 'quiz',
    question:
      'Что такое chain replication и какое преимущество она даёт по сравнению с классической master-slave?',
    options: [
      'Реплики расположены в цепочке: запись идёт на head, передаётся по цепочке, чтение — с tail; это обеспечивает строгую консистентность при высокой пропускной способности',
      'Каждая реплика хранит только часть данных, образуя цепочку ответственности',
      'Данные записываются параллельно на все узлы через multicast',
      'Реплики автоматически перестраиваются в цепочку при failover',
    ],
    correctIndex: 0,
    explanation:
      'Chain replication — модель, в которой узлы выстроены в цепочку. Запись направляется на head (голову) цепочки и последовательно передаётся через все узлы до tail (хвоста). Чтение обслуживается только tail-ом. Преимущества: строгая консистентность (tail содержит все подтверждённые записи), высокая пропускная способность (нагрузка записи распределяется по цепочке), простая семантика. Используется в Microsoft Azure Storage, HDFS (адаптированный вариант). Недостатки: задержка записи пропорциональна длине цепочки, один медленный узел замедляет всю цепочку.',
  },
];
