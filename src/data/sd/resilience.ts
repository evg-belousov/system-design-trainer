import type { Question } from '../types';

export const resilienceQuestions: Question[] = [
  {
    id: 'sd-resilience-001',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое отказоустойчивость (fault tolerance) в контексте распределённых систем?',
    options: [
      'Способность системы работать без каких-либо сбоев',
      'Способность системы продолжать корректную работу при отказе одного или нескольких компонентов',
      'Автоматическое масштабирование системы при увеличении нагрузки',
      'Шифрование данных для защиты от внешних угроз',
    ],
    correctIndex: 1,
    explanation:
      'Отказоустойчивость -- это свойство системы продолжать корректно функционировать даже при выходе из строя части её компонентов. Это достигается за счёт избыточности, репликации, автоматического переключения на резервные узлы и других механизмов. В распределённых системах отказы неизбежны (сеть, диски, процессы), поэтому проектирование с учётом отказоустойчивости -- фундаментальное требование.',
  },
  {
    id: 'sd-resilience-002',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какой паттерн предотвращает каскадные отказы, прекращая вызовы к неработающему сервису после определённого порога ошибок?',
    options: [
      'Retry Pattern',
      'Bulkhead Pattern',
      'Circuit Breaker Pattern',
      'Saga Pattern',
    ],
    correctIndex: 2,
    explanation:
      'Circuit Breaker (предохранитель) -- паттерн, который отслеживает количество ошибок при вызове внешнего сервиса. При превышении порога ошибок он «размыкается» и немедленно возвращает ошибку без попытки вызова, давая отказавшему сервису время на восстановление. Через определённый интервал переходит в состояние half-open и пропускает пробный запрос. Если он успешен -- замыкается обратно. Это предотвращает каскадные отказы и экономит ресурсы.',
  },
  {
    id: 'sd-resilience-003',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какие три состояния имеет паттерн Circuit Breaker?',
    options: [
      'Open, Closed, Locked',
      'Closed, Open, Half-Open',
      'Active, Passive, Standby',
      'Ready, Blocked, Recovery',
    ],
    correctIndex: 1,
    explanation:
      'Circuit Breaker имеет три состояния: Closed (замкнут) -- запросы проходят нормально, ошибки считаются; Open (разомкнут) -- все запросы немедленно отклоняются без вызова зависимого сервиса; Half-Open (полуоткрыт) -- пропускается ограниченное количество пробных запросов, чтобы проверить, восстановился ли сервис. Если пробные запросы успешны, Circuit Breaker переходит в Closed; если нет -- обратно в Open.',
  },
  {
    id: 'sd-resilience-004',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое Health Check в микросервисной архитектуре?',
    options: [
      'Проверка наличия обновлений для зависимостей сервиса',
      'Периодический эндпоинт, позволяющий определить, работоспособен ли сервис и готов ли он принимать трафик',
      'Аудит безопасности сервиса перед деплоем',
      'Нагрузочный тест для проверки производительности сервиса',
    ],
    correctIndex: 1,
    explanation:
      'Health Check -- это специальный эндпоинт (обычно /health или /healthz), который возвращает статус работоспособности сервиса. Различают liveness probe (сервис жив и не завис) и readiness probe (сервис готов принимать трафик). Балансировщики нагрузки и оркестраторы (Kubernetes) используют health checks для маршрутизации трафика только на здоровые инстансы и автоматического перезапуска зависших.',
  },
  {
    id: 'sd-resilience-005',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое стратегия retry с экспоненциальным backoff?',
    options: [
      'Повтор запроса с постоянным интервалом между попытками',
      'Повтор запроса с экспоненциально увеличивающимся интервалом ожидания между попытками',
      'Повтор запроса к другому инстансу того же сервиса',
      'Однократный повтор запроса после длительной паузы',
    ],
    correctIndex: 1,
    explanation:
      'Exponential backoff -- стратегия повторных попыток, при которой интервал между ретраями увеличивается экспоненциально (например, 1с, 2с, 4с, 8с). Это предотвращает перегрузку уже нагруженного сервиса лавиной повторных запросов. Часто добавляют jitter (случайное отклонение), чтобы избежать синхронизации ретраев от множества клиентов. Формула: delay = min(base * 2^attempt + random_jitter, max_delay).',
  },
  {
    id: 'sd-resilience-006',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Какая модель избыточности подразумевает, что обе реплики одновременно обрабатывают трафик?',
    options: [
      'Active-Passive',
      'Active-Active',
      'Master-Slave',
      'Cold Standby',
    ],
    correctIndex: 1,
    explanation:
      'В модели Active-Active все реплики одновременно обрабатывают запросы, распределяя нагрузку между собой. При отказе одной реплики остальные продолжают работать, и трафик перераспределяется автоматически. Преимущества: более эффективное использование ресурсов, минимальное время переключения (near-zero downtime). Недостатки: сложность синхронизации данных между репликами, потенциальные конфликты при одновременной записи.',
  },
  {
    id: 'sd-resilience-007',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое паттерн Bulkhead и какую проблему он решает?',
    options: [
      'Разделение системы на изолированные отсеки, чтобы отказ одного компонента не исчерпал ресурсы всей системы',
      'Кэширование ответов для снижения нагрузки на downstream-сервисы',
      'Шифрование данных между микросервисами для обеспечения безопасности',
      'Автоматическое масштабирование подов в Kubernetes при увеличении нагрузки',
    ],
    correctIndex: 0,
    explanation:
      'Bulkhead (переборка) -- паттерн, заимствованный из кораблестроения: корпус корабля разделён на отсеки, и затопление одного не приводит к потоплению всего судна. В ПО это реализуется через выделение отдельных пулов потоков, connection pools или отдельных инстансов для разных зависимостей. Если один downstream-сервис тормозит, он исчерпает только свой пул ресурсов, не затрагивая остальные вызовы.',
  },
  {
    id: 'sd-resilience-008',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Чем отличаются RPO (Recovery Point Objective) и RTO (Recovery Time Objective)?',
    options: [
      'RPO -- максимально допустимое время простоя, RTO -- максимальный объём потерянных данных',
      'RPO -- максимально допустимый объём потерянных данных (за какой период), RTO -- максимально допустимое время восстановления после отказа',
      'RPO -- количество реплик данных, RTO -- количество попыток восстановления',
      'RPO и RTO -- это одно и то же, просто разные термины для времени восстановления',
    ],
    correctIndex: 1,
    explanation:
      'RPO (Recovery Point Objective) определяет максимально допустимый период потери данных: если RPO = 1 час, значит при аварии допустима потеря данных за последний час. Это влияет на частоту бэкапов и репликации. RTO (Recovery Time Objective) определяет максимально допустимое время восстановления работы системы после аварии. RPO=0 требует синхронной репликации, RTO=0 требует active-active конфигурации. Оба показателя напрямую влияют на стоимость инфраструктуры.',
  },
  {
    id: 'sd-resilience-009',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое Graceful Degradation?',
    options: [
      'Полная остановка сервиса при возникновении любой ошибки',
      'Постепенное снижение функциональности системы при отказе компонентов, сохраняя работоспособность ключевых функций',
      'Автоматическое удаление устаревших данных из базы данных',
      'Снижение версии приложения при обнаружении критической уязвимости',
    ],
    correctIndex: 1,
    explanation:
      'Graceful Degradation (грациозная деградация) -- подход, при котором система продолжает предоставлять основную функциональность даже при отказе некритических компонентов. Например, интернет-магазин может отключить рекомендации и отзывы при сбое соответствующих сервисов, но продолжать работу каталога и оформления заказов. Это требует чёткого определения приоритетов функций и реализации fallback-механизмов.',
  },
  {
    id: 'sd-resilience-010',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Зачем при retry-стратегии добавляют jitter (случайное отклонение) к интервалу ожидания?',
    options: [
      'Чтобы ускорить обработку запросов за счёт параллельных ретраев',
      'Чтобы избежать ситуации, когда множество клиентов одновременно повторяют запросы и создают всплеск нагрузки (thundering herd)',
      'Чтобы обеспечить криптографическую стойкость повторных запросов',
      'Чтобы пропустить заведомо неуспешные попытки и сэкономить ресурсы',
    ],
    correctIndex: 1,
    explanation:
      'Без jitter множество клиентов, начавших ретраи одновременно (например, после общего сбоя), будут повторять запросы синхронно: через 1с, через 2с, через 4с -- создавая периодические всплески нагрузки. Jitter добавляет случайное отклонение к интервалу, распределяя ретраи во времени. AWS рекомендует «full jitter»: delay = random(0, base * 2^attempt), что эмпирически показывает наилучшие результаты при минимизации общего времени восстановления.',
  },
  {
    id: 'sd-resilience-011',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'open',
    question: 'Опишите разницу между failover-стратегиями Active-Active и Active-Passive. Когда какую лучше использовать?',
    sampleAnswer:
      'Active-Active: все узлы одновременно обрабатывают трафик. Плюсы: эффективное использование ресурсов, минимальное время переключения, естественная балансировка нагрузки. Минусы: сложность синхронизации данных, потенциальные конфликты записи, более высокая стоимость обеспечения консистентности. Active-Passive: основной узел обрабатывает трафик, резервный находится в режиме ожидания. Плюсы: простота, нет конфликтов данных, гарантия консистентности. Минусы: резервный узел простаивает, время переключения (failover time) может составлять секунды-минуты. Active-Active подходит для систем с высокими требованиями к доступности (99.99%+) и масштабируемости. Active-Passive -- для систем, где критична строгая консистентность данных и допустим кратковременный простой.',
    explanation:
      'Выбор стратегии зависит от бизнес-требований. Для глобальных систем (CDN, DNS) active-active необходим. Для баз данных с финансовыми транзакциями часто предпочитают active-passive с синхронной репликацией. Существует также вариант Active-Hot Standby, где пассивный узел постоянно получает реплику и может переключиться быстрее, чем Cold Standby.',
  },
  {
    id: 'sd-resilience-012',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'open',
    question: 'Что такое Chaos Engineering? Опишите принципы и приведите примеры инструментов.',
    sampleAnswer:
      'Chaos Engineering -- дисциплина экспериментирования над распределённой системой с целью выявления слабых мест до того, как они приведут к реальным сбоям. Принципы: 1) Определить «стабильное состояние» системы через измеримые метрики. 2) Сформулировать гипотезу: «система продолжит работать при X». 3) Внести реальные сбои: убить контейнер, увеличить latency сети, заполнить диск. 4) Сравнить поведение с гипотезой. 5) Минимизировать «радиус взрыва» эксперимента. Инструменты: Netflix Chaos Monkey (случайное убийство инстансов), Gremlin (платформа для хаос-экспериментов), Litmus (для Kubernetes), AWS Fault Injection Simulator. Chaos Engineering должен проводиться сначала в staging, а затем в production с тщательным контролем.',
    explanation:
      'Netflix популяризировал Chaos Engineering, создав Simian Army -- набор инструментов для внесения различных типов сбоев. Ключевой принцип: если вы не тестируете отказы в контролируемых условиях, вы узнаете о них в самый неподходящий момент. Chaos Engineering -- неотъемлемая часть культуры SRE в крупных технологических компаниях.',
  },
  {
    id: 'sd-resilience-013',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'open',
    question: 'Как реализовать health checks в Kubernetes? Чем отличаются liveness, readiness и startup probes?',
    sampleAnswer:
      'Liveness Probe -- проверяет, что контейнер жив и не завис. Если проверка не проходит, Kubernetes перезапускает контейнер. Используется для обнаружения deadlocks и зависших процессов. Readiness Probe -- проверяет, что контейнер готов принимать трафик. Если проверка не проходит, pod удаляется из Service (перестаёт получать трафик), но не перезапускается. Используется при инициализации, прогреве кэша, ожидании зависимостей. Startup Probe -- проверяет, завершилась ли инициализация контейнера. До успешной startup probe liveness и readiness probes не выполняются. Используется для приложений с долгим стартом. Все три поддерживают HTTP GET, TCP Socket и exec-команды. Важно правильно настроить initialDelaySeconds, periodSeconds, failureThreshold.',
    explanation:
      'Типичная ошибка -- использование одинаковых эндпоинтов для liveness и readiness. Liveness должен проверять только базовую работоспособность процесса, а readiness -- готовность обслуживать запросы (подключение к БД, прогрев кэша). Агрессивный liveness probe может привести к cascading restarts под нагрузкой.',
  },
  {
    id: 'sd-resilience-014',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какой тип ретраев безопасно применять без риска дублирования побочных эффектов?',
    options: [
      'Ретраи для любых HTTP-методов при получении 500 ошибки',
      'Ретраи только для идемпотентных операций (GET, PUT, DELETE с idempotency key)',
      'Ретраи только для POST-запросов, так как они наиболее критичны',
      'Ретраи с бесконечным количеством попыток до успешного ответа',
    ],
    correctIndex: 1,
    explanation:
      'Безопасно ретраить только идемпотентные операции -- те, повторное выполнение которых даёт тот же результат. GET, PUT, DELETE по определению идемпотентны в REST. Для неидемпотентных операций (POST) необходим idempotency key -- уникальный идентификатор запроса, позволяющий серверу распознать дубликат. Stripe, например, поддерживает заголовок Idempotency-Key для безопасного ретрая платёжных операций.',
  },
  {
    id: 'sd-resilience-015',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Какой подход к Disaster Recovery обеспечивает наименьшее RTO, но является наиболее дорогостоящим?',
    options: [
      'Backup & Restore (холодное резервирование)',
      'Pilot Light (минимальная инфраструктура в резервном регионе)',
      'Warm Standby (уменьшенная копия продакшена в резервном регионе)',
      'Multi-Site Active-Active (полная копия продакшена в нескольких регионах)',
    ],
    correctIndex: 3,
    explanation:
      'Multi-Site Active-Active обеспечивает near-zero RTO, так как трафик уже распределён между регионами. При отказе одного региона остальные автоматически принимают нагрузку. Это наиболее дорогой подход, требующий полной инфраструктуры в каждом регионе и сложной синхронизации данных. AWS классифицирует DR-стратегии по возрастанию стоимости и убыванию RTO: Backup & Restore (часы) → Pilot Light (десятки минут) → Warm Standby (минуты) → Multi-Site (секунды).',
  },
  {
    id: 'sd-resilience-016',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое self-healing система и какой ключевой механизм лежит в её основе?',
    options: [
      'Система, которая никогда не ломается благодаря идеальному коду',
      'Система, способная автоматически обнаруживать и устранять сбои через control loops (петли обратной связи)',
      'Система, которая самостоятельно обновляет свои зависимости до последних версий',
      'Система с ручным процессом восстановления, документированным в runbook',
    ],
    correctIndex: 1,
    explanation:
      'Self-healing (самовосстанавливающаяся) система использует control loops -- непрерывные циклы наблюдения за текущим состоянием, сравнения с желаемым и автоматической коррекции. Kubernetes -- яркий пример: контроллеры постоянно сравнивают actual state с desired state и выполняют действия (перезапуск подов, масштабирование, перепланирование). Механизмы: auto-restart, auto-scaling, auto-replacement, automated rollback.',
  },
  {
    id: 'sd-resilience-017',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'open',
    question: 'Спроектируйте стратегию отказоустойчивости для платёжного сервиса, обрабатывающего финансовые транзакции. Какие паттерны вы применили бы и почему?',
    sampleAnswer:
      'Для платёжного сервиса критичны как доступность, так и консистентность данных. Стратегия: 1) Circuit Breaker для вызовов к платёжным провайдерам (Stripe, PayPal) с fallback на альтернативного провайдера. 2) Idempotency keys для всех операций -- каждая транзакция имеет уникальный UUID, предотвращающий двойное списание при ретраях. 3) Retry с exponential backoff + jitter для временных сбоев сети. 4) Saga Pattern для распределённых транзакций (резервирование → списание → подтверждение) с компенсирующими транзакциями при ошибке. 5) Active-Passive репликация БД с синхронной репликацией (RPO=0). 6) Write-ahead log для восстановления незавершённых транзакций после сбоя. 7) Outbox Pattern для надёжной отправки событий. 8) Rate limiting и bulkhead для изоляции от перегрузок. 9) Health checks с проверкой подключения к БД и платёжным провайдерам.',
    explanation:
      'Финансовые системы имеют наивысшие требования к надёжности. Ключевой принцип: лучше отклонить транзакцию, чем выполнить её дважды. Idempotency и at-least-once delivery в сочетании с дедупликацией на стороне сервера обеспечивают exactly-once семантику на уровне бизнес-логики.',
  },
  {
    id: 'sd-resilience-018',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'open',
    question: 'Объясните, как реализовать Bulkhead Pattern на уровне приложения. Приведите конкретные примеры изоляции ресурсов.',
    sampleAnswer:
      'Bulkhead Pattern на уровне приложения реализуется через изоляцию ресурсов для различных зависимостей: 1) Thread Pool Isolation: отдельные пулы потоков для каждого downstream-сервиса (Hystrix использовал этот подход). Если Service A тормозит, исчерпывается только его пул, остальные сервисы работают нормально. 2) Connection Pool Isolation: отдельные connection pools для разных баз данных или внешних API. 3) Semaphore Isolation: ограничение количества одновременных запросов к каждому сервису через семафоры (легковеснее thread pool). 4) Process Isolation: запуск критичных компонентов в отдельных контейнерах/подах. 5) Rate Limiting per dependency: ограничение RPS к каждому downstream. Пример: в e-commerce checkout зависит от Inventory, Payment и Notification. Каждый получает свой пул из 20 потоков. Если Notification зависнет, checkout продолжит работать через Inventory и Payment.',
    explanation:
      'Resilience4j (Java) и Polly (.NET) предоставляют готовые реализации Bulkhead. В Kubernetes bulkhead реализуется через resource limits и отдельные deployments. Комбинация Bulkhead + Circuit Breaker + Timeout даёт максимальную защиту от каскадных отказов.',
  },
  {
    id: 'sd-resilience-019',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'open',
    question: 'Как спроектировать multi-region disaster recovery с RPO < 1 минуты и RTO < 5 минут? Какие компромиссы придётся принять?',
    sampleAnswer:
      'Для RPO < 1 мин и RTO < 5 мин: 1) Данные: асинхронная репликация БД между регионами с lag < 1 мин (например, Aurora Global Database с replication lag ~1с, CockroachDB с multi-region). Синхронная репликация даст RPO=0, но увеличит latency записи. 2) Инфраструктура: Warm Standby -- уменьшенная копия продакшена во втором регионе, с автоскейлингом. 3) DNS failover: Route53 health checks + failover routing policy, TTL = 60с. 4) Stateless сервисы в обоих регионах с общим конфигом. 5) Очереди: SQS/Kafka с cross-region replication. 6) Автоматизация: runbook как код через Lambda/Step Functions для автоматического failover. Компромиссы: повышенная стоимость (~1.5-2x), increased write latency при синхронной репликации, сложность тестирования (нужны регулярные DR-drill), потенциальная потеря данных за последнюю минуту при async-репликации, split-brain при network partition между регионами.',
    explanation:
      'Netflix и Amazon проводят регулярные DR-тренировки (Gameday), переключая трафик между регионами. Ключевое правило: DR-план, который не тестируется регулярно, не работает. Стоимость multi-region DR нужно обосновывать через расчёт потерь от простоя (cost of downtime).',
  },
  {
    id: 'sd-resilience-020',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Какой паттерн позволяет ограничить время ожидания ответа от зависимого сервиса, предотвращая зависание вызывающего потока?',
    options: [
      'Circuit Breaker с порогом ошибок 50%',
      'Timeout Pattern в сочетании с cancel/deadline propagation',
      'Retry с фиксированным интервалом 30 секунд',
      'Bulkhead с одним потоком на сервис',
    ],
    correctIndex: 1,
    explanation:
      'Timeout Pattern -- базовый, но критически важный паттерн. Каждый вызов к внешнему сервису должен иметь таймаут. Deadline propagation передаёт оставшееся время (deadline) вниз по цепочке вызовов: если клиенту осталось 2 секунды, нет смысла ждать 10 секунд от downstream. gRPC поддерживает deadline propagation нативно. Без таймаутов один медленный сервис может заблокировать все потоки вызывающего сервиса, вызвав каскадный отказ.',
  },
  {
    id: 'sd-resilience-021',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое Single Point of Failure (SPOF)?',
    options: [
      'Компонент, который работает быстрее всех остальных',
      'Компонент системы, отказ которого приводит к неработоспособности всей системы',
      'Единственная точка входа в API',
      'Главный сервер в кластере',
    ],
    correctIndex: 1,
    explanation:
      'Single Point of Failure (SPOF) -- компонент, отказ которого приводит к отказу всей системы или её критической части. Примеры: единственный сервер БД без реплик, единственный дата-центр, уникальный API без резервирования. Устранение SPOF -- ключевая задача проектирования отказоустойчивых систем. Методы: репликация, резервирование, географическое распределение, использование managed services с встроенной избыточностью.',
  },
  {
    id: 'sd-resilience-022',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'junior',
    type: 'open',
    question: 'Какие типичные причины отказов в распределённых системах вы знаете?',
    sampleAnswer:
      'Типичные причины отказов: 1) Hardware failures: диски, память, CPU, сетевые карты, блоки питания. 2) Network issues: потеря пакетов, задержки, разделение сети (partition), DNS-сбои. 3) Software bugs: ошибки в коде, memory leaks, deadlocks, race conditions. 4) Resource exhaustion: нехватка CPU, памяти, дискового пространства, file descriptors, connection pool. 5) Dependency failures: отказ внешних сервисов, БД, message brokers. 6) Configuration errors: неправильные настройки, опечатки в конфигах, несовместимые версии. 7) Human errors: неудачный деплой, ошибочная команда, удаление данных. 8) Security incidents: DDoS-атаки, взлом, ransomware. 9) Natural disasters: пожар, наводнение, землетрясение в дата-центре.',
    explanation:
      'Философия "Design for Failure": отказы неизбежны, система должна быть спроектирована так, чтобы продолжать работать при отказе компонентов. Netflix Simian Army систематически вносит различные типы сбоев для проверки устойчивости.',
  },
  {
    id: 'sd-resilience-023',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое Failover?',
    options: [
      'Процесс плановой остановки системы на обслуживание',
      'Автоматическое переключение на резервный компонент при отказе основного',
      'Ручное восстановление системы после сбоя',
      'Тестирование системы на нагрузку',
    ],
    correctIndex: 1,
    explanation:
      'Failover -- процесс автоматического переключения на резервный компонент (сервер, БД, дата-центр) при обнаружении отказа основного. Типы: 1) Hot failover: резервный компонент готов немедленно принять нагрузку (Active-Standby, Active-Active). 2) Cold failover: резервный компонент нужно запустить (дольше, но дешевле). Важные метрики: время обнаружения отказа (detection time), время переключения (switchover time). Автоматический failover требует надёжного механизма health checking.',
  },
  {
    id: 'sd-resilience-024',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое Chaos Monkey?',
    options: [
      'Инструмент для мониторинга приложений',
      'Инструмент Netflix для случайного отключения production-серверов с целью проверки устойчивости системы',
      'Алгоритм балансировки нагрузки',
      'Название бага в коде',
    ],
    correctIndex: 1,
    explanation:
      'Chaos Monkey -- инструмент, созданный Netflix, который случайным образом отключает production-серверы в рабочее время. Цель: убедиться, что система устойчива к отказам отдельных инстансов, и заставить инженеров проектировать с учётом отказов. Chaos Monkey -- часть Simian Army, набора инструментов хаос-инжиниринга: Chaos Gorilla (отключение дата-центров), Latency Monkey (добавление задержек), Chaos Kong (отключение регионов). Это основа практики Chaos Engineering.',
  },
  {
    id: 'sd-resilience-025',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'open',
    question: 'Опишите принципы Chaos Engineering. Как правильно проводить хаос-эксперименты?',
    sampleAnswer:
      'Принципы Chaos Engineering: 1) Определить steady state: измеримые метрики нормальной работы системы (latency, error rate, throughput). 2) Сформулировать гипотезу: "При отключении X система продолжит работать в steady state". 3) Внести реальные сбои: не симуляция, а настоящие проблемы -- kill процессов, network latency, disk failures. 4) Минимизировать blast radius: начинать с малого (один инстанс, не все), иметь возможность быстро остановить эксперимент. 5) Автоматизировать эксперименты для регулярного выполнения. 6) Проводить в production (после тестирования в staging): только production показывает реальное поведение. Правила: днём в рабочие часы (когда инженеры готовы реагировать), начинать с наименее критичных систем, иметь rollback plan, информировать команду.',
    explanation:
      'Chaos Engineering отличается от обычного тестирования: оно проверяет не функциональность, а устойчивость. Gremlin, Litmus, AWS Fault Injection Simulator -- инструменты для проведения экспериментов. Netflix проводит "Chaos Days" -- регулярные масштабные учения.',
  },
  {
    id: 'sd-resilience-026',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какой паттерн позволяет приложению предоставить упрощённую функциональность при недоступности зависимого сервиса?',
    options: [
      'Circuit Breaker',
      'Fallback Pattern',
      'Retry Pattern',
      'Timeout Pattern',
    ],
    correctIndex: 1,
    explanation:
      'Fallback Pattern -- предоставление альтернативного ответа при недоступности основного сервиса. Варианты fallback: 1) Кэшированные данные (stale data better than no data). 2) Значение по умолчанию (default recommendations, generic response). 3) Упрощённая логика (локальный расчёт вместо вызова ML-сервиса). 4) Graceful degradation UI (скрыть недоступный блок). Часто используется вместе с Circuit Breaker: при открытом circuit breaker вызывается fallback. Важно: fallback должен быть полезным для пользователя, а не просто сообщением об ошибке.',
  },
  {
    id: 'sd-resilience-027',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'open',
    question: 'Что такое SRE (Site Reliability Engineering) и какие ключевые практики оно включает?',
    sampleAnswer:
      'SRE (Site Reliability Engineering) -- подход Google к обеспечению надёжности систем, применяющий принципы software engineering к операциям. Ключевые практики: 1) SLI/SLO/SLA: Service Level Indicators (метрики), Objectives (цели), Agreements (соглашения). Error budget -- допустимый процент ошибок. 2) Toil reduction: автоматизация рутинных операций, чтобы инженеры занимались инжинирингом, а не ручной работой. 3) Postmortems: blameless анализ инцидентов для предотвращения повторения. 4) Capacity planning: прогнозирование и обеспечение ресурсов. 5) Release engineering: безопасные деплои, canary, rollback. 6) Monitoring & Alerting: golden signals (latency, traffic, errors, saturation). 7) On-call: ротация дежурств с обязательством команды разработки. Принцип: если error budget исчерпан, новые фичи останавливаются до улучшения надёжности.',
    explanation:
      'SRE формализует trade-off между скоростью разработки и надёжностью через error budget. Книги Google "Site Reliability Engineering" и "The Site Reliability Workbook" -- каноническое описание практик. SRE стал стандартом индустрии наряду с DevOps.',
  },
  {
    id: 'sd-resilience-028',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое SLO (Service Level Objective)?',
    options: [
      'Контракт с клиентом о компенсации при простое',
      'Целевое значение метрики надёжности, например "99.9% запросов успешны"',
      'Инструмент для мониторинга',
      'Методология разработки',
    ],
    correctIndex: 1,
    explanation:
      'SLO (Service Level Objective) -- внутренняя цель по уровню надёжности сервиса. Определяется через SLI (Service Level Indicator) -- конкретную метрику. Пример: SLI = процент успешных запросов, SLO = 99.9% запросов успешны за месяц. SLA (Service Level Agreement) -- внешний контракт с клиентами на основе SLO, обычно с финансовыми последствиями. Иерархия: SLI (метрика) -> SLO (цель) -> SLA (контракт). Error budget = 100% - SLO = допустимый процент ошибок (при 99.9% SLO error budget = 0.1% = ~43 минуты downtime в месяц).',
  },
  {
    id: 'sd-resilience-029',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'open',
    question: 'Опишите процесс проведения Postmortem после инцидента. Какие элементы должен содержать документ?',
    sampleAnswer:
      'Postmortem -- документированный анализ инцидента для извлечения уроков. Принцип: blameless -- фокус на системных причинах, а не на поиске виноватых. Структура документа: 1) Summary: краткое описание инцидента. 2) Impact: затронутые пользователи, длительность, потери. 3) Timeline: хронология событий с точным временем. 4) Root Cause: глубинная причина (не "человеческая ошибка", а "почему система позволила ошибку"). 5) Resolution: как проблема была решена. 6) Lessons Learned: что сработало, что нет. 7) Action Items: конкретные задачи по предотвращению повторения (с владельцами и сроками). Процесс: собрать участников инцидента, восстановить timeline по логам и чатам, провести анализ "5 почему", определить действия, опубликовать документ для всей организации.',
    explanation:
      'Google, Etsy, GitLab публикуют postmortems публично, демонстрируя культуру открытости. Blameless culture критична: если люди боятся наказания, они будут скрывать ошибки. Инцидент -- возможность улучшить систему, а не наказать людей.',
  },
  {
    id: 'sd-resilience-030',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое Redundancy (избыточность) в контексте отказоустойчивости?',
    options: [
      'Удаление лишних компонентов системы',
      'Дублирование критических компонентов для продолжения работы при отказе одного из них',
      'Сжатие данных для экономии места',
      'Оптимизация производительности',
    ],
    correctIndex: 1,
    explanation:
      'Redundancy (избыточность) -- дублирование компонентов системы для обеспечения работоспособности при отказе. Типы: 1) Hardware redundancy: дублирование серверов, дисков (RAID), блоков питания, сетевых карт. 2) Software redundancy: несколько инстансов приложения. 3) Data redundancy: репликация данных между серверами/дата-центрами. 4) Geographic redundancy: размещение в нескольких дата-центрах/регионах. Избыточность увеличивает стоимость, но уменьшает вероятность полного отказа. N+1, N+2 redundancy -- наличие 1 или 2 запасных компонентов.',
  },
  {
    id: 'sd-resilience-031',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'open',
    question: 'Что такое Load Shedding и когда его следует применять?',
    sampleAnswer:
      'Load Shedding -- контролируемый сброс части нагрузки при приближении системы к пределу мощности. Цель: сохранить работоспособность для части запросов вместо деградации для всех. Стратегии: 1) Random shedding: отклонять случайный процент запросов (HTTP 503). 2) Priority-based: отклонять низкоприоритетные запросы, сохраняя критичные (платящие клиенты > бесплатные). 3) LIFO shedding: отклонять новые запросы, завершая уже принятые. 4) Client-based quotas: ограничить запросы от "шумных" клиентов. Когда применять: DDoS-атака, неожиданный всплеск трафика, деградация downstream-сервисов, нехватка ресурсов. Реализация: monitoring saturation metrics, adaptive thresholds, graceful 503 responses с Retry-After.',
    explanation:
      'Load shedding -- последняя линия защиты. Лучше быстро отказать 20% запросов, чем медленно обслужить 100% с таймаутами. Amazon использует adaptive load shedding в пиковые нагрузки (Prime Day). Важно: shed early (до полной деградации) и shed predictably (клиенты знают, как retry).',
  },
  {
    id: 'sd-resilience-032',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое GameDay в контексте SRE?',
    options: [
      'День, когда команда играет в игры для тимбилдинга',
      'Запланированное упражнение по симуляции сбоев и отработке процедур реагирования',
      'День релиза новой версии продукта',
      'Ежемесячное планирование спринта',
    ],
    correctIndex: 1,
    explanation:
      'GameDay -- запланированное упражнение, при котором команда симулирует различные сбои и отрабатывает процедуры реагирования. Цели: проверить runbooks и автоматизацию, обучить команду реагированию на инциденты, выявить слабые места в системе и процессах. Отличие от Chaos Engineering: GameDay фокусируется на людях и процессах, не только на технической устойчивости. Примеры сценариев: отказ дата-центра, компрометация credentials, потеря данных. Amazon проводит GameDays перед крупными событиями (Prime Day, Black Friday).',
  },
  {
    id: 'sd-resilience-033',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какие "Four Golden Signals" мониторинга определены в SRE?',
    options: [
      'CPU, Memory, Disk, Network',
      'Latency, Traffic, Errors, Saturation',
      'Uptime, Downtime, MTTR, MTBF',
      'Requests, Responses, Retries, Rejects',
    ],
    correctIndex: 1,
    explanation:
      'Four Golden Signals (Google SRE book): 1) Latency -- время обработки запросов. Важно отслеживать отдельно успешные и неуспешные (ошибки могут быть быстрыми). 2) Traffic -- объём нагрузки (RPS, transactions/sec). 3) Errors -- процент неуспешных запросов (HTTP 5xx, exceptions). 4) Saturation -- насколько "заполнена" система (CPU, memory utilization, queue depth). Эти метрики дают полную картину здоровья сервиса. RED method (Rate, Errors, Duration) и USE method (Utilization, Saturation, Errors) -- альтернативные подходы.',
  },
  {
    id: 'sd-resilience-034',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'open',
    question: 'Как спроектировать систему алертинга, чтобы избежать alert fatigue?',
    sampleAnswer:
      'Alert fatigue -- состояние, когда инженеры игнорируют алерты из-за их избыточности или частых false positives. Принципы проектирования: 1) Actionable alerts: каждый алерт требует действия. Если действия нет -- это не алерт, а информация для дашборда. 2) Симптомы, не причины: алертить на "высокий error rate", а не на "CPU high" (который может быть нормой). 3) Чёткий runbook: ссылка на инструкцию по реагированию в алерте. 4) Правильные пороги: использовать percentiles (p99 latency), не averages. Учитывать нормальные колебания. 5) Группировка: объединять связанные алерты, не спамить. 6) Приоритизация: разделять critical (будит ночью) и warning (в рабочее время). 7) Регулярный review: удалять алерты, на которые не реагируют. 8) SLO-based alerting: алерт при превышении burn rate error budget.',
    explanation:
      'Google рекомендует: если on-call инженер получает больше 2 алертов за дежурство, требующих реакции -- это слишком много. Цель: каждый алерт важен и требует действия. PagerDuty, Opsgenie помогают управлять escalation и routing алертов.',
  },
  {
    id: 'sd-resilience-035',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое MTTR (Mean Time To Recovery)?',
    options: [
      'Среднее время между отказами',
      'Среднее время восстановления системы после отказа',
      'Максимальное время работы без перезагрузки',
      'Минимальное время для бэкапа',
    ],
    correctIndex: 1,
    explanation:
      'MTTR (Mean Time To Recovery/Repair) -- среднее время восстановления системы после отказа. Включает: время обнаружения проблемы, время диагностики, время исправления, время верификации. Связанные метрики: MTTD (Mean Time To Detect), MTTI (Mean Time To Identify), MTTF (Mean Time To Failure), MTBF (Mean Time Between Failures). Формула availability: MTBF / (MTBF + MTTR). Для повышения availability можно уменьшать MTTR (быстрее восстанавливаться) или увеличивать MTBF (реже ломаться).',
  },
  {
    id: 'sd-resilience-036',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'open',
    question: 'Что такое Incident Management и какие роли участвуют в процессе реагирования на инцидент?',
    sampleAnswer:
      'Incident Management -- процесс реагирования на сбои для минимизации влияния и быстрого восстановления. Роли (ICS-based): 1) Incident Commander (IC): координирует действия, принимает решения, ведёт коммуникацию с stakeholders. Не занимается техническими задачами. 2) Technical Lead: ведёт техническую диагностику и исправление. Может делегировать задачи инженерам. 3) Communications Lead: информирует внешних stakeholders (status page, клиенты, менеджмент). 4) Scribe: документирует timeline, действия, решения в реальном времени. Процесс: 1) Detection (мониторинг, алерты). 2) Triage (оценка severity). 3) Response (митигация, исправление). 4) Recovery (восстановление нормальной работы). 5) Post-incident (postmortem, action items).',
    explanation:
      'ICS (Incident Command System) заимствован из пожарных служб и адаптирован для IT. PagerDuty, Atlassian Incident Management, OpsGenie предоставляют tooling. Важно: регулярные тренировки (tabletop exercises) для отработки процесса без реального инцидента.',
  },
  {
    id: 'sd-resilience-037',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое Error Budget Policy?',
    options: [
      'Политика выделения бюджета на исправление ошибок',
      'Политика, определяющая действия команды при исчерпании допустимого уровня ошибок (error budget)',
      'Политика документирования багов',
      'Правила приоритизации задач',
    ],
    correctIndex: 1,
    explanation:
      'Error Budget Policy -- соглашение между SRE и продуктовой командой о действиях при исчерпании error budget. Типичные правила: 1) При исчерпании budget (> SLO ошибок): заморозить feature releases, все усилия на надёжность. 2) При сжигании budget быстрее нормы: review архитектуры, добавить chaos testing. 3) При избытке budget: можно рисковать больше, ускорить releases. Error budget -- механизм балансировки скорости разработки и надёжности. Если budget есть -- можно двигаться быстрее и принимать риски. Если исчерпан -- фокус на стабильности.',
  },
  {
    id: 'sd-resilience-038',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Какой инструмент Kubernetes автоматически перезапускает pod при падении?',
    options: [
      'Ingress Controller',
      'ReplicaSet / Deployment controller',
      'ConfigMap',
      'Service',
    ],
    correctIndex: 1,
    explanation:
      'ReplicaSet (и Deployment, который управляет ReplicaSet) -- контроллер Kubernetes, обеспечивающий, что заданное количество реплик pod всегда запущено. Если pod падает (crash, OOM, node failure), контроллер создаёт новый pod для восстановления desired state. Это реализация self-healing паттерна. Дополнительно: liveness probe перезапускает "зависший" pod, readiness probe исключает unhealthy pod из балансировки трафика.',
  },
  {
    id: 'sd-resilience-039',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'open',
    question: 'Как спроектировать Disaster Recovery план для критического приложения?',
    sampleAnswer:
      'Компоненты DR-плана: 1) Business Impact Analysis: определить критические системы, допустимый downtime (RTO), допустимую потерю данных (RPO), финансовые потери от простоя. 2) DR Strategy: выбрать подход (Backup/Restore, Pilot Light, Warm Standby, Multi-Site Active-Active) на основе RTO/RPO и бюджета. 3) Infrastructure: резервный дата-центр/регион, репликация данных (sync для RPO=0, async для RPO>0), network connectivity. 4) Data backup: регулярные бэкапы, тестирование восстановления, offsite storage (3-2-1 rule: 3 копии, 2 разных носителя, 1 offsite). 5) Runbooks: пошаговые инструкции для failover и recovery. 6) Testing: регулярные DR-тренировки (минимум раз в год), chaos engineering. 7) Communication plan: кто уведомляет кого, contact lists, status page. 8) Recovery verification: как проверить, что система восстановлена корректно.',
    explanation:
      'DR-план, который не тестируется -- не DR-план. Многие компании узнают о проблемах в DR только при реальном инциденте. GitLab публично описал свой DR fail в 2017 (потеря данных из-за нетестированных бэкапов). Регулярные DR-drill обязательны.',
  },
  {
    id: 'sd-resilience-040',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'junior',
    type: 'quiz',
    question: 'Что такое Availability (доступность) и как она измеряется?',
    options: [
      'Скорость ответа системы',
      'Процент времени, когда система работоспособна и отвечает на запросы',
      'Количество одновременных пользователей',
      'Объём обрабатываемых данных',
    ],
    correctIndex: 1,
    explanation:
      'Availability -- процент времени, когда система доступна и функционирует. Измеряется как: Uptime / (Uptime + Downtime) * 100%. "Девятки" доступности: 99% = ~3.65 дней downtime/год, 99.9% (три девятки) = ~8.76 часов/год, 99.99% (четыре девятки) = ~52.6 минут/год, 99.999% (пять девяток) = ~5.26 минут/год. Каждая дополнительная девятка значительно сложнее и дороже. High Availability (HA) обычно означает 99.9%+.',
  },
  {
    id: 'sd-resilience-041',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'open',
    question: 'Какие стратегии резервного копирования (backup) существуют? Объясните правило 3-2-1.',
    sampleAnswer:
      'Стратегии backup: 1) Full backup: полная копия всех данных. Долго создаётся, быстро восстанавливается. 2) Incremental backup: только изменения с последнего бэкапа. Быстро создаётся, восстановление требует chain всех инкрементов. 3) Differential backup: изменения с последнего full backup. Компромисс между full и incremental. 4) Continuous/CDP (Continuous Data Protection): постоянная репликация изменений, минимальный RPO. Правило 3-2-1: 3 копии данных (оригинал + 2 бэкапа), 2 разных типа носителей (диск + tape/cloud), 1 копия offsite (другой дата-центр/регион). Дополнение 3-2-1-1-0: ещё 1 offline копия (защита от ransomware), 0 ошибок при проверке восстановления.',
    explanation:
      'Бэкап без тестирования восстановления -- не бэкап. Регулярно проверяйте возможность восстановления. GitLab в 2017 узнали, что 5 из 5 типов бэкапов не работали, только во время реального инцидента. Test your backups!',
  },
  {
    id: 'sd-resilience-042',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое Blue-Green Deployment с точки зрения отказоустойчивости?',
    options: [
      'Деплой в два разных облака одновременно',
      'Стратегия деплоя с двумя идентичными средами для мгновенного отката и минимизации downtime',
      'Цветовая индикация состояния сервисов',
      'Тестирование production и staging одновременно',
    ],
    correctIndex: 1,
    explanation:
      'Blue-Green Deployment -- стратегия с двумя идентичными production-средами. Blue -- текущая, Green -- новая версия. Процесс: деплой в Green, тестирование, переключение трафика с Blue на Green. Преимущества для отказоустойчивости: 1) Мгновенный rollback: при проблемах переключить трафик обратно на Blue. 2) Zero downtime: переключение атомарно. 3) Полное тестирование перед production traffic. Недостатки: двойные ресурсы, сложность с БД-миграциями (обе версии должны работать с одной схемой).',
  },
  {
    id: 'sd-resilience-043',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое Circuit Breaker half-open state и зачем он нужен?',
    options: [
      'Состояние, когда circuit breaker частично пропускает трафик для балансировки нагрузки',
      'Состояние для проверки восстановления сервиса: пропускает ограниченное число пробных запросов',
      'Ошибка конфигурации circuit breaker',
      'Режим отладки circuit breaker',
    ],
    correctIndex: 1,
    explanation:
      'Half-open state -- промежуточное состояние Circuit Breaker между Open (все запросы отклоняются) и Closed (все пропускаются). После таймаута в Open состоянии breaker переходит в Half-Open и пропускает ограниченное число пробных запросов. Если они успешны -- переход в Closed (сервис восстановился). Если хотя бы один неуспешен -- возврат в Open (сервис ещё не готов). Это позволяет автоматически восстановить работу без ручного вмешательства, при этом не перегружая восстанавливающийся сервис.',
  },
  {
    id: 'sd-resilience-044',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'open',
    question: 'Как реализовать rate limiting для защиты от перегрузки? Какие алгоритмы существуют?',
    sampleAnswer:
      'Алгоритмы rate limiting: 1) Token Bucket: "ведро" с токенами, каждый запрос потребляет токен, токены добавляются с фиксированной скоростью. Позволяет burst (всплески) в пределах размера ведра. 2) Leaky Bucket: запросы попадают в очередь и обрабатываются с фиксированной скоростью. Сглаживает трафик. 3) Fixed Window: подсчёт запросов в фиксированных временных окнах (например, 100/минуту). Проблема: на границе окон возможен двойной лимит. 4) Sliding Window Log: хранение timestamp каждого запроса, подсчёт в скользящем окне. Точный, но memory-intensive. 5) Sliding Window Counter: комбинация fixed window и взвешенного подсчёта -- эффективный компромисс. Реализация: в API Gateway, middleware, через Redis (атомарные операции INCR + EXPIRE). Response: 429 Too Many Requests + Retry-After header.',
    explanation:
      'Token Bucket -- наиболее популярный из-за баланса между сглаживанием и разрешением burst. AWS API Gateway, Stripe используют token bucket. Важно: rate limiting должен быть distributed (через Redis/Memcached) для корректной работы при нескольких инстансах.',
  },
  {
    id: 'sd-resilience-045',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'junior',
    type: 'open',
    question: 'Что такое Runbook и зачем он нужен?',
    sampleAnswer:
      'Runbook -- документированная пошаговая инструкция для выполнения операционных задач или реагирования на инциденты. Содержит: 1) Описание ситуации/симптомов. 2) Шаги диагностики. 3) Команды/действия для исправления. 4) Критерии успешности (как проверить, что проблема решена). 5) Escalation path (к кому обращаться, если не помогло). Зачем нужен: 1) Быстрое реагирование ночью: on-call инженер, не знакомый с системой, может следовать инструкции. 2) Consistency: все реагируют одинаково. 3) Обучение: новые инженеры изучают систему через runbooks. 4) Automation: хорошо документированные процедуры легче автоматизировать. Хранение: Confluence, Notion, GitHub wiki, специализированные платформы (Runbook.com).',
    explanation:
      'Принцип: если процедура выполняется больше 2 раз вручную -- должен быть runbook. Если runbook выполняется часто -- автоматизировать. Google SRE: "если что-то можно автоматизировать -- автоматизируйте, если нельзя -- документируйте runbook".',
  },
  {
    id: 'sd-resilience-046',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Что такое Dependency Isolation и как паттерн Bulkhead его реализует?',
    options: [
      'Шифрование данных между зависимыми сервисами',
      'Изоляция ресурсов (пулы потоков, connections) для разных зависимостей, чтобы проблема с одной не затронула другие',
      'Удаление зависимостей из кода',
      'Кэширование ответов зависимых сервисов',
    ],
    correctIndex: 1,
    explanation:
      'Dependency Isolation -- принцип изоляции зависимостей, чтобы отказ или замедление одной не повлиял на работу с другими. Bulkhead реализует это через разделение ресурсов: отдельный thread pool для каждого downstream service. Если Service A тормозит и исчерпывает свой pool -- запросы к Service B продолжают работать через свой pool. Дополнительно: separate connection pools, separate rate limits, dedicated instances для критичных зависимостей. Resilience4j, Hystrix (deprecated) предоставляют готовые реализации Bulkhead.',
  },
  {
    id: 'sd-resilience-047',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое Canary Release и какое преимущество он даёт для отказоустойчивости?',
    options: [
      'Релиз только для тестировщиков',
      'Постепенный rollout новой версии на малую часть трафика для раннего обнаружения проблем',
      'Релиз в нерабочее время',
      'Автоматический rollback при любой ошибке',
    ],
    correctIndex: 1,
    explanation:
      'Canary Release -- стратегия деплоя, при которой новая версия сначала получает малую долю production-трафика (1-5%). Метрики сравниваются с baseline (старой версией). При проблемах -- rollback до масштабного влияния. Преимущество: минимальный "blast radius" -- если в новой версии баг, пострадает только 1-5% пользователей, а не все. Реализация: Kubernetes с Istio/Flagger для traffic splitting, AWS с CodeDeploy, Argo Rollouts. Automated canary analysis сравнивает error rates, latency и автоматически принимает решение о продолжении или откате.',
  },
  {
    id: 'sd-resilience-048',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'open',
    question: 'Как реализовать graceful degradation для e-commerce системы? Приведите конкретные примеры.',
    sampleAnswer:
      'Graceful degradation для e-commerce: 1) Recommendation Service недоступен: показать популярные товары из кэша или статический список бестселлеров вместо персонализированных рекомендаций. 2) Search Service деградирует: переключиться на упрощённый поиск (только по названию), показать категории для навигации. 3) Inventory Service медленный: показать "в наличии" по умолчанию, проверить реальное наличие при checkout. 4) Review Service недоступен: скрыть блок отзывов, показать средний рейтинг из кэша. 5) Payment Service проблемы: предложить альтернативный способ оплаты, сохранить заказ для позднего завершения. 6) Высокая нагрузка: отключить тяжёлые операции (live chat, видео-обзоры), упростить страницы. Принцип: core flow (просмотр -> корзина -> checkout -> оплата) должен работать, декоративные функции можно отключить.',
    explanation:
      'Amazon во время Prime Day использует extensive graceful degradation: отключаются некритичные функции для обеспечения core shopping experience. Feature flags позволяют быстро отключать/включать компоненты. Важно заранее определить приоритеты функций и реализовать fallback для каждой.',
  },
  {
    id: 'sd-resilience-049',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'middle',
    type: 'quiz',
    question: 'Что такое Observability и чем она отличается от мониторинга?',
    options: [
      'Это одно и то же',
      'Observability -- способность понять внутреннее состояние системы по её внешним выходам (метрики, логи, трейсы); мониторинг -- сбор предопределённых метрик',
      'Observability -- только для микросервисов, мониторинг -- для монолитов',
      'Мониторинг включает observability как подмножество',
    ],
    correctIndex: 1,
    explanation:
      'Observability (наблюдаемость) -- свойство системы, позволяющее понять её внутреннее состояние по внешним выходам: метрики, логи, трейсы (three pillars). Отличие от мониторинга: мониторинг отвечает на известные вопросы ("какой error rate?"), observability позволяет исследовать неизвестные проблемы ("почему этот конкретный запрос медленный?"). High cardinality data, distributed tracing, structured logging -- ключевые элементы observability. Инструменты: Datadog, Honeycomb, Grafana + Loki + Tempo, OpenTelemetry.',
  },
  {
    id: 'sd-resilience-050',
    block: 'sd',
    topic: 'resilience',
    topicLabel: 'Отказоустойчивость',
    difficulty: 'senior',
    type: 'quiz',
    question: 'Какой принцип описывает подход "проектировать систему так, чтобы она корректно работала при отказах"?',
    options: [
      'Defensive Programming',
      'Design for Failure',
      'Fail Safe',
      'Error Handling',
    ],
    correctIndex: 1,
    explanation:
      'Design for Failure -- фундаментальный принцип проектирования распределённых систем: отказы неизбежны, система должна быть спроектирована так, чтобы продолжать работать (пусть с ограничениями) при отказах компонентов. Практики: redundancy, circuit breakers, timeouts, retries, graceful degradation, chaos engineering. Amazon: "Everything fails, all the time" (Werner Vogels). Netflix: "The best way to avoid failure is to fail constantly" (Chaos Engineering). Это означает proactive подход: не надеяться, что всё будет работать, а готовиться к сбоям.',
  },
];
